[
  {
    "instruction": "Explain a in simple terms for a beginner in data science.",
    "input": "",
    "output": "decreasing. From this it follows that q is monotonically increasing. In the iteration going from p to q, we do not get the true marginal probabilities at\neach level since we ignored the effect of the portion of the tree above.",
    "source": "book_fods",
    "section_title": "A"
  },
  {
    "instruction": "When or why would a data scientist use l = d−a?",
    "input": "",
    "output": "where A is the adjacency matrix and D is a diagonal matrix of degrees. Since A has a\nnegative sign, we look at the lowest two singular values and corresponding vectors rather\nthan the highest,\nL is a symmetric matrix and is easily seen to be posiitve semi-definite: for any vector\nx, we have\n(cid:88) (cid:88) 1 (cid:88)\nxTLx = d x2 − x x = (x −x )2.\nii i i j 2 i j\ni (i,j)∈E (i,j)∈E\nAlso since all row sums of L (and L is symmetric) are zero, its lowest eignvalue is 0 with\nthe eigenvector 1 of all 1’s. This is also the lowest singular vector of L. The projection\nof all data points (rows) to this vector is just the origin and so gives no information. If\nwe take the second lowest singular vector and project to it which is essentially projecting\nto the space of the bottom two singular vectors, we get the very simple problem of n real\nnumbers which we need to cluster into two clusters.",
    "source": "book_fods",
    "section_title": "L = D−A"
  },
  {
    "instruction": "Describe the typical steps involved in missing values and matrix completion 519 in a data science workflow.",
    "input": "",
    "output": "−3 −2 −1 0 1 2 3\nFIGURE 12.6. As described in the text, in each of 100 trials, we left out 20\nelements of the USArrests dataset. In each trial, we applied Algorithm 12.1 with\nM = 1 to impute the missing elements and compute the principal components. Left: For each of the 50 states, the imputed first principal component scores\n(averagedover100trials,anddisplayedwithastandarddeviationbar)areplotted\nagainst the first principal component scores computed using all the data. Right:\nTheimputedprincipalcomponentloadings(averagedover100trials,anddisplayed\nwith a standard deviation bar) are plotted against the true principal component\nloadings. jth movie.",
    "source": "book_islr",
    "section_title": "12.3 Missing Values and Matrix Completion 519"
  },
  {
    "instruction": "How is data science part 5 different from data science part 6 in data science?",
    "input": "",
    "output": "data science part 5: Cloud computing can offer access to large amounts of computational power and storage. In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks. data science part 6: Data science involves collecting, processing, and analyzing data which often includes personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts.",
    "source": "web_wikipedia",
    "section_title": "Data science part 5 vs Data science part 6"
  },
  {
    "instruction": "When or why would a data scientist use 2 r?",
    "input": "",
    "output": "3. Assuming that the eigenvectors of ATA are unique up to multiplicative constants,\nconclude that the singular vectors of A (which by definition must be unit length) are\nunique up to sign. Exercise 3.13 Let (cid:80) σ u vT be the singular value decomposition of a rank r matrix A.\ni i i\ni\nk\nLet A = (cid:80) σ u vT be a rank k approximation to A for some k < r. Express the following\nk i i i\ni=1\nquantities in terms of the singular values {σ ,1 ≤ i ≤ r}. i",
    "source": "book_fods",
    "section_title": "1 2 r"
  },
  {
    "instruction": "Explain 2 n in simple terms for a beginner in data science.",
    "input": "",
    "output": "situations where the sum of a large number of independent random variables arises. For\nexample, x may be the amount of a good that the ith consumer buys, the length of the ith\ni\nmessage sent over a network, or the indicator random variable of whether the ith record\nin a large database has a certain property. Each x is modeled by a simple probability\ni\ndistribution.",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "What is the case of p < 1/n in data science?",
    "input": "",
    "output": "When p < 1/n, then with high probability all components in G(n,p) are of size O(logn). This is easiest to see by considering a variation on the above dfs that (a) begins with\nF containing a specific start vertex u , and then (b) when a vertex u is taken from\nstart\nF to explore, it pops u off of F, explores u fully by querying to find all edges between\nu and U, and then pushes the endpoints v of those edges onto F. Thus, this is like an\nexplicit-stack version of dfs, compared to the previous recursive-call version of dfs. Let us\ncall the exploration of such a vertex u a step.",
    "source": "book_fods",
    "section_title": "8.3.3 The case of p < 1/n"
  },
  {
    "instruction": "Explain quadratic discriminant analysis in simple terms for a beginner in data science.",
    "input": "",
    "output": "We will now fit a QDA model to the Smarket data. QDA is implemented\nviaQuadraticDiscriminantAnalysis()inthesklearnpackage,whichweab-\nQuadratic\nbreviate to QDA(). The syntax is very similar to LDA().",
    "source": "book_islr",
    "section_title": "4.7.4 Quadratic Discriminant Analysis"
  },
  {
    "instruction": "Explain 2 p in simple terms for a beginner in data science.",
    "input": "",
    "output": "typically fits this model using least squares. In the chapters that follow, we consider some approaches for extending\nthe linear model framework. In Chapter 7 we generalize (6.1) in order to\naccommodate non-linear, but still additive, relationships, while in Chap-\nters 8 and 10 we consider even more general non-linear models.",
    "source": "book_islr",
    "section_title": "1 2 p"
  },
  {
    "instruction": "What is 2 r in data science?",
    "input": "",
    "output": "we can take the average of each cluster and that should be a good approximation to the\ncorresponding column of B. It would also suffice to find the sets S of terms, since from\nl\nthem we could read off the sets T of topics. We now formally state the document gen-\nl\neration process under the Pure Topic Assumption and the associated clustering problem.",
    "source": "book_fods",
    "section_title": "1 2 r"
  },
  {
    "instruction": "What is 6. linear model selection and regularization in data science?",
    "input": "",
    "output": "Applied\n8. In this exercise, we will generate simulated data, and will then use\nthis data to perform forward and backward stepwise selection. (a) Createarandomnumbergeneratoranduseitsnormal()method\nto generate a predictor X of length n = 100, as well as a noise\nvector \" of length n=100.",
    "source": "book_islr",
    "section_title": "286 6. Linear Model Selection and Regularization"
  },
  {
    "instruction": "When or why would a data scientist use 2?",
    "input": "",
    "output": "λ , plays an important role for matrices representing graphs. It may be the case that\n2\n|λ | > |λ |. n 2\nWhy is the second eigenvalue so important? Consider partitioning the vertices of a\nregular degree d graph G = (V,E) into two blocks of equal size so as to minimize the\nnumber of edges between the two blocks. Assign value +1 to the vertices in one block and\n-1 to the vertices in the other block.",
    "source": "book_fods",
    "section_title": "1 2"
  },
  {
    "instruction": "When or why would a data scientist use i?",
    "input": "",
    "output": "ones by the coin-flip approximate counting algorithm, in Section 6.2.2. Repeat the process\nfor p=1/4, 1/8, and 1/16. How close is the approximation? Counting Frequent Elements\nThe Majority and Frequent Algorithms\nThe Second Moment\nExercise 6.12\n1. Construct an example in which the majority algorithm gives a false positive, i.e.,\nstores a non majority element at the end.",
    "source": "book_fods",
    "section_title": "2 i"
  },
  {
    "instruction": "Explain i+1 1 2 i in simple terms for a beginner in data science.",
    "input": "",
    "output": "is at most (cid:80)n/1000 e−10k ≤ 1/n4. So, with probability at least 1−(1/n4), each S is\nk=20lnn i+1\nat least double the sum of the previous S ’s, which implies that in O(lnn) steps, i +1\nj 0\nis reached. Consider any other vertex w. We wish to find a short O(lnn) length path between\nv and w. By the same argument as above, the number of vertices at distance O(lnn)\nfrom w is at least n/1000.",
    "source": "book_fods",
    "section_title": "0 i+1 1 2 i"
  },
  {
    "instruction": "What is algorithm and analysis in data science?",
    "input": "",
    "output": "Before presenting an algorithm, we begin with a helpful lemma that will guide our\ndesign. For a given data point a , define its weight w(a ) to be its distance to the center\ni i\nof its cluster in C∗. Notice that the k-median cost of C∗ is OPT = (cid:80)n w(a ).",
    "source": "book_fods",
    "section_title": "7.6.3 Algorithm and Analysis"
  },
  {
    "instruction": "Explain logistic regression part 10 in simple terms for a beginner in data science.",
    "input": "",
    "output": "In machine learning applications where logistic regression is used for binary classification, the MLE minimises the cross-entropy loss function. Logistic regression is an important machine learning algorithm. The goal is to model the probability of a random variable \n\n Y\n\n {\\displaystyle Y}\n \n being 0 or 1 given experimental data.",
    "source": "web_wikipedia",
    "section_title": "Logistic regression part 10"
  },
  {
    "instruction": "When or why would a data scientist use s t s t s s?",
    "input": "",
    "output": "s=1 t=s+1 s=1\nm m m\n(cid:88) (cid:88) (cid:88)\n= 6 f2f2+ f4\ns t s\ns=1 t=s+1 s=1\n(cid:32)\nm\n(cid:33)2\n(cid:88)\n≤ 3 f2 = 3E2(a). s\ns=1\nTherefore, Var(a) = E(a2)−E2(a) ≤ 2E2(a). Sincethevarianceiscomparabletothesquareoftheexpectation,repeatingtheprocess\nseveral times and taking the average, gives high accuracy with high probability. Theorem 6.3 The average x of r = 2 estimates a ,...,a using independent sets of\nε2δ 1 r\n4-way independent random variables is\nVar(x)\nProb(|x−E(x)| > εE(x)) < ≤ δ. (cid:15)2E2(x)\nProof: The proof follows from the fact that taking the average of r independent repe-\ntitions reduces variance by a factor of r, so that Var(x) ≤ δε2E2(x), and then applying\nChebyshev’s inequality.",
    "source": "book_fods",
    "section_title": "2 s t s t s s"
  },
  {
    "instruction": "Explain c1,...,ck = x in simple terms for a beginner in data science.",
    "input": "",
    "output": "k=1\n0\nIn words, this formula says that we want to partition the observations into\nK clusters such that the total within-cluster variation, summed over all K\nclusters, is as small as possible. Solving (12.15) seems like a reasonable idea, but in order to make it\nactionable we need to define the within-cluster variation. There are many\npossible ways to define this concept, but by far the most common choice\ninvolves squared Euclidean distance.",
    "source": "book_islr",
    "section_title": "C1,...,CK = X"
  },
  {
    "instruction": "When or why would a data scientist use definitions and key theorems?",
    "input": "",
    "output": "Definition 5.1 Given a set S of examples and a concept class H, we say that S is\nshattered by H if for every A ⊆ S there exists some h ∈ H that labels all examples in A\nas positive and all examples in S \\A as negative. Definition 5.2 The VC-dimension of H is the size of the largest set shattered by H.\nFor example, there exist sets of four points in the plane that can be shattered by rect-\nangles with axis-parallel edges, e.g., four points at the vertices of a diamond (see Figure\n5.5). Given such a set S, for any A ⊆ S, there exists a rectangle with the points in A\ninside the rectangle and the points in S \\ A outside the rectangle. However, rectangles\nwith axis-parallel edges cannot shatter any set of five points. To see this, assume for\ncontradiction that there is a set of five points shattered by the family of axis-parallel\nrectangles.",
    "source": "book_fods",
    "section_title": "5.11.1 Definitions and Key Theorems"
  },
  {
    "instruction": "When or why would a data scientist use using pretrained cnn models?",
    "input": "",
    "output": "We now show how to use a CNN pretrained on the imagenet database to\nclassify natural images, and demonstrate how we produced Figure 10.10. We copied six JPEG images from a digital photo album into the direc-\ntorybook_images.Theseimagesareavailablefromthedatasectionofwww. statlearning.com, the ISLP book website. Download book_images.zip;\nwhen clicked it creates the book_images directory. The pretrained network we use is called resnet50; specification details\ncanbefoundontheweb.Wewillreadintheimages,andconverttheminto\nthearrayformatexpectedbythetorchsoftwaretomatchthespecifications\nin resnet50.",
    "source": "book_islr",
    "section_title": "10.9.4 Using Pretrained CNN Models"
  },
  {
    "instruction": "What is tv 0.0191 0.002 12.70 <0.0001 in data science?",
    "input": "",
    "output": "radio 0.0289 0.009 3.24 0.0014\nTV radio 0.0011 0.000 20.73 <0.0001\n×\nTABLE3.9. FortheAdvertisingdata,leastsquarescoefficientestimatesasso-\nciated with the regression of sales onto TV and radio, with an interaction term,\nas in (3.33). will depend on the number of workers, since if no workers are available\nto operate the lines, then increasing the number of lines will not increase\nproduction.Thissuggeststhatitwouldbeappropriatetoincludeaninter-\naction term between lines and workers in a linear model to predict units.",
    "source": "book_islr",
    "section_title": "TV 0.0191 0.002 12.70 <0.0001"
  },
  {
    "instruction": "What is k-means clustering in data science?",
    "input": "",
    "output": "K-means clustering is a simple and elegant approach for partitioning a\ndata set into K distinct, non-overlapping clusters. To perform K-means\nclustering,wemustfirstspecifythedesirednumberofclustersK;thenthe\nK-means algorithm will assign each observation to exactly one of the K\nclusters. Figure 12.7 shows the results obtained from performing K-means\nclustering on a simulated example consisting of 150 observations in two\ndimensions, using three different values of K.\nThe K-means clustering procedure results from a simple and intuitive\nmathematicalproblem.Webeginbydefiningsomenotation.LetC ,...,C",
    "source": "book_islr",
    "section_title": "12.4.1 K-Means Clustering"
  },
  {
    "instruction": "Explain 0 in simple terms for a beginner in data science.",
    "input": "",
    "output": "2WeexcludeC0(X)asapredictorin(7.5)becauseitisredundantwiththeintercept. Thisissimilartothefactthatweneedonlytwodummyvariablestocodeaqualitative\nvariablewiththreelevels,providedthatthemodelwillcontainanintercept.Thedecision\nto exclude C0(X) instead of some other Ck(X) in (7.5) is arbitrary. Alternatively, we\ncouldincludeC0(X),C1(X),...,CK(X),andexcludetheintercept.",
    "source": "book_islr",
    "section_title": "1 0"
  },
  {
    "instruction": "When or why would a data scientist use 2 n i?",
    "input": "",
    "output": "{1,2,...,m}, and want to determine whether there is some s ∈ {1,2,...,m} which oc-\ncurs more than n/2 times and if so which s. It is easy to see that to solve the problem\nexactlyonread-oncestreamingdatawithadeterministicalgorithm,requiresΩ(min(n,m))\nspace. Suppose n is even and the last n/2 items are identical. Suppose also that after\nreading the first n/2 items, there are two different sets of elements that result in the same\ncontent of our memory. In that case, a mistake would occur if the second half of the\nstream consists solely of an element that is in one set, but not in the other. If n/2 ≥ m\nthen there are at least 2m − 1 possible subsets of the first n/2 elements.",
    "source": "book_fods",
    "section_title": "1 2 n i"
  },
  {
    "instruction": "What is m in data science?",
    "input": "",
    "output": "x z φ . (12.5)\nij im jm\n≈\nm=1\n0\nWe can state this more formally by writing down an optimization prob-\nlem.SupposethedatamatrixXiscolumn-centered.Outofallapproxima-\ntions of the form x M a b , we could ask for the one with the\nij ≈ m=1 im jm\nsmallest residual sum of squares:\n)\np n M 2\nminimize x a b . (12.6)\nA ∈ Rn × M,B ∈ Rp × M 0 j=1 0 i=1 > ij − m 0 =1 im jm ?",
    "source": "book_islr",
    "section_title": "M"
  },
  {
    "instruction": "How is d, different from 8. tree-based methods in data science?",
    "input": "",
    "output": "d,: High,\ncv=validation)\nresults['test_score']\nOut[10]:array([0.685])\nNext,weconsiderwhetherpruningthetreemightleadtoimprovedclas-\nsification performance. We first split the data into a training and test set. 8. tree-based methods: cv=kfold,\nscoring='accuracy')\ngrid.fit(X_train, High_train)\ngrid.best_score_\nOut[14]:0.685\nLet’s take a look at the pruned true. In[15]: ax = subplots(figsize=(12, 12))[1]\nbest_ = grid.best_estimator_\nplot_tree(best_,\nfeature_names=feature_names,\nax=ax);\nThisisquiteabushytree.Wecouldcounttheleaves,orquerybest_instead.",
    "source": "book_islr",
    "section_title": "D, vs 358 8. Tree-Based Methods"
  },
  {
    "instruction": "What is 35 in data science?",
    "input": "",
    "output": "Name: status, dtype: int64\nBefore beginning an analysis, it is important to know how the status\nvariable has been coded. Most software uses the convention that a status\nof 1 indicates an uncensored observation (often death), and a status of 0\nindicatesacensoredobservation.Butsomescientistsmightusetheopposite\ncoding.Forthe BrainCancer dataset35 patientsdiedbefore theendof the\nstudy, so we are using the conventional coding. To begin the analysis, we re-create the Kaplan-Meier survival curve\nshown in Figure 11.2.",
    "source": "book_islr",
    "section_title": "1 35"
  },
  {
    "instruction": "Explain scenario 3 in simple terms for a beginner in data science.",
    "input": "",
    "output": "05.0\n54.0\n04.0\n53.0\n03.0\n52.0\n02.0\nFIGURE 4.11. Boxplots of the test error rates for each of the linear scenarios\ndescribed in the main text. Scenario 1:Therewere20trainingobservationsineachoftwoclasses.The\nobservations within each class were uncorrelated random normal variables\nwithadifferentmeanineachclass.Theleft-handpanelofFigure4.11shows\nthat LDA performed well in this setting, as one would expect since this is\nthe model assumed by LDA.",
    "source": "book_islr",
    "section_title": "SCENARIO 3"
  },
  {
    "instruction": "What is a perspective on symbolic machine learning in physical sciences in data science?",
    "input": "",
    "output": "Machine learning is rapidly making its pathway across all of the natural sciences, including physical sciences. The rate at which ML is impacting non-scientific disciplines is incomparable to that in the physical sciences. This is partly due to the uninterpretable nature of deep neural networks.",
    "source": "web_arxiv",
    "section_title": "A Perspective on Symbolic Machine Learning in Physical Sciences"
  },
  {
    "instruction": "How is y different from y 52 in data science?",
    "input": "",
    "output": "y: ∗\n3. The p-value is given by B b=1 1 ( |B T∗ b |≥| T | ). y 52: −\nis virtually indistinguishable from a N(0,1) distribution.) If we instead\napply Algorithm 13.3 with B =10,000, then we obtain a p-value of 0.042.",
    "source": "book_islr",
    "section_title": "Y vs Y 52"
  },
  {
    "instruction": "When or why would a data scientist use a re-sampling approach to p-values and false discovery rates 577?",
    "input": "",
    "output": "the m = 2,000 fund managers appear to have beaten the market without\nperforming correction for multiple testing — for instance, 13 of them have\np-values below0.001. By contrast,when the FDR is controlledat level0.3,\nwecanconcludethat279fundmanagersarebeatingthemarket:weexpect\nthatnomorethanaround279 0.3=83.7ofthesefundmanagershadgood\n×\nperformance only due to chance. Thus, we see that FDR control is much\nmilder — and more powerful — than FWER control, in the sense that it\nallows us to reject many more null hypotheses, with a cost of substantially\nmore false positives. The Benjamini–Hochberg procedure has been around since the mid-\n1990s.Whileagreatmanypapershavebeenpublishedsincethenproposing\nalternative approaches for FDR control that can perform better in partic-\nular scenarios, the Benjamini–Hochberg procedure remains a very useful\nand widely-applicable approach.",
    "source": "book_islr",
    "section_title": "13.5 A Re-Sampling Approach to p-Values and False Discovery Rates 577"
  },
  {
    "instruction": "How is 2 r different from 4 4 1−(3δ/4) in data science?",
    "input": "",
    "output": "2 r: from the last section. We know that a for catchword i of topic l, the maximum value of\np ,j = 1,2,...,n occurs for a pure document and indeed if the assumption above holds,\nij\nthe set of δn/4 documents with the top δn/4 values of p should be all pure documents. 4 4 1−(3δ/4): (cid:18) (cid:19)\n3\n≥ b 1− δ , (9.18)\nil(cid:48)\n4\nusing (9.15) and δ ≤ 0.4. From (9.18), it follows that\n(cid:18) (cid:19)\n3\ng(i,l(cid:48)) ≥ b 1− δ .",
    "source": "book_fods",
    "section_title": "1 2 r vs 4 4 4 1−(3δ/4)"
  },
  {
    "instruction": "What is 0 05− in data science?",
    "input": "",
    "output": "001−\nFirst Principal Component\ntnenopmoC\nlapicnirP dnoceS\n−0.5 0.0 0.5 1.0\n* * * * ** * * ** * * ** * ** * ** * * *** * * * * * * ** * * ** *** * * * * * * * * * *\n0.1\n5.0\n0.0\n5.0−\nUnscaled\nUrbanPop\nRape Murder Assault\nFIGURE 12.4. Two principal component biplots for the USArrests data. Left:\nthesameasFigure12.1,withthevariablesscaledtohaveunitstandarddeviations.",
    "source": "book_islr",
    "section_title": "05 0 05−"
  },
  {
    "instruction": "Explain 2 n in simple terms for a beginner in data science.",
    "input": "",
    "output": "do so with less data per function than one would need to learn each function separately. This is the idea of multi-task learning. One natural example is object recognition.",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "When or why would a data scientist use 20 40 60 80 100?",
    "input": "",
    "output": "52\n02\n51\n01\n5\nNewspaper\nselaS\nFIGURE2.1. TheAdvertisingdataset.Theplotdisplayssales,inthousands\nof units, as a function of TV, radio, and newspaper budgets, in thousands of\ndollars, for 200 different markets. In each plot we show the simple least squares\nfit of sales to that variable, as described in Chapter 3. In other words, each blue\nline represents a simple model that can be used to predict sales using TV, radio,\nand newspaper, respectively. Heref issomefixedbutunknownfunctionofX ,...,X ,and\"isarandom",
    "source": "book_islr",
    "section_title": "0 20 40 60 80 100"
  },
  {
    "instruction": "Explain alert-transformer: bridging asynchronous and synchronous machine learning for real-time event-based spatio-temporal data in simple terms for a beginner in data science.",
    "input": "",
    "output": "We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method. These embeddings are then processed by a transformer model trained for object and gesture recognition.",
    "source": "web_arxiv",
    "section_title": "ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data"
  },
  {
    "instruction": "When or why would a data scientist use b=1000,?",
    "input": "",
    "output": "seed=0)\nalpha_SE\nOut[19]:0.0912\nThe final output shows that the bootstrap estimate for SE(αˆ) is 0.0912. Estimating the Accuracy of a Linear Regression Model\nThe bootstrap approach can be used to assess the variability of the coef-\nficient estimates and predictions from a statistical learning method. Here\nwe use the bootstrap approach in order to assess the variability of the",
    "source": "book_islr",
    "section_title": "B=1000,"
  },
  {
    "instruction": "What is machine learning part 13 in data science?",
    "input": "",
    "output": "Automated machine learning – Process of automating the application of machine learning\nBig data – Extremely large or complex datasets\nDeep learning — branch of ML concerned with artificial neural networks\nDifferentiable programming – Programming paradigm\nList of datasets for machine-learning research\nList of machine learning algorithms and List of algorithms for machine learning and statistical classification\nM-theory (learning framework) – Framework in machine learning\nMachine unlearning – Field of study in artificial intelligence\nOutline of machine learning\nSolomonoff's theory of inductive inference – Mathematical theory",
    "source": "web_wikipedia",
    "section_title": "Machine learning part 13"
  },
  {
    "instruction": "How is semi-supervised learning different from 2 1 2 1 1 2 2 in data science?",
    "input": "",
    "output": "semi-supervised learning: Semi-supervised learning refers to the idea of trying to use a large unlabeled data set U to\naugment a given labeled data set L in order to produce more accurate rules than would\nhave been achieved using just L alone. The motivation is that in many settings (e.g.,\ndocument classification, image classification, speech recognition), unlabeled data is much\nmore plentiful than labeled data, so one would like to make use of it if possible. 2 1 2 1 1 2 2: otherwise; err (h) is then the probability mass of examples on which h and h disagree. unl 1 2\n172\nAs with the class H, one can either assume that the target is fully compatible (i.e.,\nerr (c∗) = 0) or instead aim to do well as a function of how compatible the target is.",
    "source": "book_fods",
    "section_title": "5.16.1 Semi-Supervised Learning vs 1 2 1 2 1 1 2 2"
  },
  {
    "instruction": "When or why would a data scientist use 6. linear model selection and regularization?",
    "input": "",
    "output": "(6.7) was small; however, when p was larger then the lowest validation\nset error was achieved using a larger value of λ. In each boxplot, rather\nthan reporting the values of λ used, the degrees of freedom of the resulting\nlassosolutionisdisplayed;thisissimplythenumberofnon-zerocoefficient\nestimates in the lasso solution, and is a measure of the flexibility of the\nlasso fit. Figure 6.24 highlights three important points: (1) regularization\norshrinkageplaysakeyroleinhigh-dimensionalproblems,(2)appropriate\ntuning parameter selection is crucial for good predictive performance, and\n(3) the test error tends to increase as the dimensionality of the problem\n(i.e. the number of features or predictors) increases, unless the additional\nfeatures are truly associated with the response. The third point above is in fact a key principle in the analysis of high-\ndimensionaldata,whichisknownasthecurseofdimensionality.Onemight\ncurseofdi-\nthink that as the number of features used to fit a model increases, the\nmensionality\nquality of the fitted model will increase as well.",
    "source": "book_islr",
    "section_title": "266 6. Linear Model Selection and Regularization"
  },
  {
    "instruction": "Explain logistic regression part 6 in simple terms for a beginner in data science.",
    "input": "",
    "output": "There are various equivalent specifications and interpretations of logistic regression, which fit into different types of more general models, and allow different generalizations. === As a generalized linear model ===\nThe particular model used by logistic regression, which distinguishes it from standard linear regression and from other types of regression analysis used for binary-valued outcomes, is the way the probability of a particular outcome is linked to the linear predictor function:\n\n logit\n ⁡\n (\n\n E\n\n ⁡\n [\n \n Y\n \n i\n\n ∣\n \n x\n \n 1\n ,\n i\n\n ,\n …\n ,\n \n x\n \n m\n ,\n i\n\n ]\n )\n =\n logit\n ⁡\n (\n \n p\n \n i\n\n )\n =\n ln\n ⁡\n \n (\n\n p\n \n i\n\n 1\n −\n \n p\n \n i\n\n )\n \n =\n \n β\n \n 0\n\n +\n \n β\n \n 1\n\n x\n \n 1\n ,\n i\n\n +\n ⋯\n +\n \n β\n \n m\n\n x\n \n m\n ,\n i\n\n {\\displaystyle \\operatorname {logit} (\\operatorname {\\mathbb {E} } [Y_{i}\\mid x_{1,i},\\ldots ,x_{m,i}])=\\operatorname {logit} (p_{i})=\\ln \\left({\\frac {p_{i}}{1-p_{i}}}\\right)=\\beta _{0}+\\beta _{1}x_{1,i}+\\cdots +\\beta _{m}x_{m,i}}\n\nWritten using the more compact notation described above, this is:\n\n logit\n ⁡\n (\n\n E\n\n ⁡\n [\n \n Y\n \n i\n\n ∣\n\n X\n\n i\n\n ]\n )\n =\n logit\n ⁡\n (\n \n p\n \n i\n\n )\n =\n ln\n ⁡\n \n (\n\n p\n \n i\n\n 1\n −\n \n p\n \n i\n\n )\n \n =\n \n β\n \n ⋅\n\n X\n\n i\n\n {\\displaystyle \\operatorname {logit} (\\operatorname {\\mathbb {E} } [Y_{i}\\mid \\mathbf {X} _{i}])=\\operatorname {logit} (p_{i})=\\ln \\left({\\frac {p_{i}}{1-p_{i}}}\\right)={\\boldsymbol {\\beta }}\\cdot \\mathbf {X} _{i}}\n\nThis formulation expresses logistic regression as a type of generalized linear model, which predicts variables with various types of probability distributions by fitting a linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable. The intuition for transforming using the logit function (the natural log of the odds) was explained above.",
    "source": "web_wikipedia",
    "section_title": "Logistic regression part 6"
  },
  {
    "instruction": "How is 2 m i different from 2 n in data science?",
    "input": "",
    "output": "2 m i: is evaluated on the variables in subset S . The problem is to minimize\n(cid:81)m\nf (y ,j ∈ S )\ni i=1 i j i\nsubject to constrained values. 2 n: (cid:81)\ncan be expressed as f (x) = f (x ) where each factor depends only on some small\nα α\nα\nnumber of variables x . The difference from Markov random fields is that the variables\nα\ncorresponding to factors do not necessarily form a clique.",
    "source": "book_fods",
    "section_title": "1 2 m i vs 1 2 n"
  },
  {
    "instruction": "When or why would a data scientist use 10 15 20?",
    "input": "",
    "output": "02\n51\n01\n5\n0\nObservation 1\nObservation 2\nObservation 3\n2\n3\n1\nVariable Index\nFIGURE 12.15. Three observations with measurements on 20 variables are\nshown. Observations 1 and 3 have similar values for each variable and so there\nis a small Euclidean distance between them. But they are very weakly correlated,\nso they have a large correlation-based distance. On the other hand, observations\n1 and 2 have quite different values for each variable, and so there is a large\nEuclidean distance between them.",
    "source": "book_islr",
    "section_title": "5 10 15 20"
  },
  {
    "instruction": "What is 3 6 in data science?",
    "input": "",
    "output": "delete row 5 of C, add 4 times column 5 of B to column 3 of B, add 3 times column 5\nof B to column 6 of B, and delete column 5 of B. After repeating this, each row of C is\npositively independent of the other rows of C, i.e., it cannot be expressed as a nonnegative\nlinear combination of the other rows. If A = BC is a NMF of A and there are rows in A that are positive linear combinations\nof other rows, the rows can be remove and the corresponding rows of B remove to give a\nˆ ˆ ˆ ˆ\nNMF A = BC where A and B are the matrices A and B with the removed rows.",
    "source": "book_fods",
    "section_title": "5 3 6"
  },
  {
    "instruction": "Explain probability in simple terms for a beginner in data science.",
    "input": "",
    "output": "sifier is called the Bayes classifier. In a two-class problem where there are\nBayes\nonlytwopossibleresponsevalues,sayclass1orclass2,theBayesclassifier\nclassifier\ncorresponds to predicting class one if Pr(Y = 1X = x ) > 0.5, and class\n0\n|\ntwo otherwise. Figure 2.13 provides an example using a simulated data set in a two-\ndimensional space consisting of predictors X and X .",
    "source": "book_islr",
    "section_title": "0 probability"
  },
  {
    "instruction": "Explain p in simple terms for a beginner in data science.",
    "input": "",
    "output": "hood gives us (up to a proportionality constant) the posterior distribution,\nposterior\nwhich takes the form\ndistribution\np(β X,Y) f(Y X,β)p(β X)=f(Y X,β)p(β),\n| ∝ | | |\nwhere the proportionality above follows from Bayes’ theorem, and the\nequality above follows from the assumption that X is fixed. We assume the usual linear model,\nY =β +X β + +X β +\",",
    "source": "book_islr",
    "section_title": "1 p"
  },
  {
    "instruction": "What is ··· k in data science?",
    "input": "",
    "output": "variance term across all K classes, which for simplicity we can denote by\nσ2. Plugging (4.16) into (4.15), we find that\nπ 1 exp 1 (x µ )2\np (x)= k√2πσ −2σ2 − k . (4.17)\nk K\nl=1\nπ\nl√2\n1\nπσ\ne’xp\n−2σ\n1\n2\n(x\n−\nµ l()2\n(Note that in (4.17), π)denotes the pr’ior probability (that an observation\nk\nbelongs to the kth class, not to be confused with π 3.14159, the math-\n≈\nematical constant.)",
    "source": "book_islr",
    "section_title": "1 ··· K"
  },
  {
    "instruction": "What is j in data science?",
    "input": "",
    "output": "(c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients\nfrom (a) on the x-axis, and the multiple regression coefficients\nfrom (b) on the y-axis. That is, each predictor is displayed as a\nsingle point in the plot.",
    "source": "book_islr",
    "section_title": "0 j"
  },
  {
    "instruction": "Explain 2 n in simple terms for a beginner in data science.",
    "input": "",
    "output": "holds after the change, it must hold before. By continuing this process, one can make all\nthe a equal. i\nApproximating sums by integrals\n417\nn+1 n n\n(cid:82) (cid:80) (cid:82)\nf (x)dx ≤ f (i) ≤ f (x)dx\nx=m i=m x=m−1\nm−1 m n n+1\nFigure 12.1: Approximating sums by integrals\nFor monotonic decreasing f(x),\nn+1 n\n(cid:90) n (cid:90)\n(cid:88)\nf (x)dx ≤ f (i) ≤ f (x)dx.",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "Explain lab: linear models and regularization methods 271 in simple terms for a beginner in data science.",
    "input": "",
    "output": "ax.set_ylim([50000,250000]);\nNotice the expression None in Y[:,None] above. This adds an axis (dimen-\nsion) to the one-dimensional array Y, which allows it to be recycled when\nsubtracted from the two-dimensional Yhat_in. We are now ready to use cross-validation to estimate test error along\nthe model path.",
    "source": "book_islr",
    "section_title": "6.5 Lab: Linear Models and Regularization Methods 271"
  },
  {
    "instruction": "When or why would a data scientist use 4.5 24?",
    "input": "",
    "output": "FIGURE 8.2. The three-region partition for the Hitters data set from the\nregression tree illustrated in Figure 8.1.\ntheregionsasafunctionofYearsandHits.Thepredictedsalariesforthese\nthree groups are $1,000 e5.107 =$165,174, $1,000 e5.999 =$402,834, and\n× ×\n$1,000 e6.740 =$845,346 respectively. ×\nInkeepingwiththetreeanalogy,theregionsR ,R ,andR areknownas",
    "source": "book_islr",
    "section_title": "1 4.5 24"
  },
  {
    "instruction": "Describe the typical steps involved in 1 n n in a data science workflow.",
    "input": "",
    "output": "a classifier. We want our classifier to perform well not only on the training\ndata,butalsoontestobservationsthatwerenotusedtotraintheclassifier. In this chapter, we will illustrate the concept of classification using the\nsimulated Default data set. We are interested in predicting whether an\nindividual will default on his or her credit card payment, on the basis of\nannual income and monthly credit card balance. The data set is displayed\nin Figure 4.1. In the left-hand panel of Figure 4.1, we have plotted annual\nincome and monthly credit card balance for a subset of 10,000 individuals.",
    "source": "book_islr",
    "section_title": "1 1 n n"
  },
  {
    "instruction": "What is 2 n i=1 i i=1 i in data science?",
    "input": "",
    "output": "lemma is proved. 445\nThere are two important matrix norms, the matrix p-norm\n||A|| = max (cid:107)Ax(cid:107)\np p\n|x|=1\nand the Frobenius norm\n(cid:115)\n(cid:88)\n||A|| = a2 . F ij\nij\nLet a be the ith column of A.",
    "source": "book_fods",
    "section_title": "1 2 n i=1 i i=1 i"
  },
  {
    "instruction": "How is c different from definitions and key theorems in data science?",
    "input": "",
    "output": "c: (a) (b)\nFigure 5.5: (a) shows a set of four points that can be shattered by rectangles along with\nsome of the rectangles that shatter the set. Not every set of four points can be shattered\nas seen in (b). definitions and key theorems: Definition 5.1 Given a set S of examples and a concept class H, we say that S is\nshattered by H if for every A ⊆ S there exists some h ∈ H that labels all examples in A\nas positive and all examples in S \\A as negative. Definition 5.2 The VC-dimension of H is the size of the largest set shattered by H.\nFor example, there exist sets of four points in the plane that can be shattered by rect-\nangles with axis-parallel edges, e.g., four points at the vertices of a diamond (see Figure\n5.5).",
    "source": "book_fods",
    "section_title": "C vs 5.11.1 Definitions and Key Theorems"
  },
  {
    "instruction": "How is bagging, random forests, boosting, and different from bagging in data science?",
    "input": "",
    "output": "bagging, random forests, boosting, and: Bayesian Additive Regression Trees\nAn ensemble method is an approach that combines many simple “building\nensemble\nblock” models in order to obtain a single and potentially very powerful\nmodel. These simple building block models are sometimes known as weak\nlearners, since they may lead to mediocre predictions on their own. bagging: Thebootstrap,introducedinChapter5,isanextremelypowerfulidea.Itis\nused in many situations in which it is hard or even impossible to directly\ncompute the standard deviation of a quantity of interest. We see here that\nthe bootstrap can be used in a completely different context, in order to\nimprove statistical learning methods such as decision trees.",
    "source": "book_islr",
    "section_title": "8.2 Bagging, Random Forests, Boosting, and vs 8.2.1 Bagging"
  },
  {
    "instruction": "How is j different from exercises 131 in data science?",
    "input": "",
    "output": "j: (e) On the basis of your response to the previous question, fit a\nsmaller model that only uses the predictors for which there is\nevidence of association with the outcome. (f) How well do the models in (a) and (e) fit the data? exercises 131: (b) Now perform a simple linear regression of x onto y without an\nintercept,andreportthecoefficientestimate,itsstandarderror,\nand the corresponding t-statistic and p-values associated with\nthe null hypothesis H :β =0. Comment on these results.",
    "source": "book_islr",
    "section_title": "0 j vs 3.7 Exercises 131"
  },
  {
    "instruction": "How is index different from get_influence(), 121 in data science?",
    "input": "",
    "output": "index: multivariate normal, 150 overfitting, 21, 23, 25, 30–31, 88,\n152, 233, 371\nnaive Bayes, 135, 158–161, 164–\n167\np-value,77,82,560–562,578–579\nnamespace, 116\nadjusted, 586\nnatural spline, 297, 298, 301, 317\npackage, 42\nNCI60dataset,4,5,12,546,548–\nparameter, 71\n550\nparametric, 20–22, 111–115\nnegative binomial, 173\npartialleastsquares,254,260–262,\nnegativepredictivevalue,155,156\n282\nneural network, 5, 399\npartial likelihood, 480\nnode\npath algorithm, 249\ninternal, 333\npermutation, 578\npurity, 337–339\npermutation approach, 577–582\nterminal, 333\nperpendicular, 257\nnoise, 21, 252\nPoisson distribution, 169, 172\nnon-linear, 2, 11, 289–329\nPoisson regression, 135, 167–173\ndecision boundary, 377–382\npolynomial\nkernel, 377–382\nkernel, 380, 382\nnon-parametric, 20, 22–23, 111–\nregression,98–99,289–292,294–\n115, 193\n295\nnormal(Gaussian)distribution,146,\npooling, 410\n147, 150, 172, 476, 561\npopulation regression line, 73\nnotebook, 40\nPortfolio data set, 12\nnull, 152\npositivepredictivevalue,155,156\ndistribution, 561, 578\nposterior\nhypothesis, 76, 559\ndistribution, 251\nmodel, 87, 231, 245\nmode, 251\nnull rate, 186\nprobability, 147\nNYSE data set, 12, 422–424, 466,\npower, 108, 155, 563\n467\nprecision, 155\nOccam’s razor, 426 prediction, 17\nodds, 140, 145, 195 interval, 90, 110\nOJ data set, 12, 365, 398 predictor, 15\none-hot encoding, 92, 126, 403 principal components, 505\none-standard-error rule, 240 analysis,11,254–260,504–515\none-versus-all, 384 loading vector, 505, 506\none-versus-one, 384 missing values, 515–520\none-versus-rest, 384 proportionofvarianceexplained,\noptimalseparatinghyperplane,370 510–515, 547\noptimism of training error, 30 regression,11,254–260,280–\nordered categorical variable, 315 282, 504, 515\northogonal, 257, 506 score vector, 506\nbasis, 125 scree plot, 514–515\nout-of-bag, 345 prior\noutlier, 103–104 distribution, 251\noutput variable, 15 probability, 146\nover-parametrized, 465 probability density function, 477,\noverdispersion, 172 478\nIndex 603\nprojection, 230 drop(), 179\nproportionalhazardsassumption, dropna(), 56, 268, 461 get_influence(), 121: axhline(), 122, 551 get_prediction(),120,314\naxline(), 121, 129, 329 get_rdataset(), 535\nBART(), 362 glm(), 313\nbiplot, 537 glob(), 437\nboot_SE(), 223 GradientBoostingClassifier(),\nboxplot(), 62, 66 361\nbs(), 315, 327 GradientBoostingRegressor(),\nBSpline(), 315 354, 361\nclone(), 222 GridSearchCV(), 276\ncolumns.drop(), 122 groupby(), 490\ncompute_linkage(), 544 hist(), 62\nconfusion_table(), 176 iloc[], 58, 59\ncontour(), 50 import, 42\ncorr(), 129, 174 imshow(), 50, 449\ncost_complexity_pruning_path(), ISLP.bart, 362",
    "source": "book_islr",
    "section_title": "602 Index vs 313 get_influence(), 121"
  },
  {
    "instruction": "How is i different from graphs with a single cycle in data science?",
    "input": "",
    "output": "i: suffice to do a second pass from the root to the leaves of essentially the same message\npassing algorithm to get all the answers. Recall that in the first pass, each edge of the\ntree has sent a message “up”, from the child to the parent. graphs with a single cycle: The message passing algorithm gives the correct answers on trees and on certain other\ngraphs. One such situation is graphs with a single cycle which we treat here.",
    "source": "book_fods",
    "section_title": "1 i vs 9.17 Graphs with a Single Cycle"
  },
  {
    "instruction": "How is r2 different from r2 0.612 in data science?",
    "input": "",
    "output": "r2: statistic. 4In Table 3.1, a small p-value for the intercept indicates that we can reject the null\nhypothesisthatβ0=0,andasmallp-valueforTVindicatesthatwecanrejectthenull\nhypothesis that β1 =0. r2 0.612: F-statistic 312.1\nTABLE 3.2. For the Advertising data, more information about the least\nsquaresmodelfortheregressionofnumberofunitssoldonTVadvertisingbudget.",
    "source": "book_islr",
    "section_title": "R2 vs R2 0.612"
  },
  {
    "instruction": "What is (cid:90) ∞ in data science?",
    "input": "",
    "output": "|E(ys)| = 1+ √ 2szs−(1/2)e−zdz\ni π\n0\n≤ 2ss!. The last inequality is from the Gamma integral. 24\nSince E(y ) = 0, Var(y ) = E(y2) ≤ 222 = 8.",
    "source": "book_fods",
    "section_title": "1 (cid:90) ∞"
  },
  {
    "instruction": "How is random forest part 1 different from random forest part 2 in data science?",
    "input": "",
    "output": "random forest part 1: Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that works by creating a multitude of decision trees during training. For classification tasks, the output of the random forest is the class selected by most trees. random forest part 2: The general method of random decision forests was first proposed by Salzberg and Heath in 1993, with a method that used a randomized decision tree algorithm to create multiple trees and then combine them using majority voting. This idea was developed further by Ho in 1995.",
    "source": "web_wikipedia",
    "section_title": "Random forest part 1 vs Random forest part 2"
  },
  {
    "instruction": "How is 35 different from lab: survival analysis 491 in data science?",
    "input": "",
    "output": "35: Name: status, dtype: int64\nBefore beginning an analysis, it is important to know how the status\nvariable has been coded. Most software uses the convention that a status\nof 1 indicates an uncensored observation (often death), and a status of 0\nindicatesacensoredobservation.Butsomescientistsmightusetheopposite\ncoding.Forthe BrainCancer dataset35 patientsdiedbefore theendof the\nstudy, so we are using the conventional coding. lab: survival analysis 491: the lifelines.statistics module. The first two arguments are the event\ntimes, with the second denoting the corresponding (optional) censoring\nindicators.",
    "source": "book_islr",
    "section_title": "1 35 vs 11.8 Lab: Survival Analysis 491"
  },
  {
    "instruction": "How is k different from 1 in data science?",
    "input": "",
    "output": "k: setting, repeat the calculation in (4.32), and provide expressions for\na and b in terms of π , π , µ , µ , and σ2. k kj k K k K\n11. 1: −\nthe coefficient estimates in your friend’s model? Be as specific\nas possible.",
    "source": "book_islr",
    "section_title": "1 K vs 0 1"
  },
  {
    "instruction": "When or why would a data scientist use 0?",
    "input": "",
    "output": "| | ≥\nwhat if we wish to test 10 null hypotheses using two-sample t-statistics,\ninstead of just one? We will see in Section 13.3.2 that we can guarantee\nthat the FWER does not exceed 0.02 by rejecting only null hypotheses\nfor which the p-value falls below 0.002. This corresponds to a much more\nstringent cutpoint of 3.09 (i.e. we should reject H only if its test statistic\n0j\nT 3.09, for j = 1,...,10). In other words, controlling the FWER at\nj\n| | ≥\nlevel α amounts to a much higher bar, in terms of evidence required to\nreject any given null hypothesis, than simply controlling the Type I error\nfor each null hypothesis at level α.",
    "source": "book_islr",
    "section_title": "0 0"
  },
  {
    "instruction": "What is lab: deep learning 447 in data science?",
    "input": "",
    "output": "In[42]: mnist_results = pd.read_csv(mnist_logger.experiment. metrics_file_path)\nfig, ax = subplots(1, 1, figsize=(6, 6))\nsummary_plot(mnist_results,\nax,\ncol='accuracy',\nylabel='Accuracy')\nax.set_ylim([0.5, 1])\nax.set_ylabel('Accuracy')\nax.set_xticks(np.linspace(0, 30, 7).astype(int));\nOnce again we evaluate the accuracy using the test() method of our\ntrainer. This model achieves 97% accuracy on the test data.",
    "source": "book_islr",
    "section_title": "10.9 Lab: Deep Learning 447"
  },
  {
    "instruction": "How is 2 n different from 2 n i i in data science?",
    "input": "",
    "output": "2 n: By the central limit theorem, in the limit the probability density of x goes to that of\nthe Gaussian with variance at most nσ2. In a limit sense, this implies an upper bound\nof ce−a2/(2nσ2) for the tail probability Pr(|x| > a) for some constant c. The following\ntheorem assumes bounds on higher moments, and asserts a quantitative upper bound of\n3e−a2/(12nσ2) on the tail probability, not just in the limit, but for every n. We will apply\nthis theorem to get tail bounds on sums of Gaussian, binomial, and power law distributed\nrandom variables. 2 n i i: two. There are at most r/2 nonzero r in each set.",
    "source": "book_fods",
    "section_title": "1 2 n vs 1 2 n i i"
  },
  {
    "instruction": "How is simple linear regression different from 3. linear regression in data science?",
    "input": "",
    "output": "simple linear regression: In this section we will construct model matrices (also called design matri-\nces) using the ModelSpec() transform from ISLP.models. We will use the Boston housing data set, which is contained in the ISLP\npackage. 3. linear regression: Type Boston? to find out more about these data.",
    "source": "book_islr",
    "section_title": "3.6.2 Simple Linear Regression vs 118 3. Linear Regression"
  },
  {
    "instruction": "What is 10 15 20 in data science?",
    "input": "",
    "output": "02\n51\n01\n5\n0\nObservation 1\nObservation 2\nObservation 3\n2\n3\n1\nVariable Index\nFIGURE 12.15. Three observations with measurements on 20 variables are\nshown. Observations 1 and 3 have similar values for each variable and so there\nis a small Euclidean distance between them.",
    "source": "book_islr",
    "section_title": "5 10 15 20"
  },
  {
    "instruction": "Explain exercises 285 in simple terms for a beginner in data science.",
    "input": "",
    "output": "5. It is well-known that ridge regression tends to give similar coefficient\nvalues to correlated variables, whereas the lasso may give quite dif-\nferent coefficient values to correlated variables. We will now explore\nthis property in a very simple setting.",
    "source": "book_islr",
    "section_title": "6.6 Exercises 285"
  },
  {
    "instruction": "How is 2 different from support-vector machines in data science?",
    "input": "",
    "output": "2: Theorem 5.13 (Online to Batch via Controlled Testing) LetAbeanonlinelearn-\ning algorithm with mistake-bound M. Then this procedure will halt after O(M log(M))\n(cid:15) δ\nexamples and with probability at least 1−δ will produce a hypothesis of error at most (cid:15). Notethatinthisconversionwecannotre-useoursamples: sincethehypothesish depends\nt\non the previous data, we need to draw a fresh set of n examples to use for testing it. support-vector machines: In a batch setting, rather than running the Perceptron algorithm and adapting it via\none of the methods above, another natural idea would be just to solve for the vector w\nthat minimizes the right-hand-side in Theorem 5.11 on the given dataset S. This turns\nout to have good guarantees as well, though they are beyond the scope of this book. In\nfact, this is the Support Vector Machine (SVM) algorithm.",
    "source": "book_fods",
    "section_title": "1 2 vs 5.10 Support-Vector Machines"
  },
  {
    "instruction": "When or why would a data scientist use euclideandistance,509,522,523,?",
    "input": "",
    "output": "529–532, 554\nWage, 1, 2, 8, 9, 12, 290, 291,\nevent time, 470\n293, 295, 297–300, 302–\nexception, 45\n306, 309, 315, 327\nexpected value, 18\nWeekly, 12, 196, 226\nexploratory data analysis, 504\ndata type, 42\nexponential, 173\ndecision function, 387\nexponential family, 173\ndecision tree, 11, 331–342\ndeep learning, 399 F-statistic, 84",
    "source": "book_islr",
    "section_title": "519 Euclideandistance,509,522,523,"
  },
  {
    "instruction": "What is 0 in data science?",
    "input": "",
    "output": "that is, it is the collection of all sets in Rd that are linearly separable from their comple-\nment. InthecasethatX isthesetof4pointsintheplane{(−1,−1),(−1,1),(1,−1),(1,1)},\nthe class of linear separators contains 14 of the 24 = 16 possible subsets of X.17 Given\nan hypothesis class H and training set S, what we typically aim to do algorithmically\nis to find the hypothesis in H that most closely agrees with c∗ over S. For example, we\nsaw that the Perceptron algorithm will find a linear separator that agrees with the target\nfunction over S so long as S is linearly separable. To address overfitting, we argue that if\nS is large enough compared to some property of H, then with high probability all h ∈ H\nhave their training error close to their true error, so that if we find a hypothesis whose\ntraining error is low, we can be confident its true error will be low as well.",
    "source": "book_fods",
    "section_title": "0 0"
  },
  {
    "instruction": "Explain 1 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Since (cid:80) x2 = 1, the set S = {i|α ≤ x < 2α} has |S| ≤ 1 and similarly,\ni i α2\ni\nT = {j|β ≤ y ≤ 2β} has |T| ≤ 1 . Thus\nj β2\n(cid:88) (cid:88)\nx y a ≤ 4αβA(S,T)\ni j ij\ni j\nα≤xi≤2α β≤yj≤2β\n(cid:112)\n≤ 4αβd(S,T) |S||T|\n≤ 4d(S,T)\n≤ 4d(A). From this it follows that\nσ (A) ≤ 4d(A)lognlogd\n1\nor\nd(A) ≥ σ1(A)\n4lognlogd\nproving the second inequality.",
    "source": "book_fods",
    "section_title": "4 1"
  },
  {
    "instruction": "When or why would a data scientist use lab: tree-based methods?",
    "input": "",
    "output": "We import some of our usual libraries at this top level. In[1]: import numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\nfrom statsmodels.datasets import get_rdataset\nimport sklearn.model_selection as skm\nfrom ISLP import load_data, confusion_table\nfrom ISLP.models import ModelSpec as MS\nWe also collect the new imports needed for this lab. In[2]: from sklearn.tree import (DecisionTreeClassifier as DTC,\nDecisionTreeRegressor as DTR,\nplot_tree,\nexport_text)\nfrom sklearn.metrics import (accuracy_score,\nlog_loss)\nfrom sklearn.ensemble import \\\n(RandomForestRegressor as RF,\nGradientBoostingRegressor as GBR)\nfrom ISLP.bart import BART",
    "source": "book_islr",
    "section_title": "8.3 Lab: Tree-Based Methods"
  },
  {
    "instruction": "When or why would a data scientist use 2994.0 4.770e+06 1.0 1282.563 0.805 3.697e-01?",
    "input": "",
    "output": "Notice the * in the anova_lm() line above. This function takes a variable\nnumber of non-keyword arguments, in this case fitted models. When these\nmodels are provided as a list (as is done here), it must be prefixed by *. The p-value comparing the linear models[0] to the quadratic models[1]\nisessentiallyzero,indicatingthatalinearfitisnotsufficient.8 Similarlythe\np-value comparing the quadratic models[1] to the cubic models[2] is very\nlow(0.0017),sothequadraticfitisalsoinsufficient.Thep-valuecomparing\nthecubicanddegree-fourpolynomials,models[2]andmodels[3],isapprox-\nimately 5%, while the degree-five polynomial models[4] seems unnecessary\nbecause its p-value is 0.37. Hence, either a cubic or a quartic polynomial\nappear to provide a reasonable fit to the data, but lower- or higher-order\nmodels are not justified.",
    "source": "book_islr",
    "section_title": "4 2994.0 4.770e+06 1.0 1282.563 0.805 3.697e-01"
  },
  {
    "instruction": "Describe the typical steps involved in small world graphs in a data science workflow.",
    "input": "",
    "output": "In the 1960’s, Stanley Milgram carried out an experiment that indicated that most\npairs of individuals in the United States were connected by a short sequence of acquain-\ntances. Milgram would ask a source individual, say in Nebraska, to start a letter on its\njourney to a target individual in Massachusetts. The Nebraska individual would be given\nbasic information about the target including his address and occupation and asked to\nsend the letter to someone he knew on a first name basis, who was closer to the target\nindividual, in order to transmit the letter to the target in as few steps as possible. Each\n294\nperson receiving the letter would be given the same instructions. In successful experi-\nments, it would take on average five to six steps for a letter to reach its target. This\nresearch generated the phrase “six degrees of separation” along with substantial research\nin social science on the interconnections between people.",
    "source": "book_fods",
    "section_title": "8.10 Small World Graphs"
  },
  {
    "instruction": "How is . ||a−a ||2 different from 0 0 ··· 0 0 in data science?",
    "input": "",
    "output": ". ||a−a ||2: k 2\nExercise 3.14 If A is a symmetric matrix with distinct singular values, show that the\nleft and right singular vectors are the same and that A = VDVT. Exercise 3.15 Let A be a matrix. 0 0 ··· 0 0: 69\n2. Modify the power method to find the first four singular vectors of a matrix A as\nfollows.",
    "source": "book_fods",
    "section_title": "4. ||A−A ||2 vs 10 0 0 ··· 0 0"
  },
  {
    "instruction": "When or why would a data scientist use 1?",
    "input": "",
    "output": "(f) Dotheresultsobtainedin(c)–(e)contradicteachother?Explain\nyour answer. (g) Supposeweobtainoneadditionalobservation,whichwasunfor-\ntunatelymismeasured.Weusethefunctionnp.concatenate()to\nnp.conca-\nadd this additional observation to each of x1, x2 and y. tenate()\nx1 = np.concatenate([x1, [0.1]])\nx2 = np.concatenate([x2, [0.8]])\ny = np.concatenate([y, [6]])\nRe-fitthelinearmodelsfrom(c)to(e)usingthisnewdata.What\neffectdoesthisnewobservationhaveontheeachofthemodels? In each model, is this observation an outlier? A high-leverage\npoint? Both?",
    "source": "book_islr",
    "section_title": "0 1"
  },
  {
    "instruction": "What is m in data science?",
    "input": "",
    "output": "model simply predicts the sample mean for each observation. 2. For k =1,2,...p:\n(a) Fit all p models that contain exactly k predictors.",
    "source": "book_islr",
    "section_title": "M"
  },
  {
    "instruction": "Explain 4. classification in simple terms for a beginner in data science.",
    "input": "",
    "output": "We close with a brief discussion of K-nearest neighbors (KNN), intro-\nducedinChapter2.RecallthatKNNtakesacompletelydifferentapproach\nfrom the classifiers seen in this chapter. In order to make a prediction for\nan observation X = x, the training observations that are closest to x are\nidentified. Then X is assigned to the class to which the plurality of these\nobservationsbelong.HenceKNNisacompletelynon-parametricapproach:\nno assumptions are made about the shape of the decision boundary.",
    "source": "book_islr",
    "section_title": "164 4. Classification"
  },
  {
    "instruction": "How is lab: deep learning 461 different from 10. deep learning in data science?",
    "input": "",
    "output": "lab: deep learning 461: Next we set up the lagged versions of the data, dropping any rows with\nmissing values using the dropna() method. In[94]: for lag in range(1, 6):\nfor col in cols:\nnewcol = np.zeros(X.shape[0]) * np.nan\nnewcol[lag:] = X[col].values[:-lag]\nX.insert(len(X.columns), \"{0}_{1}\".format(col, lag), newcol)\nX.insert(len(X.columns), 'train', NYSE['train'])\nX = X.dropna()\nFinally,weextracttheresponse,trainingindicator,anddropthecurrent\nday’s DJ_return and log_volatility to predict only from previous day’s\ndata. 10. deep learning: For an input shape (5,3), each row represents a lagged version of the\nthreevariables.Thenn.RNN()layeralsoexpectsthefirstrowofeachobser-\nvation to be earliest in time, so we must reverse the current order. Hence\nwe loop over range(5,0,-1) below, which is an example of using a slice()\nto index iterable objects.",
    "source": "book_islr",
    "section_title": "10.9 Lab: Deep Learning 461 vs 462 10. Deep Learning"
  },
  {
    "instruction": "What is 9. support vector machines in data science?",
    "input": "",
    "output": "Indeed no training errors were made and only three support vectors were\nused.Infact,thelargevalueofCalsomeansthatthesethreesupportpoints\nare on the margin, and define it. One may wonder how good the classifier\ncould be on test data that depends on only three data points! We now try\na smaller value of C.\nIn[17]: svm_ = SVC(C=0.1, kernel='linear').fit(X, y)\ny_hat = svm_.predict(X)\nconfusion_table(y_hat, y)\nOut[17]: Truth -1 1\nPredicted\n-1 25 0",
    "source": "book_islr",
    "section_title": "390 9. Support Vector Machines"
  },
  {
    "instruction": "What is 1.0 1.5 2.0 in data science?",
    "input": "",
    "output": "0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nTime\nytilibaborP\nlavivruS\nFIGURE 11.4. Top: In a simple example with p = 1 and a binary covariate\nx i 0,1 , the log hazard and the survival function under the model (11.14)\n∈ { }\nare shown (green for x i = 0 and black for x i = 1). Because of the proportional\nhazardsassumption (11.14),theloghazardfunctionsdifferbyaconstant,andthe\nsurvival functions do not cross.",
    "source": "book_islr",
    "section_title": "0.5 1.0 1.5 2.0"
  },
  {
    "instruction": "Explain 6. linear model selection and regularization in simple terms for a beginner in data science.",
    "input": "",
    "output": "5e−03 5e−02 5e−01 5e+00\nrorrE\nnoitadilaV−ssorC\n6.52\n4.52\n2.52\n0.52\n5e−03 5e−02 5e−01 5e+00\nstneiciffeoC\ndezidradnatS\n003\n001\n0\n001−\n003−\nλ λ\nFIGURE 6.12. Left: Cross-validation errors that result from applying ridge\nregression to the Credit data set with various values of λ. Right: The coefficient\nestimates as a function of λ.",
    "source": "book_islr",
    "section_title": "252 6. Linear Model Selection and Regularization"
  },
  {
    "instruction": "When or why would a data scientist use 7. moving beyond linearity?",
    "input": "",
    "output": "We do not have to use splines as the building blocks for GAMs: we can\njustaswelluselocalregression,polynomialregression,oranycombination\nof the approaches seen earlier in this chapter in order to create a GAM. GAMsareinvestigatedinfurtherdetailinthelabattheendofthischapter. Pros and Cons of GAMs\nBefore we move on, let us summarize the advantages and limitations of a",
    "source": "book_islr",
    "section_title": "308 7. Moving Beyond Linearity"
  },
  {
    "instruction": "When or why would a data scientist use prob 1−p?",
    "input": "",
    "output": "x = . i 1 Prob p\nn\n(cid:80)\nConsider the sum s = x . Here the expected value of each x is p and by linearity\ni i\ni=1\nof expectation, the expected value of the sum is m=np. Chernoff bounds bound the\nprobability that the sum s exceeds (1+δ)m or is less than (1−δ)m. We state these\nbounds as Theorems 12.3 and 12.4 below and give their proofs. Theorem 12.3 For any δ > 0, Prob (cid:0) s > (1+δ)m (cid:1) <\n(cid:16)\neδ\n(cid:17)m\n(1+δ)(1+δ)\nTheorem 12.4 Let 0 < γ ≤ 1, then Prob (cid:0) s < (1−γ)m (cid:1) < (cid:16) e−γ (cid:17)m < e− γ2m .",
    "source": "book_fods",
    "section_title": "0 Prob 1−p"
  },
  {
    "instruction": "When or why would a data scientist use lab: introduction to python 49?",
    "input": "",
    "output": "In[41]: fig, ax = subplots(figsize=(8, 8))\nax.plot(x, y, 'o');\nDifferentvaluesofthisadditionalargumentcanbeusedtoproducedifferent\ncolored lines as well as different linestyles. As an alternative, we could use the ax.scatter() function to create a\n.scatter()\nscatterplot. In[42]: fig, ax = subplots(figsize=(8, 8))\nax.scatter(x, y, marker='o');\nNotice that in the code blocks above, we have ended the last line with a\nsemicolon.Thispreventsax.plot(x, y)fromprintingtexttothenotebook. However, it does not prevent a plot from being produced. If we omit the\ntrailing semi-colon, then we obtain the following output:\nIn[43]: fig, ax = subplots(figsize=(8, 8))\nax.scatter(x, y, marker='o')\nOut[43]:<matplotlib.collections.PathCollection at 0x7fb3d9c8f310>\nFigure(432x288)\nIn what follows, we will use trailing semicolons whenever the text that\nwould be output is not germane to the discussion at hand.",
    "source": "book_islr",
    "section_title": "2.3 Lab: Introduction to Python 49"
  },
  {
    "instruction": "What is learning curves for decision making in supervised machine learning: a survey in data science?",
    "input": "",
    "output": "Learning curves are a concept from social sciences that has been adopted in the context of machine learning to assess the performance of a learning algorithm with respect to a certain resource, e.g., the number of training examples or the number of training iterations. Learning curves have important applications in several machine learning contexts, most notably in data acquisition, early stopping of model training, and model selection. For instance, learning curves can be used to model the performance of the combination of an algorithm and its hyperparameter configuration, providing insights into their potential suitability at an early stage and often expediting the algorithm selection process.",
    "source": "web_arxiv",
    "section_title": "Learning Curves for Decision Making in Supervised Machine Learning: A Survey"
  },
  {
    "instruction": "When or why would a data scientist use 12. unsupervised learning?",
    "input": "",
    "output": "informative, and the approach described here for handling missing data is\nnot suitable. Sometimesdataismissingbynecessity.Forexample,ifweformamatrix\nof the ratings (on a scale from 1 to 5) that n customers have given to the\nentire Netflix catalog of p movies, then most of the matrix will be missing,\nsincenocustomerwillhaveseenandratedmorethanatinyfractionofthe\ncatalog.Ifwecanimputethemissingvalueswell,thenwewillhaveanidea\nof what each customer will think of movies they have not yet seen. Hence\nmatrix completion can be used to power recommender systems. recommender\nsystems\nPrincipal Components with Missing Values\nIn Section 12.2.2, we showed that the first M principal component score\nand loading vectors provide the “best” approximation to the data matrix\nX, in the sense of (12.6). Suppose that some of the observations x are\nij\nmissing.",
    "source": "book_islr",
    "section_title": "516 12. Unsupervised Learning"
  },
  {
    "instruction": "How is i i different from 0 1 0 in data science?",
    "input": "",
    "output": "i i: be solved by linear programming by writing x = u−v, u ≥ 0, and v ≥ 0, and minimizing\n(cid:80) (cid:80)\nthe linear function u + v subject to Au-Av=b, u ≥ 0, and v ≥ 0.\ni i\ni i\n42This can be seen by selecting the vectors one at a time. The probability that the ith new vector lies\nfullyinthelowerdimensionalsubspacespannedbythepreviousi−1vectorsiszero, andsobytheunion\nbound the overall probability is zero. 0 1 0: column of A is unit length, the matrix ATA has ones on its diagonal. Since every pair of\ndistinct columns of A has dot-product in the range (− 1 , 1 ), each off-diagonal entry in\n2s 2s\nATA is in the range (− 1 , 1 ).",
    "source": "book_fods",
    "section_title": "1 i i vs 1 0 1 0"
  },
  {
    "instruction": "Describe the typical steps involved in a review of clustering models in educational data science towards fairness-aware learning in a data science workflow.",
    "input": "",
    "output": "Ensuring fairness is essential for every education system. Machine learning is increasingly supporting the education system and educational data science (EDS) domain, from decision support to educational activities and learning analytics. However, the machine learning-based decisions can be biased because the algorithms may generate the results based on students' protected attributes such as race or gender. Clustering is an important machine learning technique to explore student data in order to support the decision-maker, as well as support educational activities, such as group assignments. Therefore, ensuring high-quality clustering models along with satisfying fairness constraints are important requirements. This chapter comprehensively surveys clustering models and their fairness in EDS.",
    "source": "web_arxiv",
    "section_title": "A review of clustering models in educational data science towards fairness-aware learning"
  },
  {
    "instruction": "When or why would a data scientist use otherwise?",
    "input": "",
    "output": "and\n(cid:88)\nn\n(cid:88)\nk ew\nj\nTxi\nJ(W) = δ(l = j)log . i (cid:80)k ew\nh\nTxi\ni=1 j=1 h=1\nThe derivative of the cost function with respect to the weights is\nn\n(cid:88) (cid:0) (cid:1)\n∇ J(W) = − x δ(l = k)−Prob(l = k)|x ,W . w j j j j\ni\nj=1\nNote ∇ J(W) is a vector. Since w is a vector, each component of ∇ J(W) is the\nw i w\ni i\nderivative with respect to one component of the vector w . i\nOver fitting is a major concern in deep learning since large networks can have hun-\ndreds of millions of weights.",
    "source": "book_fods",
    "section_title": "0 otherwise"
  },
  {
    "instruction": "Explain clustering methods 521 in simple terms for a beginner in data science.",
    "input": "",
    "output": "• PCA looks to find a low-dimensional representation of the observa-\ntions that explain a good fraction of the variance;\n• Clustering looks to find homogeneous subgroups among the observa-\ntions. Another application of clustering arises in marketing. We may have ac-\ncess to a large number of measurements (e.g.",
    "source": "book_islr",
    "section_title": "12.4 Clustering Methods 521"
  },
  {
    "instruction": "Explain the bootstrap in simple terms for a beginner in data science.",
    "input": "",
    "output": "Thebootstrapisawidelyapplicableandextremelypowerfulstatisticaltool\nbootstrap\nthat can be used to quantify the uncertainty associated with a given esti-\nmator or statistical learning method. As a simple example, the bootstrap\ncanbeusedtoestimatethestandarderrorsofthecoefficientsfromalinear\nregressionfit.Inthespecificcaseoflinearregression,thisisnotparticularly\nuseful,sincewesawinChapter3thatstandardstatisticalsoftwaresuchas\nR outputs such standard errors automatically. However, the power of the\nbootstrap lies in the fact that it can be easily applied to a wide range of\nstatistical learning methods, including some for which a measure of vari-\nability is otherwise difficult to obtain and is not automatically output by\nstatistical software.",
    "source": "book_islr",
    "section_title": "5.2 The Bootstrap"
  },
  {
    "instruction": "What is definitions and key theorems in data science?",
    "input": "",
    "output": "Definition 5.1 Given a set S of examples and a concept class H, we say that S is\nshattered by H if for every A ⊆ S there exists some h ∈ H that labels all examples in A\nas positive and all examples in S \\A as negative. Definition 5.2 The VC-dimension of H is the size of the largest set shattered by H.\nFor example, there exist sets of four points in the plane that can be shattered by rect-\nangles with axis-parallel edges, e.g., four points at the vertices of a diamond (see Figure\n5.5). Given such a set S, for any A ⊆ S, there exists a rectangle with the points in A\ninside the rectangle and the points in S \\ A outside the rectangle.",
    "source": "book_fods",
    "section_title": "5.11.1 Definitions and Key Theorems"
  },
  {
    "instruction": "How is ··· k different from 4. classification in data science?",
    "input": "",
    "output": "··· k: 4. When the number of features p is large, there tends to be a deteri-\noration in the performance of KNN and other local approaches that\nperformpredictionusingonlyobservationsthatarenear thetestob-\nservation for which a prediction must be made. 4. classification: (a) Suppose that we have a set of observations, each with measure-\nments on p = 1 feature, X. We assume that X is uniformly\n(evenly) distributed on [0,1].",
    "source": "book_islr",
    "section_title": "1 ··· K vs 194 4. Classification"
  },
  {
    "instruction": "Explain 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "makes at most\n(cid:0) (cid:1)\nmin R2|w∗|2 +2L (w∗,S)\nhinge\nw∗\nmistakes, where R = max |x |. t t\nProof: As before, each update of the Perceptron algorithm increases |w|2 by at most R2,\nso if the algorithm makes M mistakes, we have |w|2 ≤ MR2. What we can no longer say is that each update of the algorithm increases wTw∗ by\nat least 1.",
    "source": "book_fods",
    "section_title": "1 2"
  },
  {
    "instruction": "What is 20 40 60 80 100 120 in data science?",
    "input": "",
    "output": "0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nMonths\ndehsilbuP\ngnieB\ntoN\nfo\nytilibaborP\nLow Risk\nMedium Risk\nHigh Risk\nFIGURE 11.8. For the Publication data introduced in Section 11.5.4, we\ncomputetertilesof“risk”inthetestsetusingcoefficientsestimatedonthetraining\nset. There is clear separation between the resulting survival curves.",
    "source": "book_islr",
    "section_title": "0 20 40 60 80 100 120"
  },
  {
    "instruction": "When or why would a data scientist use m2?",
    "input": "",
    "output": "h∼H\n184\nWe now give an example of a 2-universal family of hash functions. Let M be a prime\ngreater than m. For each pair of integers a and b in the range [0,M −1], define a hash\nfunction\nh (x) = ax+b (mod M)\nab\nTo store the hash function h , store the two integers a and b. This requires only O(logM)\nab\nspace. To see that the family is 2-universal note that h(x) = w and h(y) = z if and only\nif\n(cid:18) (cid:19)(cid:18) (cid:19) (cid:18) (cid:19)\nx 1 a w\n= (mod M)\ny 1 b z\n(cid:18) (cid:19)\nx 1\nIf x (cid:54)= y, the matrix is invertible modulo M.28 Thus\ny 1\n(cid:18)\na\n(cid:19) (cid:18)\nx 1\n(cid:19)−1(cid:18)\nw\n(cid:19)\n= (mod M)\nb y 1 z\nand for each\n(cid:0)w(cid:1)\nthere is a unique\n(cid:0)a(cid:1)\n. Hence\nz b\n1\n(cid:0) (cid:1)\nProb h(x) = w and h(y) = z =",
    "source": "book_fods",
    "section_title": "M2"
  },
  {
    "instruction": "How is y = different from logistic regression in data science?",
    "input": "",
    "output": "y =: =1 if drug overdose. We could then fit a linear regression to this binary response, and predict\ndrug overdose if Yˆ >0.5 and stroke otherwise. logistic regression: Consider again the Default data set, where the response default falls into\none of two categories, Yes or No. Rather than modeling this response Y\ndirectly, logistic regression models the probability that Y belongs to a par-\nticular category.",
    "source": "book_islr",
    "section_title": "Y = vs 4.3 Logistic Regression"
  },
  {
    "instruction": "Explain otherwise in simple terms for a beginner in data science.",
    "input": "",
    "output": "and\n(cid:88)\nn\n(cid:88)\nk ew\nj\nTxi\nJ(W) = δ(l = j)log . i (cid:80)k ew\nh\nTxi\ni=1 j=1 h=1\nThe derivative of the cost function with respect to the weights is\nn\n(cid:88) (cid:0) (cid:1)\n∇ J(W) = − x δ(l = k)−Prob(l = k)|x ,W . w j j j j\ni\nj=1\nNote ∇ J(W) is a vector.",
    "source": "book_fods",
    "section_title": "0 otherwise"
  },
  {
    "instruction": "How is 5. resampling methods different from y, in data science?",
    "input": "",
    "output": "5. resampling methods: package, we provide a wrapper, sklearn_sm(), that enables us to easily use\nsklearn_sm()\nthe cross-validation tools of sklearn with models fit by statsmodels. Theclasssklearn_sm()hasasitsfirstargumentamodelfromstatsmodels. y,: cv=Auto.shape[0])\ncv_err = np.mean(cv_results['test_score'])\ncv_err\nOut[9]:24.2315\nThe arguments to cross_validate() are as follows: an object with the ap-\npropriatefit(),predict(),andscore()methods,anarrayoffeaturesXand\naresponseY.Wealsoincludedanadditionalargumentcvtocross_validate();\nspecifyinganintegerK resultsinK-foldcross-validation.Wehaveprovided\na value corresponding to the total number of observations, which results\nin leave-one-out cross-validation (LOOCV). The cross_validate() func-\ncross_\ntion produces a dictionary with several components; we simply want the\nvalidate()\ncross-validated test score here (MSE), which is estimated to be 24.23.",
    "source": "book_islr",
    "section_title": "218 5. Resampling Methods vs Y,"
  },
  {
    "instruction": "When or why would a data scientist use 2?",
    "input": "",
    "output": "3. find x=argmin(cid:107)x(cid:107) such that (cid:107)r(cid:107)2 < ε\n¯ 1 2\n383\nExercise 10.23 Let M = L+R where L is a low rank matrix corrupted by a sparse noise\nmatrix R. Why can we not recover L from M if R is low rank or if L is sparse? Exercise 10.24\n1. Suppose for a univariate convex function f and a finite interval D, |f(cid:48)(cid:48)(x)| ≤ δ|f(cid:48)(x)|\nfor every x. Then, what is a good step size to choose for gradient descent? Derive a\nbound on the number of steps needed to get an approximate minimum of f in terms\nof as few parameters as possible.",
    "source": "book_fods",
    "section_title": "1 2"
  },
  {
    "instruction": "What is 2. statistical learning in data science?",
    "input": "",
    "output": "guments are loc, scale, and size. These are keyword arguments, which\nkeyword\nmeans that when they are passed into the function, they can be referred\nto by name (in any order).3 By default, this function will generate random\nnormalvariable(s)withmean(loc)0andstandarddeviation(scale)1;fur-\nthermore, a single random variable will be generated unless the argument\nto size is changed. We now generate 50 independent random variables from a N(0,1) dis-\ntribution.",
    "source": "book_islr",
    "section_title": "46 2. Statistical Learning"
  },
  {
    "instruction": "When or why would a data scientist use 10 20 30 40?",
    "input": "",
    "output": "051\n001\n05\n0\nNumber of Components\nrorrE\nderauqS\nnaeM\nSquared Bias\nTest MSE\nVariance\nFIGURE 6.18. PCRwasappliedtotwosimulateddatasets.Ineachpanel,the\nhorizontal dashed line represents the irreducible error. Left: Simulated data from\nFigure 6.8. Right: Simulated data from Figure 6.9.\ndirections in which X ,...,X show the most variation are the directions",
    "source": "book_islr",
    "section_title": "0 10 20 30 40"
  },
  {
    "instruction": "When or why would a data scientist use 4 1 2 3 4 5?",
    "input": "",
    "output": "without needing to further adjust for multiple testing. To summarize, Holm’s procedure and Bonferroni’s procedure are very\ngeneral approaches for multiple testing correction that can be applied un-\nderallcircumstances.However,incertainspecialcases,morepowerfulpro-\nceduresformultipletestingcorrectionmaybeavailable,inordertocontrol\nthe FWER while achieving higher power (i.e. committing fewer Type II\nerrors) than would be possible using Holm or Bonferroni. In this section,\nwe have illustrated two such examples.",
    "source": "book_islr",
    "section_title": "0 4 1 2 3 4 5"
  },
  {
    "instruction": "How is 1.0 2.94 different from 1.0 2.94 in data science?",
    "input": "",
    "output": "1.0 2.94: Inthissimplecase,thefit()methoddoesverylittle;itsimplychecksthat\nthe variable 'lstat' specified in design exists in Boston. Then transform()\nconstructs the model matrix with two columns: an intercept and the vari-\nable lstat. 1.0 2.94: Note that, as in the previous code chunk when the two steps were done\nseparately, the design object is changed as a result of the fit() operation. The power of this pipeline will become clearer when we fit more complex\nmodels that involve interactions and transformations.",
    "source": "book_islr",
    "section_title": "3 1.0 2.94 vs 3 1.0 2.94"
  },
  {
    "instruction": "When or why would a data scientist use generating points uniformly at random from a ball?",
    "input": "",
    "output": "Consider generating points uniformly at random on the surface of the unit ball. For\nthe 2-dimensional version of generating points on the circumference of a unit-radius cir-\ncle, independentlygenerateeachcoordinateuniformlyatrandomfromtheinterval[−1,1]. This produces points distributed over a square that is large enough to completely contain\nthe unit circle. Project each point onto the unit circle. The distribution is not uniform\nsince more points fall on a line from the origin to a vertex of the square than fall on a line\nfrom the origin to the midpoint of an edge of the square due to the difference in length.",
    "source": "book_fods",
    "section_title": "2.5 Generating Points Uniformly at Random from a Ball"
  },
  {
    "instruction": "Describe the typical steps involved in convergence of random walks on undirected graphs in a data science workflow.",
    "input": "",
    "output": "The Metropolis-Hasting algorithm and Gibbs sampling both involve random walks\non edge-weighted undirected graphs. Given an edge-weighted undirected graph, let w\nxy\ndenote the weight of the edge between nodes x and y, with w = 0 if no such edge exists. xy\n(cid:80)\nLet w = w . The Markov chain has transition probabilities p = w /w . We\nx y xy xy xy x\nassume the chain is connected. We now claim that the stationary distribution π of this walk has π proportional to\nx\n(cid:80)\nw , i.e., π = w /w for w = w .",
    "source": "book_fods",
    "section_title": "4.4 Convergence of Random Walks on Undirected Graphs"
  },
  {
    "instruction": "Explain 2 n i in simple terms for a beginner in data science.",
    "input": "",
    "output": "measurements for the ith observation. That is,\nx\ni1\nx\ni2\nx i = . .",
    "source": "book_islr",
    "section_title": "1 2 n i"
  },
  {
    "instruction": "What is comparison to polynomial regression in data science?",
    "input": "",
    "output": "Figure 7.7 compares a natural cubic spline with 15 degrees of freedom to a\ndegree-15polynomialontheWagedataset.Theextraflexibilityinthepoly-\nnomial produces undesirable results at the boundaries, while the natural\ncubic spline still provides a reasonable fit to the data. Regression splines\noften give superior results to polynomial regression. This is because unlike\npolynomials,whichmustuseahighdegree(exponentinthehighestmono-\nmial term, e.g.",
    "source": "book_islr",
    "section_title": "7.4.5 Comparison to Polynomial Regression"
  },
  {
    "instruction": "Describe the typical steps involved in o in a data science workflow.",
    "input": "",
    "output": "are observed in X. 2. Repeat steps (a)–(c) until the objective (12.14) fails to decrease:\n(a) Solve\np n M 2\nminimize x˜ a b (12.13)\nA ∈ Rn × M,B ∈ Rp × M 0 j=1 0 i=1 > ij − m 0 =1 im jm ?  \nby computing the principal components of X˜. \n(b) For each element (i,j) / , set x˜ M aˆ ˆb . ∈O ij ← m=1 im jm\n(c) Compute the objective\n)\n2",
    "source": "book_islr",
    "section_title": "O"
  },
  {
    "instruction": "Describe the typical steps involved in y, in a data science workflow.",
    "input": "",
    "output": "test_size=1/3,\nrandom_state=1)\nLinear Models\nWe fit the linear model and evaluate the test error directly. In[13]: hit_lm = LinearRegression().fit(X_train, Y_train)\nYhat_test = hit_lm.predict(X_test)\nnp.abs(Yhat_test - Y_test).mean()\nOut[13]:259.7153\nNext we fit the lasso using sklearn. We are using mean absolute error\nto select and evaluate a model, rather than mean squared error. The spe-\ncialized solver we used in Section 6.5.2 uses only mean squared error. So\nhere, with a bit more work, we create a cross-validation grid and perform\nthe cross-validation directly. We encode a pipeline with two steps: we first normalize the features\nusing a StandardScaler() transform, and then fit the lasso without further\nnormalization.",
    "source": "book_islr",
    "section_title": "Y,"
  },
  {
    "instruction": "When or why would a data scientist use i ij j?",
    "input": "",
    "output": "i,j\nSince the entries of A are nonnegative, the components of the first left and right singular\nvectors must all be nonnegative, that is, x ≥ 0 and y ≥ 0 for all i and j. To bound\ni j\n(cid:80)\nx a y , break the summation into O(lognlogd) parts. Each part corresponds to a\ni ij j\ni,j\ngiven α and β and consists of all i such that α ≤ x < 2α and all j such that β ≤ y < 2β. i i\nThe lognlogd parts are defined by breaking the rows into logn blocks with α equal to\n1 √ 1 , √ 1 , 2√ 1 , 4√ 1 , ... , 1 and by breaking the columns into logd blocks with β equal",
    "source": "book_fods",
    "section_title": "1 i ij j"
  },
  {
    "instruction": "How is subset selection 235 different from m m in data science?",
    "input": "",
    "output": "subset selection 235: efficient alternative to best subset selection. However, unlike forward step-\nwise selection, it begins with the full least squares model containing all p\npredictors, and then iteratively removes the least useful predictor, one-at-\na-time. m m: diction error on a validation set, C (AIC), BIC, or adjusted R2. Or\np\nuse the cross-validation method.",
    "source": "book_islr",
    "section_title": "6.1 Subset Selection 235 vs M M"
  },
  {
    "instruction": "How is gams for regression problems different from 1 i1 2 i2 p ip i in data science?",
    "input": "",
    "output": "gams for regression problems: A natural way to extend the multiple linear regression model\ny =β +β x +β x + +β x +\"\ni 0 1 i1 2 i2 p ip i\n···\nin order to allow for non-linear relationships between each feature and the\nresponse is to replace each linear component β x with a (smooth) non-\nj ij\nlinear function f (x ). We would then write the model as\nj ij\np\ny = β + f (x )+\"\ni 0 j ij i\nj=1\n0\n= β +f (x )+f (x )+ +f (x )+\" . 1 i1 2 i2 p ip i: ···\nThis is an example of a GAM. It is called an additive model because we\ncalculate a separate f for each X , and then add together all of their\nj j\ncontributions.",
    "source": "book_islr",
    "section_title": "7.7.1 GAMs for Regression Problems vs 0 1 i1 2 i2 p ip i"
  },
  {
    "instruction": "What is cross-validation (statistics) part 5 in data science?",
    "input": "",
    "output": "The goal of cross-validation is to estimate the expected level of fit of a model to a data set that is independent of the data that were used to train the model. It can be used to estimate any quantitative measure of fit that is appropriate for the data and model. For example, for binary classification problems, each case in the validation set is either predicted correctly or incorrectly.",
    "source": "web_wikipedia",
    "section_title": "Cross-validation (statistics) part 5"
  },
  {
    "instruction": "When or why would a data scientist use 3 5 3?",
    "input": "",
    "output": "Graph with boundary vertices Values of harmonic function\nsatisfying boundary conditions\ndark and boundary conditions\nwhere the edge weights at\nspecified. each vertex are equal\nFigure 4.7: Graph illustrating an harmonic function. solutionsisitselfharmonic. Sinceh(x) = f(x)−g(x)isharmonicandhasvaluezeroonthe\nboundary, by the min and max principles it has value zero everywhere. Thus f(x) = g(x).",
    "source": "book_fods",
    "section_title": "5 3 5 3"
  },
  {
    "instruction": "When or why would a data scientist use stepwise selection?",
    "input": "",
    "output": "For computational reasons, best subset selection cannot be applied with\nverylargep.Bestsubsetselectionmayalsosufferfromstatisticalproblems\nwhenpislarge.Thelargerthesearchspace,thehigherthechanceoffinding\nmodels that look good on the training data, even though they might not\nhave any predictive power on future data. Thus an enormous search space\ncan lead to overfitting and high variance of the coefficient estimates. For both of these reasons, stepwise methods, which explore a far more\nrestrictedsetofmodels,areattractivealternativestobestsubsetselection. Forward Stepwise Selection\nForward stepwise selectionisacomputationallyefficientalternativetobest\nforward\nsubset selection. While the best subset selection procedure considers all\nstepwise\n2p possible models containing subsets of the p predictors, forward step-\nselection\nwise considers a much smaller set of models.",
    "source": "book_islr",
    "section_title": "6.1.2 Stepwise Selection"
  },
  {
    "instruction": "What is c in data science?",
    "input": "",
    "output": "ij\n−\ni!j ij\n−\nkj\nk\n| |i,i 0!∈ Ck0 j=1 i 0∈ Ck0 j=1\nwhere x¯ = 1 x is the mean for feature j in cluster C . kj\n|\nCk| i\n∈\nCk ij k\nIn Step 2(a) the cluster means for each feature are the constants that\n)\nminimize the sum-of-squared deviations, and in Step 2(b), reallocating the\nobservations can only improve (12.18). This means that as the algorithm\nis run, the clustering obtained will continually improve until the result no\nlongerchanges;theobjectiveof(12.17)willneverincrease.Whentheresult\nno longer changes, a local optimum has been reached.",
    "source": "book_islr",
    "section_title": "C"
  },
  {
    "instruction": "Explain . ||a−a ||2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "k 2\nExercise 3.14 If A is a symmetric matrix with distinct singular values, show that the\nleft and right singular vectors are the same and that A = VDVT. Exercise 3.15 Let A be a matrix. How would you compute\nv = argmax |Av|?",
    "source": "book_fods",
    "section_title": "4. ||A−A ||2"
  },
  {
    "instruction": "What is exercises 131 in data science?",
    "input": "",
    "output": "(b) Now perform a simple linear regression of x onto y without an\nintercept,andreportthecoefficientestimate,itsstandarderror,\nand the corresponding t-statistic and p-values associated with\nthe null hypothesis H :β =0. Comment on these results. 0\n(c) Whatistherelationshipbetweentheresultsobtainedin(a)and\n(b)?",
    "source": "book_islr",
    "section_title": "3.7 Exercises 131"
  },
  {
    "instruction": "How is the basics of decision trees 341 different from m in data science?",
    "input": "",
    "output": "the basics of decision trees 341: servations. Why, then, is the split performed at all? m: Which model is better? It depends on the problem at hand.",
    "source": "book_islr",
    "section_title": "8.1 The Basics of Decision Trees 341 vs 1 M"
  },
  {
    "instruction": "Explain exercises 501 in simple terms for a beginner in data science.",
    "input": "",
    "output": "(b) Sketch the Kaplan-Meier survival curve corresponding to this\ndataset. (Youdonotneedtouseanysoftwaretodothis—you\ncan sketch it by hand using the results obtained in (a).) (c) Basedonthesurvivalcurveestimatedin(b),whatistheproba-\nbility that the event occurs within 200 days?",
    "source": "book_islr",
    "section_title": "11.9 Exercises 501"
  },
  {
    "instruction": "When or why would a data scientist use additional graphical and numerical summaries?",
    "input": "",
    "output": "We can use the ax.plot() or ax.scatter() functions to display the quan-\ntitative variables. However, simply typing the variable names will produce\nan error message, because Python does not know to look in the Auto data\nset for those variables. In[101]: fig, ax = subplots(figsize=(8, 8))\nax.plot(horsepower, mpg, 'o');\nNameError: name 'horsepower' is not defined\nWe can address this by accessing the columns directly:\nIn[102]: fig, ax = subplots(figsize=(8, 8))\nax.plot(Auto['horsepower'], Auto['mpg'], 'o');\nAlternatively,wecanusetheplot()methodwiththecallAuto.plot().Us-\n.plot()\ning this method, the variables can be accessed by name. The plot methods\nof a data frame return a familiar object: an axes. We can use it to update\nthe plot as we did previously:\nIn[103]: ax = Auto.plot.scatter('horsepower', 'mpg');\nax.set_title('Horsepower vs. MPG')\nIf we want to save the figure that contains a given axes, we can find the\nrelevant figure by accessing the figure attribute:\nIn[104]: fig = ax.figure\nfig.savefig('horsepower_mpg.png');\nWecanfurtherinstructthedataframetoplottoaparticularaxesobject.",
    "source": "book_islr",
    "section_title": "2.3.9 Additional Graphical and Numerical Summaries"
  },
  {
    "instruction": "How is c different from 12. unsupervised learning in data science?",
    "input": "",
    "output": "c: ij\n−\ni!j ij\n−\nkj\nk\n| |i,i 0!∈ Ck0 j=1 i 0∈ Ck0 j=1\nwhere x¯ = 1 x is the mean for feature j in cluster C . kj\n|\nCk| i\n∈\nCk ij k\nIn Step 2(a) the cluster means for each feature are the constants that\n)\nminimize the sum-of-squared deviations, and in Step 2(b), reallocating the\nobservations can only improve (12.18). 12. unsupervised learning: Data Step 1 Iteration 1, Step 2a\nIteration 1, Step 2b Iteration 2, Step 2a Final Results\nFIGURE 12.8. The progress of the K-means algorithm on the example of\nFigure 12.7 with K=3.",
    "source": "book_islr",
    "section_title": "C vs 524 12. Unsupervised Learning"
  },
  {
    "instruction": "Explain choice of time scale in simple terms for a beginner in data science.",
    "input": "",
    "output": "In the examples considered thus far in this chapter, it has been fairly clear\nhowtodefinetime.Forexample,inthePublicationexample,time zerofor\neach paper was defined to be the calendar time at the end of the study,\nand the failure time was defined to be the number of months that elapsed\nfrom the end of the study until the paper was published. However, in other settings, the definitions of time zero and failure time\nmaybemoresubtle.Forexample,whenexaminingtheassociationbetween\nrisk factors and disease occurrence in an epidemiological study, one might\nuse the patient’s age to define time, so that time zero is the patient’s date\nofbirth.Withthischoice,theassociationbetweenageandsurvivalcannot\nbe measured; however, there is no need to adjust for age in the analysis. When examining covariates associated with disease-free survival (i.e.",
    "source": "book_islr",
    "section_title": "11.7.2 Choice of Time Scale"
  },
  {
    "instruction": "Explain 7. moving beyond linearity in simple terms for a beginner in data science.",
    "input": "",
    "output": "10. This question relates to the College data set. (a) Splitthedataintoatrainingsetandatestset.Usingout-of-state\ntuitionastheresponseandtheothervariablesasthepredictors,\nperform forward stepwise selection on the training set in order\nto identify a satisfactory model that uses just a subset of the\npredictors.",
    "source": "book_islr",
    "section_title": "328 7. Moving Beyond Linearity"
  },
  {
    "instruction": "How is 1.0 2.94 different from lab: linear regression 119 in data science?",
    "input": "",
    "output": "1.0 2.94: We extract the response, and fit the model. In[10]: y = Boston['medv']\nmodel = sm.OLS(y, X)\nresults = model.fit()\nNote that sm.OLS() does not fit the model; it specifies the model, and then\nmodel.fit() does the actual fitting. lab: linear regression 119: We first describe this process for our simple regression model using a\nsingle predictor lstat in the Boston data frame, but will use it repeatedly\nin more complex tasks in this and other labs in this book. In our case the\ntransform is created by the expression design = MS(['lstat']).",
    "source": "book_islr",
    "section_title": "3 1.0 2.94 vs 3.6 Lab: Linear Regression 119"
  },
  {
    "instruction": "When or why would a data scientist use 12. unsupervised learning?",
    "input": "",
    "output": "the return type when using this method, so instances will still be of class\nAgglomerativeClustering. In the following example we use the data from\nthe previous lab to plot the hierarchical clustering dendrogram using com-\nplete, single, and averagelinkage clustering with Euclidean distance as the\ndissimilarity measure. We begin by clustering observations using complete\nlinkage. In[38]: HClust = AgglomerativeClustering\nhc_comp = HClust(distance_threshold=0,\nn_clusters=None,\nlinkage='complete')\nhc_comp.fit(X)\nThis computes the entire dendrogram. We could just as easily perform\nhierarchical clustering with average or single linkage instead:\nIn[39]: hc_avg = HClust(distance_threshold=0,\nn_clusters=None,\nlinkage='average');\nhc_avg.fit(X)\nhc_sing = HClust(distance_threshold=0,\nn_clusters=None,\nlinkage='single');\nhc_sing.fit(X);\nTo use a precomputed distance matrix, we provide an additional argu-\nmentmetric=\"precomputed\".Inthecodebelow,thefirstfourlinescomputes\nthe 50 50 pairwise-distance matrix.",
    "source": "book_islr",
    "section_title": "544 12. Unsupervised Learning"
  },
  {
    "instruction": "How is 0 5 1 different from 3 d−1 in data science?",
    "input": "",
    "output": "0 5 1: d−1 d−1\n(cid:80) (cid:80)\nBy Lemmas 11.2 and 11.4, c c = 2δ(k) and b b = 2δ(k) and for all k.\nj j−2k j j−2k\nj=0 j=0\nSeparating odd and even terms,\nd−1 d−1\n(cid:88)2 (cid:88)2\nc c + c c = 2δ(k) (11.2)\n2j 2j−2k 2j+1 2j+1−2k\nj=0 j=0\nand\nd−1 d−1\n(cid:88)2 (cid:88)2\nb b + (−1)jb b = 2δ(k) (11.3)\n2j 2j−2k 2j+1 2j+1−2k\nj=0 j=0\nfor all k.\nc c +c c +c c +···+c c +c c +c c +··· = 2 k = 0\n0 0 2 2 4 4 1 1 3 3 5 5\nc c +c c +··· +c c +c c +··· = 0 k = 1 3 d−1: these sequences. Equation 12.1 is C ∗BR+C ∗BR = 0, 12.2 is C ∗CR+C ∗CR = δ(k),\ne e o o e e o o\nand 11.3 is B ∗BR +B ∗BR = δ(k), where the superscript R stands for reversal of the\ne e o o\nsequence.",
    "source": "book_fods",
    "section_title": "4 0 5 1 vs 1 3 d−1"
  },
  {
    "instruction": "When or why would a data scientist use left singular vectors?",
    "input": "",
    "output": "The left singular vectors are also pairwise orthogonal. Intuitively if u and u , i < j, were\ni j\nnot orthogonal, one would suspect that the right singular vector v had a component of v\nj i\n48\nwhich would contradict that v and v were orthogonal. Let i be the smallest integer such\ni j\nthat u is not orthogonal to all other u . Then to prove that u and u are orthogonal,\ni j i j\nwe add a small component of v to v , normalize the result to be a unit vector\nj i\nv +(cid:15)v\nv(cid:48) = i j\ni |v +(cid:15)v |\ni j\nand show that |Av(cid:48)| > |Av |, a contradiction. i i\nTheorem 3.7 The left singular vectors are pairwise orthogonal.",
    "source": "book_fods",
    "section_title": "3.6 Left Singular Vectors"
  },
  {
    "instruction": "What is (fdp). in data science?",
    "input": "",
    "output": "discovery\nIt might be tempting to ask the data analyst to control the FDP: to\nproportion\nmake sure that no more than, say, 20% of the rejected null hypotheses are\nfalse positives. However, in practice, controlling the FDP is an impossible\ntask for the data analyst, since she has no way to be certain, on any par-\nticulardataset,whichhypothesesaretrueandwhicharefalse.Thisisvery\nsimilar to the fact that the data analyst can control the FWER, i.e. she\ncan guarantee that Pr(V 1) α for any pre-specified α, but she cannot\n≥ ≤\nguarantee that V = 0 on any particular dataset (short of failing to reject\nany null hypotheses, i.e.",
    "source": "book_islr",
    "section_title": "(FDP)."
  },
  {
    "instruction": "When or why would a data scientist use giant component in graphs with given degree distribution?",
    "input": "",
    "output": "Molloy and Reed address the issue of when a random graph with a nonuniform degree\ndistribution has a giant component. Let λ be the fraction of vertices of degree i. There\ni\n∞\n(cid:80)\nwill be a giant component if and only if i(i−2)λ > 0.\ni\ni=0\nTo see intuitively that this is the correct formula, consider exploring a component\nof a graph starting from a given seed vertex. Degree zero vertices do not occur except\nin the case where the vertex is the seed. If a degree one vertex is encountered, then\nthat terminates the expansion along the edge into the vertex.",
    "source": "book_fods",
    "section_title": "8.8.1 Giant Component in Graphs with Given Degree Distribution"
  },
  {
    "instruction": "How is 2 r different from 1 in data science?",
    "input": "",
    "output": "2 r: by a topic. Approximate Version: Partition the documents into r clusters, where at most εn of\nthe j ∈ {1,2,...,n} are misclustered. 1: mn ≥ dln2+ln . l\nγ δ\nThis proves the following result.",
    "source": "book_fods",
    "section_title": "1 2 r vs 2 1"
  },
  {
    "instruction": "What is 2 3 in data science?",
    "input": "",
    "output": "number of children at each node. Since the expected number of children at each node is the slope of f(x) at x = 1, if\nthe expected number of children is less than or equal to one, the slope of f(x) at x = 1\nis less than or equal to one and the unique root of f(x) = x in (0,1] is at x = 1 and the\nprobability of extinction is one unless f(cid:48)(1) = 1 and p = 1. If f(cid:48)(1) = 1 and p = 1,",
    "source": "book_fods",
    "section_title": "1 2 3"
  },
  {
    "instruction": "Explain 1 2 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "with high probability, k < 4log n.\n2\nExercise 8.19 (Birthday problem) What is the number of integers that must be drawn\nwith replacement from a set of n integers so that some integer, almost surely, will be\nselected twice? Exercise 8.20 Suppose you have an algorithm for finding communities in a social net-\nwork. Assume that the way the algorithm works is that given the graph G for the social\nnetwork, it finds a subset of vertices satisfying a desired property P. The specifics of prop-\nerty P are unimportant for this question.",
    "source": "book_fods",
    "section_title": "1 1 2 2"
  },
  {
    "instruction": "When or why would a data scientist use if ith person is from the west?",
    "input": "",
    "output": "x = (3.29)\ni2\n=0 if ith person is not from the West. Then both of these variables can be used in the regression equation, in\norder to obtain the model\nβ 0 +β 1 +! i if ith person is from the South\ny i =β 0 +β 1 x i1 +β 2 x i2 +! i = β 0 +β 2 +! i if ith person is from the West\nβ 0 +!",
    "source": "book_islr",
    "section_title": "1 if ith person is from the West"
  },
  {
    "instruction": "When or why would a data scientist use bibliographic notes?",
    "input": "",
    "output": "The material on the analogy between random walks on undirected graphs and electrical\nnetworks is from [DS84] as is the material on random walks in Euclidean space. Addi-\n116\ntional material on Markov chains can be found in [MR95b], [MU05], and [per10]. For\nmaterial on Markov Chain Monte Carlo methods see [Jer98] and [Liu01]. The use of normalized conductance to prove convergence of Markov Chains is by\nSinclair and Jerrum, [SJ89] and Alon [Alo86]. A polynomial time bounded Markov chain\nbased method for estimating the volume of convex sets was developed by Dyer, Frieze and\nKannan [DFK91].",
    "source": "book_fods",
    "section_title": "4.9 Bibliographic Notes"
  },
  {
    "instruction": "What is clustering methods 521 in data science?",
    "input": "",
    "output": "• PCA looks to find a low-dimensional representation of the observa-\ntions that explain a good fraction of the variance;\n• Clustering looks to find homogeneous subgroups among the observa-\ntions. Another application of clustering arises in marketing. We may have ac-\ncess to a large number of measurements (e.g.",
    "source": "book_islr",
    "section_title": "12.4 Clustering Methods 521"
  },
  {
    "instruction": "When or why would a data scientist use applications?",
    "input": "",
    "output": "Wavelets are widely used in data compression for images and speech, as well as in\ncomputer vision for representing images. Unlike the sines and cosines of the Fourier\ntransform, wavelets have spatial locality in addition to frequency information, which can\nbe useful for better understanding the contents of an image and for relating pieces of\ndifferent images to each other. Wavelets are also being used in power line communication\nprotocols that send data over highly noisy channels.",
    "source": "book_fods",
    "section_title": "11.10 Applications"
  },
  {
    "instruction": "Describe the typical steps involved in maximum weight matching in a data science workflow.",
    "input": "",
    "output": "We have seen that the belief propagation algorithm converges to the correct solution\nin trees and graphs with a single cycle. It also correctly converges for a number of prob-\nlems. Here we give one example, the maximum weight matching problem where there is\na unique solution. We apply the belief propagation algorithm to find the maximal weight matching\n(MWM) in a complete bipartite graph. If the MWM in the bipartite graph is unique,\nthen the belief propagation algorithm will converge to it. Let G = (V ,V ,E) be a complete bipartite graph where V = {a ,...,a }, V =",
    "source": "book_fods",
    "section_title": "9.19 Maximum Weight Matching"
  },
  {
    "instruction": "How is e rtr = btb. (6.4) different from f f f in data science?",
    "input": "",
    "output": "e rtr = btb. (6.4): From(6.2), weseethat 1 (cid:80)s X = CR. ThisisrepresentedinFigure6.4. f f f: equal to 1/ε2. If ε is Ω(1), so s ∈ O(1), then the multiplication CR can be carried out in\ntime O(mp).",
    "source": "book_fods",
    "section_title": "E RTR = BTB. (6.4) vs F F F"
  },
  {
    "instruction": "How is singular vectors different from 1 2 2 n n in data science?",
    "input": "",
    "output": "singular vectors: We now define the singular vectors of an n×d matrix A. Consider the rows of A as\nn points in a d-dimensional space. 1 2 2 n n: minimizing the vertical squared distances of the points to it, namely, (cid:80)n (y −mx −b)2.\ni=1 i i\n42\n|Av|2. The best fit line is the one maximizing |Av|2 and hence minimizing the sum of the\nsquared distances of the points to the line.",
    "source": "book_fods",
    "section_title": "3.3 Singular Vectors vs 1 1 2 2 n n"
  },
  {
    "instruction": "How is fund managers at level 0.05, then how many null hypothe- different from auc, 155 in data science?",
    "input": "",
    "output": "fund managers at level 0.05, then how many null hypothe-: ses do we reject? If we control the FDR for just these 10 fund\nmanagers at level 0.05, then how many null hypotheses do we\nreject? auc, 155: Benjamini–Hochbergprocedure,575–\nAutodataset,12,66,98–101,129,\n577\n197, 202–207, 327, 398\nBernoulli distribution, 172\nauto-correlation, 421\nbest subset selection, 231, 246\nautoregression, 423\nbias, 31–34, 74, 90, 159, 405\naxes, 48\nbias-variance\nbackfitting, 307, 328 decomposition, 32\n© Springer Nature Switzerland AG 2023 597\nG. James et al., An Introduction to Statistical Learning, Springer Texts in Statistics,\nhttps://doi.org/10.1007/978-3-031-38747-0",
    "source": "book_islr",
    "section_title": "10 fund managers at level 0.05, then how many null hypothe- vs AUC, 155"
  },
  {
    "instruction": "When or why would a data scientist use 4 i i!5?",
    "input": "",
    "output": "(The notation n means n(n 1)/2, and gives the number of pairs\n’ ( 2 −\namong a set of n items.) ’ (\nNotice that in (9.18), in order to evaluate the function f(x), we need to\ncomputetheinnerproductbetweenthenewpointxandeachofthetraining\npoints x . However, it turns out that α is nonzero only for the support\ni i\nvectors in the solution—that is, if a training observation is not a support\nvector, then its α equals zero. So if is the collection of indices of these\ni",
    "source": "book_islr",
    "section_title": "2 4 i i!5"
  },
  {
    "instruction": "Explain g g in simple terms for a beginner in data science.",
    "input": "",
    "output": ") )\nWhen the sample size is large, the log-rank test statistic W has ap-\nproximately a standard normal distribution; this can be used to compute\na p-value for the null hypothesis that there is no difference between the\nsurvival curves in the two groups.6\nComparing the survival times of females and males on the BrainCancer\ndatagivesalog-rankteststatisticofW =1.2,whichcorrespondstoatwo-\nsided p-value of 0.2 using the theoretical null distribution, and a p-value\nof 0.25 using the permutation null distribution with 1,000 permutations. Thus,wecannotrejectthenullhypothesisofnodifferenceinsurvivalcurves\nbetween females and males. Thelog-ranktestiscloselyrelatedtoCox’sproportionalhazardsmodel,\nwhich we discuss in Section 11.5.2.",
    "source": "book_islr",
    "section_title": "G G"
  },
  {
    "instruction": "What is t in data science?",
    "input": "",
    "output": "objective in order for the fraction of points clustered incorrectly to be less than (cid:15). We will\nspecifically focus in this discussion on the k-median objective rather than the k-means\nobjective, since it is a bit easier to work with. What we will now show is that under this condition, even though it may be NP-hard\nin general to find a clustering that is within a factor c of optimal, we can nonetheless\nefficiently find a clustering C(cid:48) such that dist(C(cid:48),C ) ≤ (cid:15), so long as all clusters in C are",
    "source": "book_fods",
    "section_title": "T"
  },
  {
    "instruction": "Explain graphics in simple terms for a beginner in data science.",
    "input": "",
    "output": "In Python, common practice is to use the library matplotlib for graphics. matplotlib\nHowever, since Python was not written with data analysis in mind, the no-\ntion of plotting is not intrinsicto the language. Wewill use the subplots()\nfunctionfrommatplotlib.pyplottocreateafigureandtheaxesontowhich\nweplotourdata.FormanymoreexamplesofhowtomakeplotsinPython,\nreaders are encouraged to visit matplotlib.org/stable/gallery/.",
    "source": "book_islr",
    "section_title": "2.3.4 Graphics"
  },
  {
    "instruction": "Explain 30 40 50 60 in simple terms for a beginner in data science.",
    "input": "",
    "output": "03\n52\n02\n51\n01\n5\nPopulation\ngnidnepS\ndA\nFIGURE 6.21. For the advertising data, the first PLS direction (solid line)\nand first PCR direction (dotted line) are shown. that the directions that best explain the predictors will also be the best\ndirections to use for predicting the response.",
    "source": "book_islr",
    "section_title": "20 30 40 50 60"
  },
  {
    "instruction": "Describe the typical steps involved in 10. deep learning in a data science workflow.",
    "input": "",
    "output": "Finally, we evaluate our model on our test data. In[59]: cifar_trainer.test(cifar_module,\ndatamodule=cifar_dm)\nOut[59]:[{'test_loss': 2.4238 'test_accuracy': 0.4206}]\nHardware Acceleration\nAs deep learning has become ubiquitous in machine learning, hardware\nmanufacturers have produced special libraries that can often speed up the\ngradient-descent steps. Forinstance,MacOSdeviceswiththeM1chipmayhavetheMetal pro-\ngrammingframeworkenabled,whichcanspeedupthetorchcomputations. We present an example of how to use this acceleration. ThemainchangesaretotheTrainer()callaswellastothemetricsthat\nwill be evaluated on the data. These metrics must be told where the data\nwill be located at evaluation time.",
    "source": "book_islr",
    "section_title": "452 10. Deep Learning"
  },
  {
    "instruction": "How is checking the proportional hazards assumption different from survival trees in data science?",
    "input": "",
    "output": "checking the proportional hazards assumption: We have seen that Cox’s proportional hazards model relies on the propor-\ntional hazards assumption (11.14). While results from the Cox model tend\ntobefairlyrobusttoviolationsofthisassumption,itisstillagoodideato\ncheckwhetheritholds.Inthecaseofaqualitativefeature,wecanplot the\nlog hazard function for each level of the feature. survival trees: InChapter8,wediscussedflexibleandadaptivelearningproceduressuchas\ntrees,randomforests,andboosting,whichweappliedinboththeregression\nand classification settings. Most of these approaches can be generalized to\nthesurvivalanalysis setting.Forexample,survival treesareamodification\nsurvival\nofclassificationandregressiontreesthatuseasplitcriterionthatmaximizes\ntrees",
    "source": "book_islr",
    "section_title": "11.7.4 Checking the Proportional Hazards Assumption vs 11.7.5 Survival Trees"
  },
  {
    "instruction": "How is |h[s]| ≤ different from vc-dimension of combinations of concepts in data science?",
    "input": "",
    "output": "|h[s]| ≤: (cid:0)n(cid:1)\n, where we are defining\n(cid:0)n(cid:1)\n=\n(cid:80)d (cid:0)n(cid:1)\n; this is the number of distinct\n≤d ≤d i=0 i\nways of choosing d or fewer elements out of n. We will do so by induction on n. As a\nbase case, our theorem is trivially true if n ≤ d.\nAs a first step in the proof, notice that:\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nn n−1 n−1\n= + (5.2)\n≤ d ≤ d ≤ d−1\nbecause we can partition the ways of choosing d or fewer items into those that do not\ninclude the first item (leaving ≤ d to be chosen from the remainder) and those that do\ninclude the first item (leaving ≤ d−1 to be chosen from the remainder). Now, consider any set S of n points and pick some arbitrary point x ∈ S. By induc-\ntion, we may assume that |H[S\\{x}]| ≤\n(cid:0)n−1(cid:1)\n. vc-dimension of combinations of concepts: Often one wants to create concepts out of other concepts. For example, given several\nlinear separators, one could take their intersection to create a convex polytope.",
    "source": "book_fods",
    "section_title": "|H[S]| ≤ vs 5.11.4 VC-Dimension of Combinations of Concepts"
  },
  {
    "instruction": "Explain 1 1 2 2 p p in simple terms for a beginner in data science.",
    "input": "",
    "output": "···\nFor any given cutoff t, we classify observations into the heart disease or\nno heart disease categories depending on whether fˆ(X) < t or fˆ(X) t.\n≥\nThe ROC curve is obtained by forming these predictions and computing\nthefalsepositiveandtruepositiveratesforarangeofvaluesoft.Anopti-\nmal classifier will hug the top left corner of the ROC plot. In this instance",
    "source": "book_islr",
    "section_title": "0 1 1 2 2 p p"
  },
  {
    "instruction": "Explain and print out the results. in simple terms for a beginner in data science.",
    "input": "",
    "output": "Hint: Recall that x**a raises x to the power a. Use the print()\nfunction to display the result. (b) Create a new function, Power2(), that allows you to pass any\ntwonumbers,xanda,andprintsoutthevalueofx**a.Youcan\ndo this by beginning your function with the line\ndef Power2(x, a):\nYoushouldbeabletocallyourfunctionbyentering,forinstance,\nPower2(3, 8)\nonthecommandline.Thisshouldoutputthevalueof38,namely,\n6,561.",
    "source": "book_islr",
    "section_title": "23 and print out the results."
  },
  {
    "instruction": "How is 2 n i=1 i i=1 i different from f f f in data science?",
    "input": "",
    "output": "2 n i=1 i i=1 i: lemma is proved. 445\nThere are two important matrix norms, the matrix p-norm\n||A|| = max (cid:107)Ax(cid:107)\np p\n|x|=1\nand the Frobenius norm\n(cid:115)\n(cid:88)\n||A|| = a2 . f f f: Proof: Let a be the ith column of A and let b be the jth column of B. By the\ni j\nCauchy-Schwartz inequality (cid:13) (cid:13)a Tb (cid:13) (cid:13) ≤ (cid:107)a (cid:107)(cid:107)b (cid:107).",
    "source": "book_fods",
    "section_title": "1 2 n i=1 i i=1 i vs F F F"
  },
  {
    "instruction": "What is 2 n in data science?",
    "input": "",
    "output": "of a random variable x. Then\n(cid:18)(cid:12) (cid:12) (cid:19)\n(cid:12)x 1 +x 2 +···+x n (cid:12) Var(x)\nProb (cid:12) −E(x)(cid:12) ≥ (cid:15) ≤\n(cid:12) n (cid:12) n(cid:15)2\nProof: By Chebychev’s inequality\nProb (cid:18)(cid:12) (cid:12) (cid:12) x 1 +x 2 +···+x n −E(x) (cid:12) (cid:12) (cid:12) ≥ (cid:15) (cid:19) ≤ Var (cid:0) x1+x2+ n ···+xn (cid:1)\n(cid:12) n (cid:12) (cid:15)2\n1\n= Var(x +x +···+x )\nn2(cid:15)2 1 2 n\n1\n(cid:0) (cid:1)\n= Var(x )+Var(x )+···+Var(x )\nn2(cid:15)2 1 2 n\nVar(x)\n= . n(cid:15)2\nThe Law of Large Numbers is quite general, applying to any random variable x of\nfinite variance.",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "Describe the typical steps involved in 2 3 in a data science workflow.",
    "input": "",
    "output": "Figure 9.3: The factor graph for the function\nf(x ,x ,x ) = (x +x +x )(x +x¯ )(x +x¯ )(x¯ +x¯ ). 1 2 3 1 2 3 1 2 1 3 2 3\nusing say sum of squares instead of sum of absolute values can be used and there are\npolynomial time algorithms as long as the function to be minimized is convex. More generally, the correction term may depend on all grid points within distance\ntwo of each point rather than just immediate neighbors. Even more generally, we may\nhave n variables y ,y ,...y with the value of some of them already specified and subsets",
    "source": "book_fods",
    "section_title": "1 2 3"
  },
  {
    "instruction": "Explain k, in simple terms for a beginner in data science.",
    "input": "",
    "output": "pred,\ndid_rent,\ndid_rent / pred))\nK=1: # predicted to rent: 62,# who did rent 9, accuracy 14.5%\nK=2: # predicted to rent: 6,# who did rent 1, accuracy 16.7%\nK=3: # predicted to rent: 20,# who did rent 3, accuracy 15.0%\nK=4: # predicted to rent: 3,# who did rent 0, accuracy 0.0%\nK=5: # predicted to rent: 7,# who did rent 1, accuracy 14.3%\nWe see some variability — the numbers for K=4 are very different from the\nrest. Comparison to Logistic Regression\nAs a comparison, we can also fit a logistic regression model to the data. This can also be done with sklearn, though by default it fits something\nlike the ridge regression version of logistic regression, which we introduce\nin Chapter 6.",
    "source": "book_islr",
    "section_title": "K,"
  },
  {
    "instruction": "What is unique reconstruction of a sparse vector in data science?",
    "input": "",
    "output": "A vector is said to be s-sparse if it has at most s nonzero elements. Let x be a d-\ndimensional, s-sparse vector with s (cid:28) d. Consider solving Ax = b for x where A is an\nn×d matrix with n < d. The set of solutions to Ax = b is a subspace. However, if we\nrestrict ourselves to sparse solutions, under certain conditions on A there is a unique s-\nsparse solution.",
    "source": "book_fods",
    "section_title": "10.2.1 Unique Reconstruction of a Sparse Vector"
  },
  {
    "instruction": "What is sketch of a large matrix in data science?",
    "input": "",
    "output": "The main result of this section is that for any matrix, a sample of columns and rows,\neach picked according to length squared distribution provides a good sketch of the matrix. Let A be an m×n matrix. Pick s columns of A according to length squared distribution.",
    "source": "book_fods",
    "section_title": "6.3.3 Sketch of a Large Matrix"
  },
  {
    "instruction": "Explain 1 in simple terms for a beginner in data science.",
    "input": "",
    "output": "A = L + R and UΣVT being the singular value decomposition of A. This can be done\nusing Lagrange multipliers (??). Write R = R+ +R− where R+ ≥ 0 and R− ≥ 0.",
    "source": "book_fods",
    "section_title": "1 1"
  },
  {
    "instruction": "When or why would a data scientist use comparison of linear regression with k-nearest neighbors 111?",
    "input": "",
    "output": "in the regression model in order to accommodate non-additive rela-\ntionships. A small p-value associated with the interaction term indi-\ncatesthepresenceofsuchrelationships.Figure3.5suggestedthatthe\nAdvertising data may not be additive. Including an interaction term\ninthemodelresultsinasubstantialincreaseinR2,fromaround90%\nto almost 97%.",
    "source": "book_islr",
    "section_title": "3.5 Comparison of Linear Regression with K-Nearest Neighbors 111"
  },
  {
    "instruction": "Explain 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "t(1)+ 1t(3). Solving for t(i+1) for 3 ≤ i < n−1 yields\n2\nt(i+1) = 2t(i)−t(i−1)\nwhich has solution t(i) = 2(n−i) for 3 ≤ i < n−1. Then solving for t(2) and t(1) yields\nt(2) = 2(n−2) and t(1) = n−1.",
    "source": "book_fods",
    "section_title": "2 2"
  },
  {
    "instruction": "When or why would a data scientist use 2 n−1 i?",
    "input": "",
    "output": "approximating n − 1 with n, the theorem follows from Theorem 12.6 in the appendix. Although the probability that the degree of a single vertex differs significantly from\nits expected value drops exponentially, the statement that the degree of every vertex is\nclose to its expected value requires that p is Ω(logn). That is, the expected degree grows\nn\nat least logarithmically with the number of vertices. Corollary 8.2 Suppose ε is a positive constant. If p ≥ 9lnn, then almost surely every\nnε2\nvertex has degree in the range (1−ε)np to (1+ε)np.",
    "source": "book_fods",
    "section_title": "1 2 n−1 i"
  },
  {
    "instruction": "How is choosing the optimal model different from 6. linear model selection and regularization in data science?",
    "input": "",
    "output": "choosing the optimal model: Best subset selection, forward selection, and backward selection result in\nthe creation of a set of models, each of which contains a subset of the p\n3Likeforwardstepwiseselection,backwardstepwiseselectionperformsaguidedsearch\nover model space, and so effectively considers substantially more than 1+p(p+1)/2\nmodels. 6. linear model selection and regularization: predictors. To apply these methods, we need a way to determine which of\nthesemodelsisbest.AswediscussedinSection6.1.1,themodelcontaining\nall of the predictors will always have the smallest RSS and the largest R2,\nsince these quantities are related to the training error.",
    "source": "book_islr",
    "section_title": "6.1.3 Choosing the Optimal Model vs 236 6. Linear Model Selection and Regularization"
  },
  {
    "instruction": "How is 1− g(x) different from 4δ in data science?",
    "input": "",
    "output": "1− g(x): g(cid:48)(x) = x . 2δ1−g(x)\nIf δ is greater than some δ , then g(1) (cid:54)= 1. 4δ: Thetwosolutionsgivenby(8.7)becomecomplexforδ > 1/8andthuscanbevalidonly\nfor 0 ≤ δ ≤ 1/8. For δ > 1/8, the only solution is g(cid:48)(1) = 1 and an infinite component\n2δ\nexists.",
    "source": "book_fods",
    "section_title": "1 1− g(x) vs 2 4δ"
  },
  {
    "instruction": "Explain constraints and splines in simple terms for a beginner in data science.",
    "input": "",
    "output": "ThetopleftpanelofFigure7.3lookswrongbecausethefittedcurveisjust\ntoo flexible. To remedy this problem, we can fit a piecewise polynomial\nunder the constraint that the fitted curve must be continuous. In other\nwords,therecannotbeajumpwhenage=50.ThetoprightplotinFigure7.3\nshows the resulting fit.",
    "source": "book_islr",
    "section_title": "7.4.2 Constraints and Splines"
  },
  {
    "instruction": "Explain o in simple terms for a beginner in data science.",
    "input": "",
    "output": "are observed in X. 2. Repeat steps (a)–(c) until the objective (12.14) fails to decrease:\n(a) Solve\np n M 2\nminimize x˜ a b (12.13)\nA ∈ Rn × M,B ∈ Rp × M 0 j=1 0 i=1 > ij − m 0 =1 im jm ?",
    "source": "book_islr",
    "section_title": "O"
  },
  {
    "instruction": "How is |s| different from |s| in data science?",
    "input": "",
    "output": "|s|: To discuss this, we need to define what we mean by a “pruning” of T and what we mean\nby the “size” of h. A pruning h of T is a tree in which some internal nodes of T have been\nturned into leaves, labeled “+” or “−” depending on whether the majority of examples in\nS that reach that node are positive or negative. Let size(h) = L(h)log(n) where L(h) is\nthe number of leaves in h.\nHint #1: it is sufficient, for each integer L = 1,2,...,L(T), to find the pruning of T\nwith L leaves of lowest empirical error on S, that is, h = argmin err (h). |s|: rate of the algorithm on S (the total number of mistakes of the algorithm divided by\n177\n(cid:113)\n|S|) will be at most err (h ) + O( Llog(L(T))), where h = argmin err (h)\nS L |S| L h:L(h)=L S\nis the pruning of T with L leaves of lowest error on S.\n3. In the above question, we assumed L was given.",
    "source": "book_fods",
    "section_title": "2|S| vs |S|"
  },
  {
    "instruction": "Explain 1 in simple terms for a beginner in data science.",
    "input": "",
    "output": "f(x)= exp (x µ)TΣ− 1(x µ) . (4.23)\n(2π)p/2 Σ1/2 −2 − −\n| | * +\nIn the case of p > 1 predictors, the LDA classifier assumes that the\nobservations in the kth class are drawn from a multivariate Gaussian dis-\ntribution N(µ ,Σ), where µ is a class-specific mean vector, and Σ is a\nk k\ncovariance matrix that is common to all K classes. Plugging the density\nfunction for the kth class, f (X = x), into (4.15) and performing a little\nk\nbitofalgebrarevealsthattheBayesclassifierassignsanobservationX =x",
    "source": "book_islr",
    "section_title": "1 1"
  },
  {
    "instruction": "How is 2. statistical learning different from 2 p in data science?",
    "input": "",
    "output": "2. statistical learning: Flexibility\nytilibaterpretnI\nLow High\nhgiH\nwoL\nSubset Selection\nLasso\nLeast Squares\nGeneralized Additive Models\nTrees\nBagging, Boosting\nSupport Vector Machines\nDeep Learning\nFIGURE 2.7. A representation of the tradeoff between flexibility and inter-\npretability,usingdifferentstatisticallearningmethods.Ingeneral,astheflexibility\nof a method increases, its interpretability decreases. 2 p: approaches, such as the splines discussed in Chapter 7 and displayed in\nFigures 2.5 and 2.6,and theboosting methods discussedin Chapter 8,can\nlead to such complicated estimates of f that it is difficult to understand\nhow any individual predictor is associated with the response. Figure2.7providesanillustrationofthetrade-offbetweenflexibilityand\ninterpretability for some of the methods that we cover in this book.",
    "source": "book_islr",
    "section_title": "24 2. Statistical Learning vs 1 2 p"
  },
  {
    "instruction": "Describe the typical steps involved in finding the term-topic matrix in a data science workflow.",
    "input": "",
    "output": "For this, we need an extra assumption, which we first motivate. Suppose as in Section\n9.8, we assume that there is a single pure document for each topic. In terms of the Figure\n327\n9.1 of three topics, this says that there is a column of P close to each vertex of the tri-\nangle. But the corresponding column of A can be very far from this. So, even if we were\ntold which document is pure for each topic, we cannot find the column of B. However, if\nwe had a large number of nearly pure documents for each topic, since the corresponding\ncolumns of A are independent even conditioned on P, the average of these columns gives\nus a good estimate of the column of B.",
    "source": "book_fods",
    "section_title": "9.9 Finding the Term-Topic Matrix"
  },
  {
    "instruction": "Explain 1 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "(d) On your sketch, indicate the margin for the maximal margin\nhyperplane. (e) Indicate the support vectors for the maximal margin classifier. (f) Argue that a slight movement of the seventh observation would\nnot affect the maximal margin hyperplane.",
    "source": "book_islr",
    "section_title": "0 1 2"
  },
  {
    "instruction": "When or why would a data scientist use s?",
    "input": "",
    "output": "disjunction. Proof: Suppose target concept c∗ is a disjunction. Then for any x that is listed in c∗,\ni\nx will not be set to 1 in any negative example by definition of an OR. Therefore, h will\ni\ninclude x as well. Since h contains all variables listed in c∗, this ensures that h will\ni\ncorrectly predict positive on all positive examples in S. Furthermore, h will correctly\npredict negative on all negative examples in S since by design all features set to 1 in any\nnegative example were discarded.",
    "source": "book_fods",
    "section_title": "S"
  },
  {
    "instruction": "Describe the typical steps involved in |s||t| in a data science workflow.",
    "input": "",
    "output": "density d(A) of A is defined as the maximum value of d(S,T) over all subsets of rows and\ncolumns. This definition applies to bipartite as well as non bipartite graphs. One important case is when A’s rows and columns both represent the same set and\na is the similarity between object i and object j. Here d(S,S) = A(S,S). If A is an n×n\nij |S|\n0-1 matrix, it can be thought of as the adjacency matrix of an undirected graph, and\nd(S,S) is the average degree of a vertex in S. The subgraph of maximum average degree\nin a graph can be found exactly by network flow techniques, as we will show in the next\n230\nFigure 7.5: Example of a bipartite graph. section.",
    "source": "book_fods",
    "section_title": "|S||T|"
  },
  {
    "instruction": "When or why would a data scientist use index?",
    "input": "",
    "output": "load_data(), 117 np.squeeze(), 457\nloc[], 58, 59, 177 np.std(), 47\nlog_loss(), 355 np.sum(), 43\nLogisticGAM(), 323 np.var(), 47\nlogrank_test(), 490 np.where(), 180\nlowess(), 324 ns(), 317\nmatplotlib, 48 numpy, 42, 555\nmax(), 66 os.chdir(), 55\nmean(), 48 outer(), 219\nmedian(), 197 pairwise_distances(),554\nmin(), 66 pairwise_tukeyhsd(), 587\nMNIST(), 444 pandas, 55\nModelSpec(), 116–118, 122, params, 175\n124, 267 partial(), 222, 269\nMS(), see ModelSpec() PCA(), 280, 537, 540, 554\nmult_test(),seemultipletests() pd, see pandas\nmultipletests(), 586 pd.crosstab(), 555\nmultipletests(), 583, 589 pd.cut(), 315\nmultivariate_logrank_test(), pd.get_dummies(), 314",
    "source": "book_islr",
    "section_title": "604 Index"
  },
  {
    "instruction": "When or why would a data scientist use 2?",
    "input": "",
    "output": "origin form an equilateral triangle). So, it is enough to prove that m > 3n unit-length\nvectors in (cid:60)n cannot all have distance at least 1 from each other. (2) use the fact that the\nvolume of a ball of radius r in (cid:60)n is proportional to rn. 381\nExercise 10.8 Create a random 100 by 100 orthonormal matrix A and a sparse 100-\ndimensional vector x. Compute Ax = b. Randomly select a few coordinates of b and\nreconstruct x from the samples of b using the minimization of 1-norm technique of Section\n10.2.2. Did you get x back?",
    "source": "book_fods",
    "section_title": "2 2"
  },
  {
    "instruction": "Explain 22 32 n2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Figure 9.9: Tree for MWM problem. matching. Consider the tree matching.",
    "source": "book_fods",
    "section_title": "12 22 32 n2"
  },
  {
    "instruction": "Explain scenario 6 in simple terms for a beginner in data science.",
    "input": "",
    "output": "54.0\n04.0\n53.0\n03.0\n52.0\n02.0\n51.0\nFIGURE 4.12. Boxplots of the test error rates for each of the non-linear\nscenarios described in the main text. other approaches.",
    "source": "book_islr",
    "section_title": "SCENARIO 6"
  },
  {
    "instruction": "Explain a(d) in simple terms for a beginner in data science.",
    "input": "",
    "output": "V (d) = dΩ rd−1dr = dΩ =\nd d\nSd r=0 Sd\nwhere A(d) is the surface area of the d-dimensional unit ball. For instance, for d = 3 the\nsurface area is 4π and the volume is 4π. The question remains, how to determine the\n3\n17\n(cid:82)\nsurface area A(d) = dΩ for general d.\nSd\nConsider a different integral\n∞ ∞ ∞\n(cid:90) (cid:90) (cid:90)\nI(d) = ···\ne−(x2\n1\n+x2\n2\n+···x2\nd\n)\ndx d ···dx 2 dx 1 .",
    "source": "book_fods",
    "section_title": "1 A(d)"
  },
  {
    "instruction": "How is application: learning decision trees different from l r in data science?",
    "input": "",
    "output": "application: learning decision trees: One popular practical method for machine learning is to learn a decision tree; see Figure\n5.4. While finding the smallest decision tree that fits a given training sample S is NP-\nhard, there are a number of heuristics that are used in practice.19 Suppose we run such\na heuristic on a training set S and it outputs a tree with k nodes. l r: right subtree. So, by Theorem 5.7, we can be confident the true error is low if we can\nproduce a consistent tree with fewer than (cid:15)|S|/log(d) nodes.",
    "source": "book_fods",
    "section_title": "5.6.3 Application: Learning Decision Trees vs L R"
  },
  {
    "instruction": "How is 2 3 different from pc1 pc2 in data science?",
    "input": "",
    "output": "2 3: plotting the projected points. We illustrate the use of PCA on the USArrests data set. pc1 pc2: Murder 0.5358995 0.4181809\n−\nAssault 0.5831836 0.1879856\n−\nUrbanPop 0.2781909 0.8728062\nRape 0.5434321 0.1673186\nTABLE 12.1. The principal component loading vectors, φ 1 and φ 2, for the\nUSArrests data.",
    "source": "book_islr",
    "section_title": "1 2 3 vs PC1 PC2"
  },
  {
    "instruction": "When or why would a data scientist use k?",
    "input": "",
    "output": "point and the center of its cluster. That is, we want to minimize\nk\nΦ (C) = maxmaxd(a ,c ). kcenter i j\nj=1 ai∈Cj\nk-center clustering makes sense when we believe clusters should be local regions in\nspace. It is also often thought of as the “firehouse location problem” since one can\nthink of it as the problem of locating k fire-stations in a city so as to minimize the\nmaximum distance a fire-truck might need to travel to put out a fire. k-median clustering: Find a partition C = {C ,...,C } of A into k clusters, with corre-",
    "source": "book_fods",
    "section_title": "1 k"
  },
  {
    "instruction": "How is tv different from 1 in data science?",
    "input": "",
    "output": "tv: selaS\nFIGURE 3.1.For the Advertising data, the least squares fit for the regression\nof sales onto TV is shown. The fit is found by minimizing the residual sum of\nsquares. 1: Hereβ istheinterceptterm—thatis,theexpectedvalueofY whenX =0,\n0\nand β is the slope—the average increase in Y associated with a one-unit\n1\nincrease in X. The error term is a catch-all for what we miss with this\nsimple model: the true relationship is probably not linear, there may be\nother variables that cause variation in Y, and there may be measurement\nerror.",
    "source": "book_islr",
    "section_title": "TV vs 0 1"
  },
  {
    "instruction": "Explain 12. unsupervised learning in simple terms for a beginner in data science.",
    "input": "",
    "output": "In[23]: pcaUS.components_\nOut[23]:array([[ 0.53589947, 0.58318363, 0.27819087, 0.54343209],\n[ 0.41818087, 0.1879856 , -0.87280619, -0.16731864],\n[-0.34123273, -0.26814843, -0.37801579, 0.81777791],\n[ 0.6492278 , -0.74340748, 0.13387773, 0.08902432]])\nThematrixUcorrespondstoastandardizedversionofthePCAscorematrix\n(eachcolumnstandardizedtohavesum-of-squaresone).Ifwemultiplyeach\ncolumn of U by the corresponding element of D, we recover the PCA scores\nexactly (up to a meaningless sign flip). In[24]: (U * D[None,:])[:3]\nOut[24]:array([[-0.9856, 1.1334, -0.4443, 0.1563],\n[-1.9501, 1.0732, 2.04 , -0.4386],\n[-1.7632, -0.746 , 0.0548, -0.8347]])\nIn[25]: scores[:3]\nOut[25]:array([[ 0.9856, -1.1334, -0.4443, 0.1563],\n[ 1.9501, -1.0732, 2.04 , -0.4386],\n[ 1.7632, 0.746 , 0.0548, -0.8347]])\nWhile it would be possible to carry out this lab using the PCA() estimator,\nhere we use the np.linalg.svd() function in order to illustrate its use. We now omit 20 entries in the 50 4 data matrix at random.",
    "source": "book_islr",
    "section_title": "540 12. Unsupervised Learning"
  },
  {
    "instruction": "How is 13. multiple testing different from exercises in data science?",
    "input": "",
    "output": "13. multiple testing: T_vals[j] = ttest_ind(D2[col],\nD4[col],\nequal_var=True).statistic\nD_ = np.hstack([D2[col], D4[col]])\nD_null = D_.copy()\nfor b in range(B):\nrng.shuffle(D_null)\nttest_ = ttest_ind(D_null[:n_],\nD_null[n_:],\nequal_var=True)\nTnull_vals[j,b] = ttest_.statistic\nNext, we compute the number of rejected null hypotheses R, the esti-\nmated number of false positives V, and the estimated FDR, for a range\nof threshold values c in Algorithm 13.4. The threshold values are chosen\nusing the absolute values of the teWst statistics from the 100 genes. exercises: Conceptual\n1. Suppose we test m null hypotheses, all of which are true.",
    "source": "book_islr",
    "section_title": "592 13. Multiple Testing vs 13.7 Exercises"
  },
  {
    "instruction": "How is survival trees different from lab: survival analysis in data science?",
    "input": "",
    "output": "survival trees: InChapter8,wediscussedflexibleandadaptivelearningproceduressuchas\ntrees,randomforests,andboosting,whichweappliedinboththeregression\nand classification settings. Most of these approaches can be generalized to\nthesurvivalanalysis setting.Forexample,survival treesareamodification\nsurvival\nofclassificationandregressiontreesthatuseasplitcriterionthatmaximizes\ntrees lab: survival analysis: In this lab, we perform survival analyses on three separate data sets. In\nSection 11.8.1 we analyze the BrainCancer data that was first described\nin Section 11.3.",
    "source": "book_islr",
    "section_title": "11.7.5 Survival Trees vs 11.8 Lab: Survival Analysis"
  },
  {
    "instruction": "How is k different from k in data science?",
    "input": "",
    "output": "k: exists some distance σ such that\n1. any two data points in different clusters have distance at least σ, and\n2. for any cluster C∗ and any partition of C∗ into two non-empty sets A and C∗ \\A,\ni i i\nthere exist points on each side of the partition of distance less than σ. Then, single-linkage will correctly recover the clustering C∗,...,C∗ . k: Proof: ConsiderrunningthealgorithmuntilallpairsofclustersC andC(cid:48) haved (C,C(cid:48)) ≥ σ.\nmin\nAt that point, by (2), each target cluster C∗ will be fully contained within some cluster\ni\nof the single-linkage algorithm. On the other hand, by (1) and by induction, each cluster\nC of the single-linkage algorithm will be fully contained within some C∗ of the target\ni\nclustering, since any merger of subsets of distinct target clusters would require d ≥ σ.\nmin\nTherefore, the single-linkage clusters are indeed the target clusters.",
    "source": "book_fods",
    "section_title": "1 k vs 1 k"
  },
  {
    "instruction": "How is w different from x y in data science?",
    "input": "",
    "output": "w: approximatedthenulldistributionviaare-samplingapproach.Wesawthat\nusing the re-sampling approach gave us substantially different results from\nusingthetheoreticalp-valueapproachinFigure13.8,butnotinFigure13.7. In general, there are two settings in which a re-sampling approach is\nparticularly useful:\n1. x y: of the theoretical null distribution will not be valid (i.e. they will not\nproperly control the Type I error).",
    "source": "book_islr",
    "section_title": "W vs X Y"
  },
  {
    "instruction": "When or why would a data scientist use 2 r?",
    "input": "",
    "output": "max |Av| = 0.\nv⊥v1,v2,...,vr\n|v|=1\nThe greedy algorithm found the v that maximized |Av| and then the best fit 2-\n1\ndimensional subspace containing v . Is this necessarily the best-fit 2-dimensional sub-\n1\nspace overall? The following theorem establishes that the greedy algorithm finds the best\nsubspaces of every dimension. Theorem 3.1 (The Greedy Algorithm Works) Let A be an n×d matrix with singu-\nlar vectors v ,v ,...,v . For 1 ≤ k ≤ r, let V be the subspace spanned by v ,v ,...,v .",
    "source": "book_fods",
    "section_title": "1 2 r"
  },
  {
    "instruction": "Explain d in simple terms for a beginner in data science.",
    "input": "",
    "output": "sampling algorithm repeats the following steps. One of the variables x is chosen to be\ni\nupdated. Its new value is chosen based on the marginal probability of x with the other\ni\nvariables fixed.",
    "source": "book_fods",
    "section_title": "1 d"
  },
  {
    "instruction": "When or why would a data scientist use −?",
    "input": "",
    "output": "observation as its own cluster. ’ (\n2. For i=n,n 1,...,2:\n−\n(a) Examine all pairwise inter-cluster dissimilarities among the i\nclustersandidentifythepairofclustersthatareleastdissimilar\n(thatis,mostsimilar).Fusethesetwoclusters.Thedissimilarity\nbetween these two clusters indicates the height in the dendro-\ngram at which the fusion should be placed. (b) Compute the new pairwise inter-cluster dissimilarities among\nthe i 1 remaining clusters. −\nThe Hierarchical Clustering Algorithm\nThehierarchicalclusteringdendrogramisobtainedviaanextremelysimple\nalgorithm.Webeginbydefiningsomesortofdissimilaritymeasurebetween\neach pair of observations.",
    "source": "book_islr",
    "section_title": "2 −"
  },
  {
    "instruction": "How is s2 different from lab: logistic regression, lda, qda, and knn 191 in data science?",
    "input": "",
    "output": "s2: Out[68]: coef std err t P>|t|\nintercept 73.5974 5.132 14.340 0.000\nmnth[Jan] -46.0871 4.085 -11.281 0.000\nmnth[Feb] -39.2419 3.539 -11.088 0.000\nmnth[March] -29.5357 3.155 -9.361 0.000\nmnth[April] -4.6622 2.741 -1.701 0.089\nmnth[May] 26.4700 2.851 9.285 0.000\nmnth[June] 21.7317 3.465 6.272 0.000\nmnth[July] -0.7626 3.908 -0.195 0.845\nmnth[Aug] 7.1560 3.535 2.024 0.043\nmnth[Sept] 20.5912 3.046 6.761 0.000\nmnth[Oct] 29.7472 2.700 11.019 0.000\nmnth[Nov] 14.2229 2.860 4.972 0.000\nhr[0] -96.1420 3.955 -24.307 0.000\nhr[1] -110.7213 3.966 -27.916 0.000\nhr[2] -117.7212 4.016 -29.310 0.000\n..... ....... ..... ...... .....\nWhatisthedifferencebetweenthetwocodings?InM2_lm,acoefficientesti-\nmateisreportedforallbutlevel23ofhrandlevelDecofmnth.Importantly,\nin M2_lm, the (unreported) coefficient estimate for the last level of mnth is\nnot zero: instead, it equals the negative of the sum of the coefficient esti-\nmatesforalloftheotherlevels.Similarly,inM2_lm,thecoefficientestimate\nforthelastlevelofhristhenegativeofthesumofthecoefficientestimates\nfor all of the other levels. This means that the coefficients of hr and mnth\nin M2_lm will always sum to zero, and can be interpreted as the difference\nfrom the mean level. lab: logistic regression, lda, qda, and knn 191: In[71]: coef_month = S2[S2.index.str.contains('mnth')]['coef']\ncoef_month\nOut[71]:mnth[Jan] -46.0871\nmnth[Feb] -39.2419\nmnth[March] -29.5357\nmnth[April] -4.6622\nmnth[May] 26.4700\nmnth[June] 21.7317\nmnth[July] -0.7626\nmnth[Aug] 7.1560\nmnth[Sept] 20.5912\nmnth[Oct] 29.7472\nmnth[Nov] 14.2229\nName: coef, dtype: float64\nNext, we append Dec as the negative of the sum of all other months. In[72]: months = Bike['mnth'].dtype.categories\ncoef_month = pd.concat([\ncoef_month,\npd.Series([-coef_month.sum()],\nindex=['mnth[Dec]'\n])\n])\ncoef_month\nOut[72]:mnth[Jan] -46.0871\nmnth[Feb] -39.2419\nmnth[March] -29.5357\nmnth[April] -4.6622\nmnth[May] 26.4700\nmnth[June] 21.7317\nmnth[July] -0.7626\nmnth[Aug] 7.1560\nmnth[Sept] 20.5912\nmnth[Oct] 29.7472\nmnth[Nov] 14.2229\nmnth[Dec] 0.3705\nName: coef, dtype: float64\nFinally,tomaketheplotneater,we’lljustusethefirstletterofeachmonth,\nwhich is the 6th entry of each of the labels in the index.",
    "source": "book_islr",
    "section_title": "S2 vs 4.7 Lab: Logistic Regression, LDA, QDA, and KNN 191"
  },
  {
    "instruction": "When or why would a data scientist use exercises?",
    "input": "",
    "output": "Exercise 10.1 Select a method that you believe is good for combining individual rankings\ninto a global ranking. Consider a set of rankings where each individual ranks b last. One\nby one move b from the bottom to the top leaving the other rankings in place. Does there\nexist a v as in Theorem 10.2 where v is the ranking that causes b to move from the bottom\nto the top in the global ranking. If not, does your method of combing individual rankings\nsatisfy the axioms of unanimity and independence of irrelevant alternatives.",
    "source": "book_fods",
    "section_title": "10.10 Exercises"
  },
  {
    "instruction": "What is survival trees in data science?",
    "input": "",
    "output": "InChapter8,wediscussedflexibleandadaptivelearningproceduressuchas\ntrees,randomforests,andboosting,whichweappliedinboththeregression\nand classification settings. Most of these approaches can be generalized to\nthesurvivalanalysis setting.Forexample,survival treesareamodification\nsurvival\nofclassificationandregressiontreesthatuseasplitcriterionthatmaximizes\ntrees",
    "source": "book_islr",
    "section_title": "11.7.5 Survival Trees"
  },
  {
    "instruction": "How is classification trees different from 7.007 in data science?",
    "input": "",
    "output": "classification trees: A classification tree is very similar to a regression tree, except that it is\nclassification\nused to predict a qualitative response rather than a quantitative one. Re-\ntree\ncall that for a regression tree, the predicted response for an observation is\ngiven by the mean response of the training observations that belong to the\nsame terminal node. 7.007: FIGURE8.4. RegressiontreeanalysisfortheHittersdata.Theunprunedtree\nthat results from top-down greedy splitting on the training data is shown.",
    "source": "book_islr",
    "section_title": "8.1.2 Classification Trees vs 6.459 7.007"
  },
  {
    "instruction": "When or why would a data scientist use b=1000,?",
    "input": "",
    "output": "seed=10)\nhp_se\nOut[23]:intercept 0.8488\nhorsepower 0.0074\ndtype: float64\nThis indicates that the bootstrap estimate for SE(βˆ ) is 0.85, and that\n0\nthe bootstrap estimate for SE(βˆ ) is 0.0074. As discussed in Section 3.1.2,\n1\nstandard formulas can be used to compute the standard errors for the\nregression coefficients in a linear model. These can be obtained using the\nsummarize() function from ISLP.sm. In[24]: hp_model.fit(Auto, Auto['mpg'])\nmodel_se = summarize(hp_model.results_)['std err']\nmodel_se\nOut[24]:intercept 0.717\nhorsepower 0.006\nName: std err, dtype: float64\nThe standard error estimates for\nβˆ\nand\nβˆ\nobtained using the formulas",
    "source": "book_islr",
    "section_title": "B=1000,"
  },
  {
    "instruction": "How is 0 different from 0 in data science?",
    "input": "",
    "output": "0: t\nThe number of degree k vertices increases whenever a new edge is added to a degree k−1\nvertex and decreases when a new edge is added to a degree k vertex. Reasoning as above,\nd (t) d (t)\nk−1 k\nd (t+1) = d (t)+2δ −2δ . 0: 1\np =\n0\n1+2δ\nand\np t p t\nk−1 k\n(t+1)p = p t+2δ −2δ\nk k\nt t\np = 2δp −2δp\nk k−1 k\n2δ\np = p\nk k−1\n1+2δ\n(cid:18)\n2δ\n(cid:19)k\n= p\n0\n1+2δ\n1\n(cid:18)\n2δ\n(cid:19)k\n= . (8.5)\n1+2δ 1+2δ\nThus, the model gives rise to a graph with a degree distribution that falls off exponentially\nfast with the degree.",
    "source": "book_fods",
    "section_title": "0 0 vs 0 0"
  },
  {
    "instruction": "How is logistic regression part 13 different from logistic regression part 14 in data science?",
    "input": "",
    "output": "logistic regression part 13: A detailed history of the logistic regression is given in Cramer (2002). The logistic function was developed as a model of population growth and named \"logistic\" by Pierre François Verhulst in the 1830s and 1840s, under the guidance of Adolphe Quetelet; see Logistic function § History for details. logistic regression part 14: There are large numbers of extensions:\n\nMultinomial logistic regression (or multinomial logit) handles the case of a multi-way categorical dependent variable (with unordered values, also called \"classification\"). The general case of having dependent variables with more than two values is termed polytomous regression.",
    "source": "web_wikipedia",
    "section_title": "Logistic regression part 13 vs Logistic regression part 14"
  },
  {
    "instruction": "How is is due to [kk10]. different from exercises in data science?",
    "input": "",
    "output": "is due to [kk10].: The definition of approximation-stability is from [BBG13] and [BBV08], and the anal-\nysis given in Section 7.6 is due to [BBG13]. Single-linkage clustering goes back at least to Florek et al. exercises: Exercise 7.1 Construct examples where using distances instead of distance squared gives\nbad results for Gaussian densities. For example, pick samples from two 1-dimensional\nunit variance Gaussians, with their centers 10 units apart.",
    "source": "book_fods",
    "section_title": "7.5.2 is due to [KK10]. vs 7.14 Exercises"
  },
  {
    "instruction": "How is 2 different from 4 4 4 in data science?",
    "input": "",
    "output": "2: Write a program to see what the solution looks like. 2. 4 4 4: Exercise 11.12 Prove that if scale functions defined by a dilation equation are orthogo-\nnal, then the sum of the even coefficients must equal the sum of the odd coefficients in the\n(cid:80) (cid:80)\ndilation equation. That is, c = c .",
    "source": "book_fods",
    "section_title": "2 2 vs 4 4 4 4"
  },
  {
    "instruction": "Explain fund managers at level 0.05, then how many null hypothe- in simple terms for a beginner in data science.",
    "input": "",
    "output": "ses do we reject? If we control the FDR for just these 10 fund\nmanagers at level 0.05, then how many null hypotheses do we\nreject? (f) Explain why the analysis in (e) is misleading.",
    "source": "book_islr",
    "section_title": "10 fund managers at level 0.05, then how many null hypothe-"
  },
  {
    "instruction": "What is is due to [kk10]. in data science?",
    "input": "",
    "output": "The definition of approximation-stability is from [BBG13] and [BBV08], and the anal-\nysis given in Section 7.6 is due to [BBG13]. Single-linkage clustering goes back at least to Florek et al. [FL(cid:32) P+51], and Wishart’s\nrobust version is from [Wis69].",
    "source": "book_fods",
    "section_title": "7.5.2 is due to [KK10]."
  },
  {
    "instruction": "How is smoothing splines and gams different from 7. moving beyond linearity in data science?",
    "input": "",
    "output": "smoothing splines and gams: A smoothing spline is a special case of a GAM with squared-error loss\nand a single feature. To fit GAMs in Python we will use the pygam package\npygam\nwhich can be installed via pip install pygam. 7. moving beyond linearity: Out[23]:LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True,\nmax_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001,\nverbose=False)\nThe pygam library generally expects a matrix of features so we reshape age\nto be a matrix (a two-dimensional array) instead of a vector (i.e. a one-\ndimensional array).",
    "source": "book_islr",
    "section_title": "7.8.3 Smoothing Splines and GAMs vs 318 7. Moving Beyond Linearity"
  },
  {
    "instruction": "When or why would a data scientist use regression models with a survival response?",
    "input": "",
    "output": "We now consider the task of fitting a regression model to survival data. As in Section 11.1, the observations are of the form (Y,δ), where Y =\nmin(T,C) is the (possibly censored) survival time, and δ is an indicator\nvariable that equals 1 if T C. Furthermore, X R p is a vector of p\n≤ ∈\nfeatures. We wish to predict the true survival time T.\nSince the observed quantity Y is positive and may have a long right\ntail, we might be tempted to fit a linear regression of log(Y) on X. But\nas the reader will surely guess, censoring again creates a problem since\nwe are actually interested in predicting T and not Y. To overcome this\ndifficulty, we instead make use of a sequential construction, similar to the\nconstructions of the Kaplan–Meier survival curve in Section 11.3 and the\nlog-rank test in Section 11.4.",
    "source": "book_islr",
    "section_title": "11.5 Regression Models With a Survival Response"
  },
  {
    "instruction": "Explain φ(s) = ≥ = = ω . in simple terms for a beginner in data science.",
    "input": "",
    "output": "min (cid:0) π(S),π(S ¯ ) (cid:1) |S|/n2 (cid:112) |S| n\nThus, in either case, after O(n2lnn/(cid:15)3) steps, |a(t)−π| ≤ (cid:15). 1\nA lattice in d-dimensions\nNext consider the n × n × ··· × n lattice in d-dimensions with a self-loop at each\nboundary point with probability 1−(number of neighbors)/2d. The self loops make all\nπ equal to n−d.",
    "source": "book_fods",
    "section_title": "Φ(S) = ≥ = = Ω ."
  },
  {
    "instruction": "What is approaches to control the family-wise error rate in data science?",
    "input": "",
    "output": "In this section, we briefly survey some approaches to control the FWER. We will illustrate these approaches on the Fund dataset, which records the\nmonthly percentage excess returns for 2,000 fund managers over n = 50\nmonths.13 Table 13.3 provides relevant summary statistics for the first five\nmanagers. We first present the Bonferroni method and Holm’s step-down proce-\ndure,whichareverygeneral-purposeapproachesforcontrollingtheFWER\nthat can be applied whenever m p-values have been computed, regardless\nof the form of the null hypotheses, the choice of test statistics, or the\n(in)dependence of the p-values.",
    "source": "book_islr",
    "section_title": "13.3.2 Approaches to Control the Family-Wise Error Rate"
  },
  {
    "instruction": "What is 1 2 t−1 in data science?",
    "input": "",
    "output": "already been defined. We start G at i = 1+ (end of G ). Let i = k. We will define l,\nt t t−1 t\nthe last element of G to be the largest integer greater than or equal to k and at most i\nt 0\nso that\nl\n(cid:88) εΦγ\nk\nπ ≤ .",
    "source": "book_fods",
    "section_title": "1 1 2 t−1"
  },
  {
    "instruction": "When or why would a data scientist use j?",
    "input": "",
    "output": "means that the power of the hypothesis test—the probability of correctly\npower\ndetecting a non-zero coefficient—is reduced by collinearity. Table3.11comparesthecoefficientestimatesobtainedfromtwoseparate\nmultiple regression models. The first is a regression of balance on age and\nlimit,andthesecondisaregressionofbalanceonratingandlimit.Inthe\nfirstregression,bothageandlimitarehighlysignificantwithverysmallp-\nvalues.Inthesecond,thecollinearitybetweenlimitandratinghascaused\nthe standard error for the limit coefficient estimate to increase by a factor\nof 12 and the p-value to increase to 0.701. In other words, the importance\nof the limit variable has been masked due to the presence of collinearity. To avoid such a situation, it is desirable to identify and address potential\ncollinearity problems while fitting the model.",
    "source": "book_islr",
    "section_title": "0 j"
  },
  {
    "instruction": "When or why would a data scientist use 2 m?",
    "input": "",
    "output": "these vectors you must find the vector Ax approximately, in the sense that you must find a\nj\nvector y satisfying |y −Ax | ≤ ε||A|| |x |. Here ε >0 is a given error bound. Describe\nj j j F j\n(cid:0) (cid:1)\nan algorithm that accomplishes this in time O d+n per x not counting the preprocessing\nε2 j\ntime. Hint: use Exercise 3.22. 70\nr\nExercise 3.24 Find the values of c to maximize (cid:80) c2σ2 where σ ≥ σ ≥ ... and\ni i i 1 2\ni=1\nr\n(cid:80) c2 = 1.\ni\ni=1\nExercise 3.25 (Document-Term Matrices): Suppose we have an m × n document-\nterm matrix A where each row corresponds to a document and has been normalized to\nlength one.",
    "source": "book_fods",
    "section_title": "1 2 m"
  },
  {
    "instruction": "Explain 1 in simple terms for a beginner in data science.",
    "input": "",
    "output": "xp + yq ≥ xy. p q\nThe left hand side of Young’s inequality, 1xp+ 1yq, is a convex combination of xp and yq\np q\nsince 1 and 1 sum to 1. ln(x) is a concave function for x > 0 and so the ln of the convex\np q\ncombination of the two elements is greater than or equal to the convex combination of\nthe ln of the two elements",
    "source": "book_fods",
    "section_title": "1 1"
  },
  {
    "instruction": "How is 2 3 4 5 different from p&s in data science?",
    "input": "",
    "output": "2 3 4 5: 003\n002\n001\n05\nEducation Level\negaW\nFIGURE 1.1. Wage data, which contains income survey information for men\nfrom the central Atlantic region of the United States. p&s: ni\negnahc\negatnecreP\nFIGURE1.2.Left:Boxplotsofthepreviousday’spercentagechangeintheS&P\nindexforthedaysforwhichthemarketincreasedordecreased,obtainedfromthe\nSmarket data. Center and Right: Same as left panel, but the percentage changes\nfor 2 and 3 days previous are shown.",
    "source": "book_islr",
    "section_title": "1 2 3 4 5 vs P&S"
  },
  {
    "instruction": "Explain j in simple terms for a beginner in data science.",
    "input": "",
    "output": "variance and better interpretation at the cost of a little bias. One possible\nalternative to the process described above is to build the tree only so long\nasthedecreaseintheRSSduetoeachsplitexceedssome(high)threshold. This strategy will result in smaller trees, but is too short-sighted since a\nseemingly worthless split early on in the tree might be followed by a very\ngood split—that is, a split that leads to a large reduction in RSS later on.",
    "source": "book_islr",
    "section_title": "1 J"
  },
  {
    "instruction": "When or why would a data scientist use topic models, nonnegative matrix factorization, hidden markov mod-?",
    "input": "",
    "output": "els, and Graphical Models 310\n9.1 Topic Models . . . . .",
    "source": "book_fods",
    "section_title": "9 Topic Models, Nonnegative Matrix Factorization, Hidden Markov Mod-"
  },
  {
    "instruction": "When or why would a data scientist use 2?",
    "input": "",
    "output": "q.\nthan one child. But this does not imply that the probability of extinction is zero. In fact,\nif p > 0, then with positive probability, the root will have no children and the process\n0\nwill become extinct right away. Recall that for G(n, d), the expected number of children\nn\nis d, so the parameter m plays the role of d.\nIf m < 1, then the slope of f (x) at x = 1 is less than one. This fact along with\nconvexity of f (x) implies that f (x) > x for x in [0,1) and there is no root of f(x) = x in\nthe interval [0,1).",
    "source": "book_fods",
    "section_title": "1 2"
  },
  {
    "instruction": "How is overview of the support vector classifier different from x 2 x 2 in data science?",
    "input": "",
    "output": "overview of the support vector classifier: In Figure 9.4, we see that observations that belong to two classes are not\nnecessarily separable by a hyperplane. In fact, even if a separating hyper-\nplane does exist, then there are instances in which a classifier based on\na separating hyperplane might not be desirable. x 2 x 2: FIGURE 9.5. Left: Two classes of observations are shown in blue and in\npurple, along with the maximal margin hyperplane.",
    "source": "book_islr",
    "section_title": "9.2.1 Overview of the Support Vector Classifier vs X 2 X 2"
  },
  {
    "instruction": "What is (2100−1) in data science?",
    "input": "",
    "output": "for x (cid:54)= 0. How does Gibbs sampling behave? 119\nExercise 4.16 Given a connected graph G and an integer k how would you generate\nconnected subgraphs of G with k vertices with probability proportional to the number of\nedges in the subgraph?",
    "source": "book_fods",
    "section_title": "2 (2100−1)"
  },
  {
    "instruction": "How is scenario 3 different from 2 t- in data science?",
    "input": "",
    "output": "scenario 3: 05.0\n54.0\n04.0\n53.0\n03.0\n52.0\n02.0\nFIGURE 4.11. Boxplots of the test error rates for each of the linear scenarios\ndescribed in the main text. 2 t-: The t-distribution has a similar shape to the normal distribution, but it\ndistribution\nhasatendencytoyieldmoreextremepoints—thatis,morepointsthatare\nfar from the mean. In this setting, the decision boundary was still linear,\nand so fit into the logistic regression framework.",
    "source": "book_islr",
    "section_title": "SCENARIO 3 vs 1 2 t-"
  },
  {
    "instruction": "How is g different from 0.2 0.4 0.6 0.8 1.0 in data science?",
    "input": "",
    "output": "g: )\nso that they are all on the same scale. In (6.6), the denominator is the\nestimated standard deviation of the jth predictor. 0.2 0.4 0.6 0.8 1.0: rorrE\nderauqS\nnaeM\n06\n05\n04\n03\n02\n01\n0\nλ !β ˆ λ R!2/!β ˆ !2\nFIGURE 6.5. Squared bias (black), variance (green), and test mean squared\nerror (purple) for the ridge regression predictions on a simulated data set, as a\nfunctionofλand $ βˆ λ R $ 2 / $ βˆ $ 2.Thehorizontaldashedlinesindicatetheminimum\npossible MSE.",
    "source": "book_islr",
    "section_title": "G vs 0.0 0.2 0.4 0.6 0.8 1.0"
  },
  {
    "instruction": "Explain a perspective on symbolic machine learning in physical sciences in simple terms for a beginner in data science.",
    "input": "",
    "output": "Machine learning is rapidly making its pathway across all of the natural sciences, including physical sciences. The rate at which ML is impacting non-scientific disciplines is incomparable to that in the physical sciences. This is partly due to the uninterpretable nature of deep neural networks.",
    "source": "web_arxiv",
    "section_title": "A Perspective on Symbolic Machine Learning in Physical Sciences"
  },
  {
    "instruction": "Explain one-versus- in simple terms for a beginner in data science.",
    "input": "",
    "output": "SVMs, each of which compares a pair of classes. For example, one such one\n’ (\nSVM might compare the kth class, coded as +1, to the k th class, coded\n$\nas 1. We classify a test observation using each of the K classifiers, and\n− 2\nwe tally the number of times that the test observation is assigned to each\n’ (\nof the K classes.",
    "source": "book_islr",
    "section_title": "2 one-versus-"
  },
  {
    "instruction": "How is 2. statistical learning different from what is statistical learning? 19 in data science?",
    "input": "",
    "output": "2. statistical learning: In\nco\nm\ne\nYears\nof\nEducation\nSeniority\nFIGURE 2.3. The plot displays income as a function of years of education\nand seniority in the Income data set. what is statistical learning? 19: • Whichpredictorsareassociatedwiththeresponse?Itisoftenthecase\nthatonlyasmallfractionoftheavailablepredictorsaresubstantially\nassociated with Y. Identifying the few important predictors among a\nlarge set of possible variables can be extremely useful, depending on\nthe application. • What is the relationship between the response and each predictor?",
    "source": "book_islr",
    "section_title": "18 2. Statistical Learning vs 2.1 What Is Statistical Learning? 19"
  },
  {
    "instruction": "When or why would a data scientist use x 2 x 2?",
    "input": "",
    "output": "FIGURE 9.5. Left: Two classes of observations are shown in blue and in\npurple, along with the maximal margin hyperplane. Right: An additional blue\nobservation has been added, leading to a dramatic shift in the maximal margin\nhyperplane shown as a solid line. The dashed line indicates the maximal margin\nhyperplane that was obtained in the absence of this additional point. • Greater robustness to individual observations, and\n• Better classification of most of the training observations.",
    "source": "book_islr",
    "section_title": "X 2 X 2"
  },
  {
    "instruction": "When or why would a data scientist use 3693142.930 2987.007 157103.978 2.993 42.448 0.000?",
    "input": "",
    "output": "There is a (verbose) summary() method for the GAM fit. (We do not\nreproduce it here.) In[39]: gam_full.summary()\nWecanmakepredictionsfromgamobjects,justlikefromlmobjects,using\nthe predict() method for the class gam. Here we make predictions on the\ntraining set. In[40]: Yhat = gam_full.predict(Xgam)\nIn order to fit a logistic regression GAM, we use LogisticGAM() from\nLogisticGAM()\npygam.",
    "source": "book_islr",
    "section_title": "2 3693142.930 2987.007 157103.978 2.993 42.448 0.000"
  },
  {
    "instruction": "When or why would a data scientist use matrix completion?",
    "input": "",
    "output": "We now re-create the analysis carried out on the USArrests data in Sec-\ntion 12.3. WesawinSection12.2.2thatsolvingtheoptimizationproblem(12.6)on\na centered data matrix X is equivalent to computing the first M principal\ncomponentsof thedata. Weuse ourscaled andcenteredUSArrests dataas\nX below. The singular value decomposition (SVD) is a general algorithm\nsingular\nfor solving (12.6). valuede-\ncomposition\nIn[21]: X = USArrests_scaled\nU, D, V = np.linalg.svd(X, full_matrices=False) svd()\nU.shape, D.shape, V.shape\nOut[21]:((50, 4), (4,), (4, 4))\nThe np.linalg.svd() function returns three components, U, D and V. The\nnp.linalg.",
    "source": "book_islr",
    "section_title": "12.5.2 Matrix Completion"
  },
  {
    "instruction": "Explain dropout learning in simple terms for a beginner in data science.",
    "input": "",
    "output": "The second row in Table 10.1 is labeled dropout. This is a relatively new\ndropout\nand efficient form of regularization, similar in some respects to ridge reg-\nularization. Inspired by random forests (Section 8.2), the idea is to ran-\ndomly remove a fraction φ of the units in a layer when fitting the model.",
    "source": "book_islr",
    "section_title": "10.7.3 Dropout Learning"
  },
  {
    "instruction": "When or why would a data scientist use sat-solvers in practice?",
    "input": "",
    "output": "While the SAT problem is NP-complete, a number of algorithms have been developed\nthat perform extremely well in practice on SAT formulas arising in a range of applica-\ntions. Such applications include hardware and software verification, creating action plans\nforrobotsandrobotteams, solvingcombinatorialpuzzles, andevenprovingmathematical\ntheorems. Broadly, therearetwoclassesofsolvers: completesolversandincompletesolvers. Com-\nplete solvers are guaranteed to find a satisfying assignment whenever one exists; if they\ndo not return a solution, then you know the formula is not satisfiable. Complete solvers\nare often based on some form of recursive tree search.",
    "source": "book_fods",
    "section_title": "8.7.1 SAT-solvers in practice"
  },
  {
    "instruction": "What is the marketing plan 109 in data science?",
    "input": "",
    "output": "In the Credit data, a regression of balance on age, rating, and limit\nindicates that the predictors have VIF values of 1.01, 160.67, and 160.59. As we suspected, there is considerable collinearity in the data! When faced with the problem of collinearity, there are two simple solu-\ntions.",
    "source": "book_islr",
    "section_title": "3.4 The Marketing Plan 109"
  },
  {
    "instruction": "How is 12 different from i2 in data science?",
    "input": "",
    "output": "12: matching for the bipartite graph. If the factor graph was a tree, then the message from a variable node x to its parent\nis a message g(x) that gives the maximum value for the subtree for each value of x. i2: i(cid:54)=1\nc = 1, then the message is zero. Thus, we get\n12\n−maxα(i,2)x+maxα(i,2)\ni(cid:54)=1 i(cid:54)=1\nand send the coefficient −maxα(i,2).",
    "source": "book_fods",
    "section_title": "11 12 vs 12 i2"
  },
  {
    "instruction": "When or why would a data scientist use (cid:16) 1(cid:17) 1?",
    "input": "",
    "output": "= + 1− . n(1−p)n−1 n 1−p\nFor p = clnn with c < 1, lim E(x) = ∞ and\nn\nn→∞\n(cid:34) (cid:35)\nE(x2) 1 (cid:16) 1(cid:17) 1 (cid:16) lnn(cid:17)\nlim = lim + 1− = lim 1+c = o(1)+1. n→∞ E2(x) n→∞ n1−c n 1−clnn n→∞ n\nn\n259\nFigure 8.7: A degree three vertex with three adjacent degree two vertices. Graph cannot\nhave a Hamilton circuit. By the second moment argument, Corollary 8.4, the probability that x = 0 goes to zero\nimplying that almost all graphs have an isolated vertex.",
    "source": "book_fods",
    "section_title": "1 (cid:16) 1(cid:17) 1"
  },
  {
    "instruction": "When or why would a data scientist use lagrange multipliers?",
    "input": "",
    "output": "Lagrange multipliers are used to convert a constrained optimization problem into an un-\nconstrained optimization. Suppose we wished to maximize a function f(x) subject to a\nconstraint g(x) = c. The value of f(x) along the constraint g(x) = c might increase for\na while and then start to decrease. At the point where f(x) stops increasing and starts\nto decrease, the contour line for f(x) is tangent to the curve of the constraint g(x) = c.\nStated another way the gradient of f(x) and the gradient of g(x) are parallel. By introducing a new variable λ we can express the condition by ∇ f = λ∇ g and\nx x\ng = c. These two conditions hold if and only if\n(cid:0) (cid:1)\n∇ f (x)+λ(g(x)−c) = 0\nxλ\nThepartialwithrespecttoλestablishesthatg(x) = c. Wehaveconvertedtheconstrained\noptimization problem in x to an unconstrained problem with variables x and λ. 456\ng(x,y) = c\nf(x,y)\nFigure 12.3: In finding the minimum of f(x,y) within the ellipse, the path head towards\nthe minimum of f(x,y) until it hits the boundary of the ellipse and then follows the\nboundary of the ellipse until the tangent of the boundary is in the same direction as the\ncontour line of f(x,y).",
    "source": "book_fods",
    "section_title": "12.10.1 Lagrange multipliers"
  },
  {
    "instruction": "How is 2 different from 2 1 2 in data science?",
    "input": "",
    "output": "2: 47Σ is the standard notation for the covariance matrix. We will use it sparingly so as not to confuse\nwith the summation sign. 2 1 2: nandsmallpthebinomialdistributioncanbeapproximatedwiththePoissondistribution. The binomial distribution with mean np and variance np(1−p) can be approximated\nbythenormaldistributionwithmeannpandvariancenp(1−p).",
    "source": "book_fods",
    "section_title": "1 2 vs 1 2 1 2"
  },
  {
    "instruction": "How is machine learning part 1 different from machine learning part 2 in data science?",
    "input": "",
    "output": "machine learning part 1: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance. machine learning part 2: The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.",
    "source": "web_wikipedia",
    "section_title": "Machine learning part 1 vs Machine learning part 2"
  },
  {
    "instruction": "Explain 3693142.930 2987.007 3602.894 2.998 0.972 0.436 in simple terms for a beginner in data science.",
    "input": "",
    "output": "WefindthatthereiscompellingevidencethataGAMwithalinearfunction\nin year is better than a GAM that does not include year at all (p-value=\n0.002). However, there is no evidence that a non-linear function of year\nis needed (p-value=0.435). In other words, based on the results of this\nANOVA, is preferred.",
    "source": "book_islr",
    "section_title": "2 3693142.930 2987.007 3602.894 2.998 0.972 0.436"
  },
  {
    "instruction": "When or why would a data scientist use shrinkage methods?",
    "input": "",
    "output": "The subset selection methods described in Section 6.1 involve using least\nsquarestofitalinearmodelthatcontainsasubsetofthepredictors.Asan\nalternative,wecanfitamodelcontainingallppredictorsusingatechnique\nthatconstrainsorregularizesthecoefficientestimates,orequivalently,that\nshrinks the coefficient estimates towards zero. It may not be immediately\nobviouswhysuchaconstraintshouldimprovethefit,butitturnsoutthat\nshrinking the coefficient estimates can significantly reduce their variance. The two best-known techniques for shrinking the regression coefficients\ntowards zero are ridge regression and the lasso.",
    "source": "book_islr",
    "section_title": "6.2 Shrinkage Methods"
  },
  {
    "instruction": "How is the second moment different from s t s t s s in data science?",
    "input": "",
    "output": "the second moment: This section focuses on computing the second moment of a stream with symbols from\n{1,2,...,m}. Let f denote the number of occurrences of the symbol s in the stream,\ns\nand recall that the second moment of the stream is given by (cid:80)m f2. s t s t s s: s=1 t=s+1 s=1\nm m m\n(cid:88) (cid:88) (cid:88)\n= 6 f2f2+ f4\ns t s\ns=1 t=s+1 s=1\n(cid:32)\nm\n(cid:33)2\n(cid:88)\n≤ 3 f2 = 3E2(a). s\ns=1\nTherefore, Var(a) = E(a2)−E2(a) ≤ 2E2(a).",
    "source": "book_fods",
    "section_title": "6.2.4 The Second Moment vs 2 s t s t s s"
  },
  {
    "instruction": "Explain 3. linear regression in simple terms for a beginner in data science.",
    "input": "",
    "output": "iii. ForafixedvalueofIQandGPA,highschoolgraduatesearn\nmore, on average, than college graduates provided that the\nGPA is high enough. iv.",
    "source": "book_islr",
    "section_title": "128 3. Linear Regression"
  },
  {
    "instruction": "When or why would a data scientist use 0m 0j?",
    "input": "",
    "output": "biomarker among mice in the control group equals the expected value of the\njth biomarker among mice in the treatment group. When conducting mul-\ntiple testing,weneedtobeverycarefulabouthowweinterprettheresults,\nin order to avoid erroneously rejecting far too many null hypotheses. This chapter discusses classical as well as more contemporary ways to\nconductmultipletestinginabig-datasetting.InSection13.2,wehighlight\nthechallengesassociated withmultipletesting. Classicalsolutionsto these\n© Springer Nature Switzerland AG 2023 557\nG. James et al., An Introduction to Statistical Learning, Springer Texts in Statistics,\nhttps://doi.org/10.1007/978-3-031-38747-0_13",
    "source": "book_islr",
    "section_title": "01 0m 0j"
  },
  {
    "instruction": "How is 1 different from 1− g in data science?",
    "input": "",
    "output": "1: t\nFor k >1, n (t) increases when two smaller components whose sizes sum to k are joined\nk\nby an edge and decreases when a vertex in a component of size k is chosen for attachment. The probability that a vertex selected at random will be in a size k component is kn k (t). 1− g: g(cid:48) = x. 2δ 1−g\n290\nPhase transition for nonfinite components\nThe generating function g(x) contains information about the finite components of the\ngraph.",
    "source": "book_fods",
    "section_title": "1 1 vs 1 1− g"
  },
  {
    "instruction": "Explain 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "- - #2 norm\n“ell 2”) of a vector, and is defined as β = p β 2. It measures the\n- - 2 j=1 j\ndistance of β from zero. As λ increases, theG% 2)norm of βˆ\nλ\nR will always\ndecrease,andsowill βˆR / βˆ .Thelatterquantityrangesfrom1(when\n- λ- 2 - - 2\nλ = 0, in which case the ridge regression coefficient estimate is the same\nas the least squares estimate, and so their % norms are the same) to 0\n2\n(when λ = , in which case the ridge regression coefficient estimate is a\n∞\nvectorofzeros,with% normequaltozero).Therefore,wecanthinkofthe\n2\nx-axis in the right-hand panel of Figure 6.4 as the amount that the ridge\nregression coefficient estimates have been shrunken towards zero; a small\nvalue indicates that they have been shrunken very close to zero.",
    "source": "book_islr",
    "section_title": "2 2"
  },
  {
    "instruction": "Explain n 1 n−1 n 2 3 1 2 1 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Let M = M D M D ···M D M D m . Assuming that M’s principal eigenvalue is\nn 1 n−1 n 2 3 1 2 1\nunique, the message passing will converge to the principal vector of M. The rate of con-\nvergences depends on the ratio of the first and second eigenvalues. An argument analogous to the above concerning the messages going clockwise around\nthe loop applies to messages moving counter-clockwise around the loop.",
    "source": "book_fods",
    "section_title": "1 n 1 n−1 n 2 3 1 2 1"
  },
  {
    "instruction": "When or why would a data scientist use (cid:90) ∞?",
    "input": "",
    "output": "|E(ys)| = 1+ √ 2szs−(1/2)e−zdz\ni π\n0\n≤ 2ss!. The last inequality is from the Gamma integral. 24\nSince E(y ) = 0, Var(y ) = E(y2) ≤ 222 = 8. Unfortunately, we do not have |E(ys)| ≤\ni i i i\n8s!asrequiredinTheorem12.5. Tofixthisproblem,performonemorechangeofvariables,\nusing w = y /2.",
    "source": "book_fods",
    "section_title": "1 (cid:90) ∞"
  },
  {
    "instruction": "When or why would a data scientist use 30 40 50 60 70?",
    "input": "",
    "output": "052\n002\n051\n001\n05\nAge\negaW\nLinear Spline\nFIGURE 7.3. Various piecewise polynomials are fit to a subset of the Wage\ndata, with a knot at age=50. TopLeft: The cubic polynomials are unconstrained. Top Right: The cubic polynomials are constrained to be continuous at age=50. BottomLeft:Thecubicpolynomialsareconstrainedtobecontinuous,andtohave\ncontinuous first and second derivatives.",
    "source": "book_islr",
    "section_title": "20 30 40 50 60 70"
  },
  {
    "instruction": "What is c=1) in data science?",
    "input": "",
    "output": "svm_flex.fit(X_train, y_train)\nfig, ax = subplots(figsize=(8,8))\nroc_curve(svm_flex,\nX_train,\ny_train,\nname='Training $\\gamma=50$',\ncolor='r',\nax=ax);\nHowever, these ROC curves are all on the training data. We are really\nmore interested in the level of prediction accuracy on the test data. When\nwe compute the ROC curves on the test data, the model with γ = 0.5\nappears to provide the most accurate results.",
    "source": "book_islr",
    "section_title": "C=1)"
  },
  {
    "instruction": "Explain the support vector machine in simple terms for a beginner in data science.",
    "input": "",
    "output": "The support vector machine (SVM) is an extension of the support vector\nsupport\nclassifier that results from enlarging the feature space in a specific way,\nvector\nusing kernels. We will now discuss this extension, the details of which are machine\nsomewhat complex and beyond the scope of this book. However, the main kernel\nideaisdescribedinSection9.3.1:wemaywanttoenlargeourfeaturespace\nin order to accommodate a non-linear boundary between the classes.",
    "source": "book_islr",
    "section_title": "9.3.2 The Support Vector Machine"
  },
  {
    "instruction": "Describe the typical steps involved in 11. survival analysis and censored data in a data science workflow.",
    "input": "",
    "output": "Applied\n10. This exercise focuses on the brain tumor data, which is included in\nthe ISLP library. (a) Plot the Kaplan-Meier survival curve with 1 standard error\n±\nbands,usingtheKaplanMeierFitter()estimatorinthelifelines\npackage. (b) Draw a bootstrap sample of size n = 88 from the pairs (y ,δ ),\ni i\nandcomputetheresultingKaplan-Meiersurvivalcurve.Repeat\nthisprocessB =200times.Usetheresultstoobtainanestimate\nofthestandarderroroftheKaplan-Meiersurvivalcurveateach\ntimepoint. Compare this to the standard errors obtained in (a). (c) Fit a Cox proportional hazards model that uses all of the pre-\ndictors to predict survival.",
    "source": "book_islr",
    "section_title": "502 11. Survival Analysis and Censored Data"
  },
  {
    "instruction": "Explain f = − . (3.24) in simple terms for a beginner in data science.",
    "input": "",
    "output": "RSS/(n p 1)\n− −\nNotice that in Table 3.4, for each individual predictor a t-statistic and\na p-value were reported. These provide information about whether each\nindividualpredictorisrelatedtotheresponse,afteradjustingfortheother\npredictors. It turns out that each of these is exactly equivalent7 to the F-\ntest that omits that single variable from the model, leaving all the others\nin—i.e.q=1in(3.24).Soitreportsthepartialeffectofaddingthatvariable\nto the model.",
    "source": "book_islr",
    "section_title": "F = − . (3.24)"
  },
  {
    "instruction": "Describe the typical steps involved in cnf in a data science workflow.",
    "input": "",
    "output": "CNF-sat, 279\nEffective resistance, 105\nCohesion, 232\nEigenvalue, 437\n466\nEigenvector, 54, 437 with preferential attachment, 293\nElectrical network, 97 without preferential attachment, 287\nErdo¨s R´enyi, 245\nHo¨lder’s inequality, 414, 416\nError correcting codes, 190\nHaar wavelet, 386\nEscape probability, 101\nHarmonic function, 98\nEuler’s constant, 108\nHash function\nEvent, 420\nuniversal, 184\nExpected degree\nHeavy tail, 248\nvertex, 245\nHidden Markov model, 332\nExpected value, 421\nHitting time, 102, 114\nExponential generating function, 454\nExtinct families\nImmortality probability, 274\nsize, 276\nIncoherent, 368, 371\nExtinction probability, 272, 274\nIncreasing property, 253, 270\nunsatisfiability, 279\nFinite fields, 457\nIndependence\nFirst moment method, 254\nlimited way, 190\nFourier transform, 370, 455\nIndependent, 421\nFrequency domain, 371\nIndicator random variable, 257\nG(n,p), 245 of triangle, 251\nGamma function, 18 Indicator variable, 422\nGamma function , 415 Ising model, 352\nGaussian, 23, 424, 456 Isolated vertices, 259, 266\nfitting to data, 29 number of, 259\ntail, 419\nJensen’s inequality, 418\nGaussians\nJohnson-Lindenstrauss lemma, 25, 26\nsparating, 27\nGeneral tail bounds, 433\nk-clustering, 215\nGenerating function, 272\nk-means clustering algorithm, 211\ncomponent size, 288\nKernel methods, 228\nfor sum of two variables, 272\nKirchhoff’s law, 99\nGenerating functions, 451\nKleinberg, 295\nGenerating points in the unit ball, 22\nGeometric mean, 417 Lagrange, 456\nGiant component, 246, 253, 259, 261, 266 Laplacian, 70\nGibbs sampling, 84 Law of large numbers, 12, 14\nGraph Learning, 129\nconnecntivity, 265 Linearity of expectation, 251, 421\nresistance, 108 Lloyd’s algorithm, 211\nGraphical model, 337 Local algorithm, 295\nGreedy Long-term probabilities, 80\nk-clustering, 215\nm-fold, 270\nGrowth models, 286\n467\nMarkov chain, 77 Power law distribution, 248\nstate, 82 Power method, 51\nMarkov Chain Monte Carlo, 78 Power-law distribution, 284\nMarkov random field, 340 Pru¨fer, 459\nMarkov’s inequality, 13 Principle component analysis, 56\nMatrix Probability density function, 420\nmultiplication Probability distribution function, 420\nby sampling, 193 Psuedo random, 191\ndiagonalizable, 438 Pure-literal heuristic, 280\nsimilar, 437\nQueue, 281\nMaximum cut problem, 63\narrival rate, 281\nMaximum likelihood estimation, 429\nMaximum likelihood estimator, 29\nRadon, 152\nMaximum principle, 98\nRandom graph, 245",
    "source": "book_fods",
    "section_title": "CNF"
  },
  {
    "instruction": "Explain time series forecasting in simple terms for a beginner in data science.",
    "input": "",
    "output": "Figure 10.14 shows historical trading statistics from the New York Stock\nExchange.ShownarethreedailytimeseriescoveringtheperiodDecember\n3, 1962 to December 31, 1986:18\n17An IMDb leaderboard can be found at https://paperswithcode.com/sota/\nsentiment-analysis-on-imdb. 18ThesedatawereassembledbyLeBaronandWeigend(1998)IEEETransactionson\nNeural Networks,9(1):213–220.",
    "source": "book_islr",
    "section_title": "10.5.2 Time Series Forecasting"
  },
  {
    "instruction": "Explain d 1 d √ in simple terms for a beginner in data science.",
    "input": "",
    "output": "of variables: y = x2 −1. We want to bound the probability that |y +...+y | ≥ β d.\ni i 1 d\nNotice that E(y ) = E(x2) − 1 = 0. To apply Theorem 12.5, we need to bound the sth\ni i\nmoments of y .",
    "source": "book_fods",
    "section_title": "1 d 1 d √"
  },
  {
    "instruction": "How is 1 ··· 1 different from 1 ··· 1 in data science?",
    "input": "",
    "output": "1 ··· 1:  a b ··· c \n \n a2 b2 ··· c2 \n \n. . 1 ··· 1:  a b ··· c \n \n a2 b2 ··· c2 \n \n. .",
    "source": "book_fods",
    "section_title": "1 1 ··· 1 vs 1 1 ··· 1"
  },
  {
    "instruction": "How is φ(s) = . different from φ(s) in data science?",
    "input": "",
    "output": "φ(s) = .: (cid:0) ¯ (cid:1)\nmin π(S),π(S)\nThere is a simple interpretation of Φ(S). Suppose without loss of generality that π(S) ≤\n¯\nπ(S). φ(s): ¯\nClearly, to be close to the stationary distribution, we must at least get to S once. So,\nmixing time is lower bounded by 1/Φ(S).",
    "source": "book_fods",
    "section_title": "Φ(S) = . vs Φ(S)"
  },
  {
    "instruction": "How is clustering a mixture of spherical gaussians different from 2 k 1 2 k in data science?",
    "input": "",
    "output": "clustering a mixture of spherical gaussians: Clustering is the task of partitioning a set of points into k subsets or clusters where\neach cluster consists of nearby points. Different definitions of the quality of a clustering\nlead to different solutions. 2 k 1 2 k: numbers called mixture weights that add up to one. Clearly, f is a probability density\nand integrates to one.",
    "source": "book_fods",
    "section_title": "3.9.3 Clustering a Mixture of Spherical Gaussians vs 1 2 k 1 2 k"
  },
  {
    "instruction": "When or why would a data scientist use 1.0 15?",
    "input": "",
    "output": "Next we compute the predictions at newX, and view them by extracting\nthe predicted_mean attribute. In[17]: new_predictions = results.get_prediction(newX);\nnew_predictions.predicted_mean\nOut[17]:array([29.80359411, 25.05334734, 20.30310057])\nWe can produce confidence intervals for the predicted values. In[18]: new_predictions.conf_int(alpha=0.05)\nOut[18]:array([[29.00741194, 30.59977628],\n[24.47413202, 25.63256267],\n[19.73158815, 20.87461299]])\nPrediction intervals are computing by setting obs=True:\nIn[19]: new_predictions.conf_int(obs=True, alpha=0.05)\nOut[19]:array([[17.56567478, 42.04151344],\n[12.82762635, 37.27906833],\n[ 8.0777421 , 32.52845905]])\nForinstance,the95%confidenceintervalassociatedwithanlstatvalueof\n10 is (24.47, 25.63), and the 95% prediction interval is (12.82, 37.28). As\nexpected, the confidence and prediction intervals are centered around the\nsame point (a predicted value of 25.05 for medv when lstat equals 10), but\nthe latter are substantially wider. Next we will plot medv and lstat using DataFrame.plot.scatter(), and\n.plot.",
    "source": "book_islr",
    "section_title": "2 1.0 15"
  },
  {
    "instruction": "Explain assessing model accuracy 35 in simple terms for a beginner in data science.",
    "input": "",
    "output": "our estimate\nfˆ\nto the training observations:\nn\n1\nI(y =yˆ). (2.8)\ni i\nn %\ni=1\n0\nHere yˆ is the predicted class label for the ith observation using fˆ . And\ni\nI(y =yˆ)isanindicatorvariablethatequals1ify =yˆ andzeroify =yˆ.",
    "source": "book_islr",
    "section_title": "2.2 Assessing Model Accuracy 35"
  },
  {
    "instruction": "What is φ(s) = . in data science?",
    "input": "",
    "output": "(cid:0) ¯ (cid:1)\nmin π(S),π(S)\nThere is a simple interpretation of Φ(S). Suppose without loss of generality that π(S) ≤\n¯\nπ(S). Then, we may write Φ(S) as\n(cid:88) π (cid:88)\nx\nΦ(S) = p .",
    "source": "book_fods",
    "section_title": "Φ(S) = ."
  },
  {
    "instruction": "How is shrinkage methods different from 0 in data science?",
    "input": "",
    "output": "shrinkage methods: The subset selection methods described in Section 6.1 involve using least\nsquarestofitalinearmodelthatcontainsasubsetofthepredictors.Asan\nalternative,wecanfitamodelcontainingallppredictorsusingatechnique\nthatconstrainsorregularizesthecoefficientestimates,orequivalently,that\nshrinks the coefficient estimates towards zero. It may not be immediately\nobviouswhysuchaconstraintshouldimprovethefit,butitturnsoutthat\nshrinking the coefficient estimates can significantly reduce their variance. 0:  \nRidgeregressionisverysimilartoleastsquares,exceptthatthecoefficients\nridge\nareestimatedbyminimizingaslightlydifferentquantity.Inparticular,the\nregression\nridge regression coefficient estimates\nβˆR\nare the values that minimize\n2\nn p p p\ny β β x +λ β2 =RSS+λ β2, (6.5)\n i − 0 − j ij  j j\ni=1 j=1 j=1 j=1",
    "source": "book_islr",
    "section_title": "6.2 Shrinkage Methods vs 0 0"
  },
  {
    "instruction": "How is 4. classification different from 0.1 0.2 0.3 0.4 0.5 in data science?",
    "input": "",
    "output": "4. classification: True default status\nNo Yes Total\nPredicted No 9432 138 9570\ndefault status Yes 235 195 430\nTotal 9667 333 10000\nTABLE 4.5. A confusion matrix compares the LDA predictions to the true\ndefault statuses for the 10,000 training observations in the Default data set,\nusing a modified threshold value that predicts default for any individuals whose\nposterior default probability exceeds 20%. 0.1 0.2 0.3 0.4 0.5: 6.0\n4.0\n2.0\n0.0\nThreshold\netaR\nrorrE\nFIGURE 4.7. FortheDefaultdataset,errorratesareshownasafunctionof\nthethresholdvaluefortheposteriorprobabilitythatisusedtoperformtheassign-\nment.",
    "source": "book_islr",
    "section_title": "154 4. Classification vs 0.0 0.1 0.2 0.3 0.4 0.5"
  },
  {
    "instruction": "When or why would a data scientist use lab: linear regression 123?",
    "input": "",
    "output": "Out[26]:Index(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis',\n'rad', 'tax', 'ptratio', 'lstat'],\ndtype='object')\nWe can now fit the model with all the variables in terms using the same\nmodel matrix builder. In[27]: X = MS(terms).fit_transform(Boston)\nmodel = sm.OLS(y, X)\nresults = model.fit()\nsummarize(results)\nOut[27]: coef std err t P>|t|\nintercept 41.6173 4.936 8.431 0.000\ncrim -0.1214 0.033 -3.678 0.000\nzn 0.0470 0.014 3.384 0.001\nindus 0.0135 0.062 0.217 0.829\nchas 2.8400 0.870 3.264 0.001\nnox -18.7580 3.851 -4.870 0.000\nrm 3.6581 0.420 8.705 0.000\nage 0.0036 0.013 0.271 0.787\ndis -1.4908 0.202 -7.394 0.000\nrad 0.2894 0.067 4.325 0.000\ntax -0.0127 0.004 -3.337 0.001\nptratio -0.9375 0.132 -7.091 0.000\nlstat -0.5520 0.051 -10.897 0.000\nWhatifwewouldliketoperformaregressionusingallofthevariablesbut\none? For example, in the above regression output, age has a high p-value. So we may wish to run a regression excluding this predictor. The following\nsyntax results in a regression using all predictors except age (output not\nshown).",
    "source": "book_islr",
    "section_title": "3.6 Lab: Linear Regression 123"
  },
  {
    "instruction": "What is 12. unsupervised learning in data science?",
    "input": "",
    "output": "In[23]: pcaUS.components_\nOut[23]:array([[ 0.53589947, 0.58318363, 0.27819087, 0.54343209],\n[ 0.41818087, 0.1879856 , -0.87280619, -0.16731864],\n[-0.34123273, -0.26814843, -0.37801579, 0.81777791],\n[ 0.6492278 , -0.74340748, 0.13387773, 0.08902432]])\nThematrixUcorrespondstoastandardizedversionofthePCAscorematrix\n(eachcolumnstandardizedtohavesum-of-squaresone).Ifwemultiplyeach\ncolumn of U by the corresponding element of D, we recover the PCA scores\nexactly (up to a meaningless sign flip). In[24]: (U * D[None,:])[:3]\nOut[24]:array([[-0.9856, 1.1334, -0.4443, 0.1563],\n[-1.9501, 1.0732, 2.04 , -0.4386],\n[-1.7632, -0.746 , 0.0548, -0.8347]])\nIn[25]: scores[:3]\nOut[25]:array([[ 0.9856, -1.1334, -0.4443, 0.1563],\n[ 1.9501, -1.0732, 2.04 , -0.4386],\n[ 1.7632, 0.746 , 0.0548, -0.8347]])\nWhile it would be possible to carry out this lab using the PCA() estimator,\nhere we use the np.linalg.svd() function in order to illustrate its use. We now omit 20 entries in the 50 4 data matrix at random.",
    "source": "book_islr",
    "section_title": "540 12. Unsupervised Learning"
  },
  {
    "instruction": "Explain 2 n i i in simple terms for a beginner in data science.",
    "input": "",
    "output": "a component for each value x can take on. To simplify the discussion assume the x take\ni i\non values 1,2,...,m.\nLet m be the message sent from vertex i to vertex i + 1 mod n. At vertex i + 1\ni\neach component of the message m is multiplied by the evidence y and the constraint\ni i+1\nfunction Ψ. This is done by forming a diagonal matrix D where the diagonal elements\ni+1\naretheevidenceandthenformingamatrixM whosejkth elementisΨ(x = j, x = k).",
    "source": "book_fods",
    "section_title": "1 2 n i i"
  },
  {
    "instruction": "Explain 8. tree-based methods in simple terms for a beginner in data science.",
    "input": "",
    "output": "predictorsofmixedtypes(i.e.qualitativeaswellasquantitative).Wehave\nnowseenfourapproachesforfittinganensembleoftrees:bagging,random\nforests, boosting, and BART. • In bagging, the trees are grown independently on random samples of\nthe observations. Consequently, the trees tend to be quite similar to\neachother.Thus,baggingcangetcaughtinlocaloptimaandcanfail\nto thoroughly explore the model space.",
    "source": "book_islr",
    "section_title": "354 8. Tree-Based Methods"
  },
  {
    "instruction": "How is lab: unsupervised learning 537 different from pca() in data science?",
    "input": "",
    "output": "lab: unsupervised learning 537: This scaling can be done via the StandardScaler() transform imported\nabove. We first fit the scaler, which computes the necessary means and\nstandard deviations and then apply it to our data using the transform\nmethod. pca(): In[8]: pcaUS = PCA()\n(By default, the PCA() transform centers the variables to have mean zero\nthough it does not scale them.) The transform pcaUS can be used to find\nthe PCA scores returned by fit().",
    "source": "book_islr",
    "section_title": "12.5 Lab: Unsupervised Learning 537 vs PCA()"
  },
  {
    "instruction": "When or why would a data scientist use 7. moving beyond linearity?",
    "input": "",
    "output": "ax.set_title('Partial dependence of wage on education',\nfontsize=20);\nax.set_xticklabels(Wage['education'].cat.categories, fontsize=8);\nANOVA Tests for Additive Models\nInallofourmodels,thefunctionofyearlooksratherlinear.Wecanperform\na series of ANOVA tests in order to determine which of these three models\nisbest:aGAMthatexcludesyear(\n1\n),aGAMthatusesalinearfunction",
    "source": "book_islr",
    "section_title": "322 7. Moving Beyond Linearity"
  },
  {
    "instruction": "What is lab: deep learning 435 in data science?",
    "input": "",
    "output": "parametrized neural network (many layers, and many hidden units),\nand then fitting all the way to zero training error. However, fitting\nto zero error is not always optimal, and whether it is advisable de-\npends on the signal-to-noise ratio. For instance, we may use ridge\nregularization to avoid overfitting a neural network, as in (10.31).",
    "source": "book_islr",
    "section_title": "10.9 Lab: Deep Learning 435"
  },
  {
    "instruction": "How is (l) different from k − z in data science?",
    "input": "",
    "output": "(l): inStep5—actuallydependsonthevaluesofall mofthep-values. (Seethe\ndefinition of L in (13.7).) k − z: 5. Reject all null hypotheses H for which p <p .",
    "source": "book_islr",
    "section_title": "(L) vs K − Z"
  },
  {
    "instruction": "When or why would a data scientist use 30 40 50 60 70 80?",
    "input": "",
    "output": "02.0\n51.0\n01.0\n50.0\n00.0\n| ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| |\n||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||||\nAge\n)egA|052>egaW(rP\nFIGURE 7.5. A natural cubic spline function with four degrees of freedom is\nfit to the Wage data. Left: A spline is fit to wage (in thousands of dollars) as\na function of age. Right: Logistic regression is used to model the binary event\nwage>250 as a function of age. The fitted posterior probability of wage exceeding\n$250,000 is shown.",
    "source": "book_islr",
    "section_title": "20 30 40 50 60 70 80"
  },
  {
    "instruction": "How is 2 different from 2 n in data science?",
    "input": "",
    "output": "2: These are the hypotheses that we don’t want to output. Consider drawing the sample S\nof size n and let A be the event that h is consistent with S. Since every h has true error\ni i i\ngreater than or equal to (cid:15)\nProb(A ) ≤ (1−(cid:15))n.\ni\nIn other words, if we fix h and draw a sample S of size n, the chance that h makes no\ni i\nmistakes on S is at most the probability that a coin of bias (cid:15) comes up tails n times in a\nrow, which is (1−(cid:15))n. By the union bound over all i we have\nProb(∪ A ) ≤ |H|(1−(cid:15))n.\ni i\nUsing the fact that (1−(cid:15)) ≤ e−(cid:15), the probability that any hypothesis in H with true error\ngreater than or equal to (cid:15) has training error zero is at most |H|e−(cid:15)n. Replacing n by the\nsample size bound from the theorem statement, this is at most |H|e−ln|H|−ln(1/δ) = δ as\ndesired. 2 n: (cid:80)\ndom variables with Prob(x = 1) = p. Let s = x (equivalently, flip n coins of bias p\ni i i\nand let s be the total number of heads). For any 0 ≤ α ≤ 1,\nProb(s/n > p+α) ≤ e−2nα2\nProb(s/n < p−α) ≤ e−2nα2.",
    "source": "book_fods",
    "section_title": "1 2 vs 1 2 n"
  },
  {
    "instruction": "Describe the typical steps involved in |s| in a data science workflow.",
    "input": "",
    "output": "rate of the algorithm on S (the total number of mistakes of the algorithm divided by\n177\n(cid:113)\n|S|) will be at most err (h ) + O( Llog(L(T))), where h = argmin err (h)\nS L |S| L h:L(h)=L S\nis the pruning of T with L leaves of lowest error on S.\n3. In the above question, we assumed L was given. Explain how we can remove this as-\n(cid:104) (cid:113) (cid:105)\nsumption and achieve a bound of min err (h )+O( Llog(L(T))) by instantiating",
    "source": "book_fods",
    "section_title": "|S|"
  },
  {
    "instruction": "How is . a = bc. different from 1 1 ij 2 kl in data science?",
    "input": "",
    "output": ". a = bc.: 2. The entries of B and C are all nonnegative. 1 1 ij 2 kl: It remains to solve the nonlinear problem in 2r2 variables. There is a classical algo-\nrithm which solves such problems in time exponential only in r2 (polynomial in the other\nparameters).",
    "source": "book_fods",
    "section_title": "1. A = BC. vs 2 1 1 ij 2 kl"
  },
  {
    "instruction": "When or why would a data scientist use 2 p?",
    "input": "",
    "output": "variateGaussian(ormultivariatenormal)distribution,withaclass-specific\nmultivariate\nmeanvectorandacommoncovariancematrix.Webeginwithabriefreview\nGaussian\nof this distribution. ThemultivariateGaussiandistributionassumesthateachindividualpre-\ndictorfollowsaone-dimensionalnormaldistribution,asin(4.16),withsome\ncorrelation between each pair of predictors. Two examples of multivariate\nGaussian distributions with p = 2 are shown in Figure 4.5. The height of\nthesurfaceatanyparticularpointrepresentstheprobabilitythatbothX\n1\nand X fall in a small region around that point. In either panel, if the sur-\n2\nfaceiscutalongtheX axisoralongtheX axis,theresultingcross-section",
    "source": "book_islr",
    "section_title": "1 2 p"
  },
  {
    "instruction": "Explain the stock market data in simple terms for a beginner in data science.",
    "input": "",
    "output": "In this lab we will examine the Smarket data, which is part of the ISLP\nlibrary. This data set consists of percentage returns for the S&P 500 stock\nindex over 1,250 days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five\nprevious trading days, Lag1 through Lag5.",
    "source": "book_islr",
    "section_title": "4.7.1 The Stock Market Data"
  },
  {
    "instruction": "When or why would a data scientist use 2 1 2?",
    "input": "",
    "output": "×\nleaving them out tends to alter the meaning of the interaction. In the previous example, we considered an interaction between TV and\nradio, both of which are quantitative variables. However, the concept of\ninteractionsappliesjustaswelltoqualitativevariables,ortoacombination\nof quantitative and qualitative variables. In fact, an interaction between\na qualitative variable and a quantitative variable has a particularly nice\ninterpretation.ConsidertheCreditdatasetfromSection3.3.1,andsuppose\nthatwewishtopredictbalanceusingtheincome(quantitative)andstudent\n(qualitative) variables. In the absence of an interaction term, the model\ntakes the form\nβ if ith person is a student\n2\nbalancei β 0 +β 1 incomei +\n≈ × =0 if ith person is not a student\nβ +β if ith person is a student",
    "source": "book_islr",
    "section_title": "1 2 1 2"
  },
  {
    "instruction": "How is ward’s algorithm different from k-center clustering in data science?",
    "input": "",
    "output": "ward’s algorithm: Another popular heuristic for k-means clustering is Ward’s algorithm. Ward’s algo-\nrithm begins with each data point in its own cluster, and then repeatedly merges pairs of\nclusters until only k clusters remain. k-center clustering: In this section, instead of using the k-means clustering criterion, we use the k-center\ncriterion. Recall that the k-center criterion partitions the points into k clusters so as to\nminimize the maximum distance of any point to its cluster center.",
    "source": "book_fods",
    "section_title": "7.2.4 Ward’s Algorithm vs 7.3 k-Center Clustering"
  },
  {
    "instruction": "What is l in data science?",
    "input": "",
    "output": "When we refit our model using the LSTM architecture for the hidden\nlayer,theperformanceimprovedto87%ontheIMDbtestdata.Thisiscom-\nparable with the 88% achieved by the bag-of-words model in Section 10.4. We give details on fitting these models in Section 10.9.6. Despite this added LSTM complexity, our RNN is still somewhat “entry\nlevel”.",
    "source": "book_islr",
    "section_title": "L"
  },
  {
    "instruction": "When or why would a data scientist use x 2?",
    "input": "",
    "output": "FIGURE 12.12. An illustration of how to properly interpret a dendrogram\nwith nine observations in two-dimensional space. Left: a dendrogram generated\nusing Euclidean distance and complete linkage. Observations 5 and 7 are quite\nsimilar to each other, as are observations 1 and 6. However, observation 9 is no\nmore similar to observation 2 than it is to observations 8,5, and 7, even though\nobservations 9 and 2 are close together in terms of horizontal distance.",
    "source": "book_islr",
    "section_title": "X 2"
  },
  {
    "instruction": "When or why would a data scientist use 1?",
    "input": "",
    "output": "report the p-value associated with the coefficient β . Here, Y\n1\nrepresents Sales and X represents one of the other quantitative\nvariables. (b) Suppose we control the Type I error at level α = 0.05 for the\np-values obtained in (a). Which null hypotheses do we reject? (c) NowsupposewecontroltheFWERatlevel0.05forthep-values.",
    "source": "book_islr",
    "section_title": "0 1"
  },
  {
    "instruction": "When or why would a data scientist use m?",
    "input": "",
    "output": "compute a p-value for each of the m null hypotheses, as in Section 13.5.1,\nand then apply the Benjamini–Hochberg procedure of Section 13.4.2 to\nthese p-values. However, it turns out that we can do this in a more direct\nway, without even needing to compute p-values. Recall from Section 13.4 that the FDR is defined as E(V/R), using the\nnotation in Table 13.2. In order to estimate the FDR via re-sampling, we\nfirst make the following approximation:",
    "source": "book_islr",
    "section_title": "1 m"
  },
  {
    "instruction": "Explain random forests in simple terms for a beginner in data science.",
    "input": "",
    "output": "Random forests provide an improvement over bagged trees by way of a\nrandom\nsmall tweak that decorrelates the trees. As in bagging, we build a number\nforest\nofdecisiontreesonbootstrappedtrainingsamples.Butwhenbuildingthese\ndecisiontrees,eachtimeasplitinatreeisconsidered,a random sample of\nm predictors is chosen as split candidates from the full set of p predictors. Thesplitisallowedtouseonlyoneofthosempredictors.Afreshsampleof\nm predictors is taken at each split, and typically we choose m √p—that\n≈\nis,thenumberofpredictorsconsideredateachsplitisapproximatelyequal\ntothesquarerootofthetotalnumberofpredictors(4outofthe13forthe\nHeart data).",
    "source": "book_islr",
    "section_title": "8.2.2 Random Forests"
  },
  {
    "instruction": "Explain introduction in simple terms for a beginner in data science.",
    "input": "",
    "output": "In this chapter, we examine the Singular Value Decomposition (SVD) of a matrix. Consider each row of an n×d matrix A as a point in d-dimensional space. The singular\nvalue decomposition finds the best-fitting k-dimensional subspace for k = 1,2,3,..., for\nthe set of n data points.",
    "source": "book_fods",
    "section_title": "3.1 Introduction"
  },
  {
    "instruction": "How is importing packages different from lab: linear regression 117 in data science?",
    "input": "",
    "output": "importing packages: We import our standard libraries at this top level. In[1]: import numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\nNew imports\nThroughoutthislabwewillintroducenewfunctionsandlibraries.However,\nwe will import them here to emphasize these are the new code objects in\nthis lab. lab: linear regression 117: 'poly',\n'quit',\n'sm',\n'summarize']\nThis shows you everything that Python can find at the top level. There\nare certain objects like __builtins__ that contain references to built-in\nfunctions like print().",
    "source": "book_islr",
    "section_title": "3.6.1 Importing packages vs 3.6 Lab: Linear Regression 117"
  },
  {
    "instruction": "When or why would a data scientist use 2 n?",
    "input": "",
    "output": "i with probability proportional to the value of a . When we see an element, we do not\ni\nknow the probability with which to select it since the normalizing constant depends on\nall of the elements including those we have not yet seen. However, the following method\nworks. Let s be the sum of the a ’s seen so far. Maintain s and an index i selected\ni\nwith probability ai.",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "Explain 2 n2 i in simple terms for a beginner in data science.",
    "input": "",
    "output": "figurations. The energy of a configuration is a function f(v) of the configuration, not of\nanysinglespin. Acentralprobleminstatisticalmechanicsistosamplespinconfigurations\naccording to their probability.",
    "source": "book_fods",
    "section_title": "1 2 n2 i"
  },
  {
    "instruction": "When or why would a data scientist use leave-one-out cross-validation?",
    "input": "",
    "output": "Leave-one-outcross-validation(LOOCV)iscloselyrelatedtothevalidation\nleave-one-\nset approach of Section 5.1.1, but it attempts to address that method’s\nout\ndrawbacks. cross-\nLike the validation set approach, LOOCV involves splitting the set of validation\nobservations into two parts. However, instead of creating two subsets of\ncomparable size, a single observation (x ,y ) is used for the validation",
    "source": "book_islr",
    "section_title": "5.1.2 Leave-One-Out Cross-Validation"
  },
  {
    "instruction": "When or why would a data scientist use one-versus-all classification?",
    "input": "",
    "output": "The one-versus-all approach (also referred to as one-versus-rest) is an al-\none-versus-\nternative procedure for applying SVMs in the case of K > 2 classes. We all\nfit K SVMs, each time comparing one of the K classes to the remaining one-versus-\nK 1 classes. Let β ,β ,...,β denote the parameters that result from rest\n0k 1k pk\n−\nfittinganSVMcomparingthekthclass(codedas+1)totheothers(coded\nas 1). Let x denote a test observation. We assign the observation to the\n∗\n−\nclassforwhichβ +β x +β x + +β x islargest,asthisamounts\n0k 1k ∗1 2k ∗2\n···\npk ∗p\nto a high level of confidence that the test observation belongs to the kth\nclass rather than to any of the other classes.",
    "source": "book_islr",
    "section_title": "9.4.2 One-Versus-All Classification"
  },
  {
    "instruction": "Describe the typical steps involved in estimates of p(class is redx): in a data science workflow.",
    "input": "",
    "output": "|\n0.1,0.15,0.2,0.2,0.55,0.6,0.6,0.65,0.7, and 0.75. There are two common ways to combine these results together into a\nsingleclassprediction.Oneisthemajorityvoteapproachdiscussedin\nthis chapter. The second approach is to classify based on the average\nprobability.Inthisexample,whatisthefinalclassificationundereach\nof these two approaches? 6. Provide a detailed explanation of the algorithm that is used to fit a\nregression tree. Applied\n7.",
    "source": "book_islr",
    "section_title": "10 estimates of P(Class is RedX):"
  },
  {
    "instruction": "What is pru¨fer in data science?",
    "input": "",
    "output": "Here we prove that the number of labeled trees with n vertices is nn−2. By a labeled\ntree we mean a tree with n vertices and n distinct labels, each label assigned to one vertex. Theorem 12.28 The number of labeled trees with n vertices is nn−2.",
    "source": "book_fods",
    "section_title": "12.10.5 Pru¨fer"
  },
  {
    "instruction": "What is overfitting part 3 in data science?",
    "input": "",
    "output": "Usually, a learning algorithm is trained using some set of \"training data\": exemplary situations for which the desired output is known. The goal is that the algorithm will also perform well on predicting the output when fed \"validation data\" that was not encountered during its training. Overfitting is the use of models or procedures that violate Occam's razor, for example by including more adjustable parameters than are ultimately optimal, or by using a more complicated approach than is ultimately optimal.",
    "source": "web_wikipedia",
    "section_title": "Overfitting part 3"
  },
  {
    "instruction": "What is and growth function h[n] = o(n4). in data science?",
    "input": "",
    "output": "Intervals of the reals\nIntervals on the real line can shatter any set of two points but no set of three points\nsince the subset of the first and last points cannot be isolated. Thus, the VC-dimension\nof intervals is two. Also, H[n] = O(n2) since we have O(n2) choices for the left and right\nendpoints.",
    "source": "book_fods",
    "section_title": "4 and growth function H[n] = O(n4)."
  },
  {
    "instruction": "How is 12. unsupervised learning different from nci60 data example in data science?",
    "input": "",
    "output": "12. unsupervised learning: can then be used as a similarity (or affinity) matrix, i.e. so that one minus\nthe correlation matrix is the dissimilarity matrix used for clustering. nci60 data example: Unsupervised techniques are often used in the analysis of genomic data. In particular, PCA and hierarchical clustering are popular tools.",
    "source": "book_islr",
    "section_title": "546 12. Unsupervised Learning vs 12.5.4 NCI60 Data Example"
  },
  {
    "instruction": "What is 4. classification in data science?",
    "input": "",
    "output": "True class\nor Null + or Non-null Total\n−\nPredicted or Null True Neg. (TN) False Neg. (FN) N∗\n−\nclass + or Non-null False Pos.",
    "source": "book_islr",
    "section_title": "156 4. Classification"
  },
  {
    "instruction": "Explain 1 4 8 in simple terms for a beginner in data science.",
    "input": "",
    "output": "whether an email message is spam or not, and our features correspond to the presence\nor absence of different possible indicators of spam-ness, then this would correspond to\nthe belief that there is some subset of these indicators such that every spam email has at\nleast one of them and every non-spam email has none of them. Formally, let H denote\nthe class of disjunctions, and notice that |H| = 2d. So, by Theorem 5.3, it suffices to find\na consistent disjunction over a sample S of size\n1\n(cid:0) (cid:1)\n|S| = dln(2)+ln(1/δ) .",
    "source": "book_fods",
    "section_title": "8 1 4 8"
  },
  {
    "instruction": "How is assessing model accuracy 35 different from 0 in data science?",
    "input": "",
    "output": "assessing model accuracy 35: our estimate\nfˆ\nto the training observations:\nn\n1\nI(y =yˆ). (2.8)\ni i\nn %\ni=1\n0\nHere yˆ is the predicted class label for the ith observation using fˆ . 0: %\nwhereyˆ isthepredictedclasslabelthatresultsfromapplyingtheclassifier\n0\nto the test observation with predictor x . A good classifier is one for which\n0\nthe test error (2.9) is smallest.",
    "source": "book_islr",
    "section_title": "2.2 Assessing Model Accuracy 35 vs 0 0"
  },
  {
    "instruction": "What is other considerations in the regression model 95 in data science?",
    "input": "",
    "output": "assumptions. Here, we briefly examine some common classical approaches\nfor extending the linear model. Removing the Additive Assumption\nInourpreviousanalysisoftheAdvertisingdata,weconcludedthatbothTV\nandradioseemtobeassociatedwithsales.Thelinearmodelsthatformed\nthe basis for this conclusion assumed that the effect on sales of increasing\none advertising medium is independent of the amount spent on the other\nmedia.Forexample,thelinearmodel(3.20)statesthattheaverageincrease\nin sales associated with a one-unit increase in TV is always β 1 , regardless\nof the amount spent on radio.",
    "source": "book_islr",
    "section_title": "3.3 Other Considerations in the Regression Model 95"
  },
  {
    "instruction": "Explain leave-one-out cross-validation in simple terms for a beginner in data science.",
    "input": "",
    "output": "Leave-one-outcross-validation(LOOCV)iscloselyrelatedtothevalidation\nleave-one-\nset approach of Section 5.1.1, but it attempts to address that method’s\nout\ndrawbacks. cross-\nLike the validation set approach, LOOCV involves splitting the set of validation\nobservations into two parts. However, instead of creating two subsets of\ncomparable size, a single observation (x ,y ) is used for the validation",
    "source": "book_islr",
    "section_title": "5.1.2 Leave-One-Out Cross-Validation"
  },
  {
    "instruction": "Explain dimension reduction methods 257 in simple terms for a beginner in data science.",
    "input": "",
    "output": "−3 −2 −1 0 1 2 3\n06\n05\n04\n03\n02\n1st Principal Component\nnoitalupoP\n−3 −2 −1 0 1 2 3\n03\n52\n02\n51\n01\n5\n1st Principal Component\ngnidnepS\ndA\nFIGURE 6.16. Plotsofthefirstprincipalcomponentscoresz i1 versuspopand\nad. The relationships are strong.",
    "source": "book_islr",
    "section_title": "6.3 Dimension Reduction Methods 257"
  },
  {
    "instruction": "What is j in data science?",
    "input": "",
    "output": "means that the power of the hypothesis test—the probability of correctly\npower\ndetecting a non-zero coefficient—is reduced by collinearity. Table3.11comparesthecoefficientestimatesobtainedfromtwoseparate\nmultiple regression models. The first is a regression of balance on age and\nlimit,andthesecondisaregressionofbalanceonratingandlimit.Inthe\nfirstregression,bothageandlimitarehighlysignificantwithverysmallp-\nvalues.Inthesecond,thecollinearitybetweenlimitandratinghascaused\nthe standard error for the limit coefficient estimate to increase by a factor\nof 12 and the p-value to increase to 0.701.",
    "source": "book_islr",
    "section_title": "0 j"
  },
  {
    "instruction": "What is 2 2 in data science?",
    "input": "",
    "output": "←− Nearly all the volume\n← Vertex of hypercube\nFigure 2.4: Illustration of the relationship between the sphere and the cube in 2, 4, and\nd-dimensions. Since this is true for each of the d dimensions, by a union bound at most a O(1) ≤ 1\nd 2\nfraction of the volume of the ball lies outside the cube, completing the proof. Discussion.",
    "source": "book_fods",
    "section_title": "2 2 2"
  },
  {
    "instruction": "What is 0 in data science?",
    "input": "",
    "output": "| | ≥\nwhat if we wish to test 10 null hypotheses using two-sample t-statistics,\ninstead of just one? We will see in Section 13.3.2 that we can guarantee\nthat the FWER does not exceed 0.02 by rejecting only null hypotheses\nfor which the p-value falls below 0.002. This corresponds to a much more\nstringent cutpoint of 3.09 (i.e.",
    "source": "book_islr",
    "section_title": "0 0"
  },
  {
    "instruction": "When or why would a data scientist use 2?",
    "input": "",
    "output": "err (h) ≤ ln|H (err (c∗)+2(cid:15))|+ln . D D,χ unl\n|L| δ\nProof: By Hoeffding bounds, |U| is sufficiently large so that with probability at least\n1−δ/2, all h ∈ H have |err (h)−err (h)| ≤ (cid:15). Thus we have:\n(cid:99)unl unl\n{f ∈ H : err (f) ≤ err (c∗)+(cid:15)} ⊆ H (err (c∗)+2(cid:15)). (cid:99)unl unl D,χ unl\nThe given bound on |L| is sufficient so that with probability at least 1−δ, all h ∈ H with\nerr(h) = 0 and err (h) ≤ err (c∗) + (cid:15) have err (h) ≤ (cid:15); furthermore, err (c∗) ≤\n(cid:99) (cid:99)unl unl D (cid:99)unl\nerr (c∗)+(cid:15), so such a function h exists. Therefore, with probability at least 1−δ, the\nunl\nh ∈ H that optimizes err (h) subject to err(h) = 0 has err (h) ≤ (cid:15), as desired.",
    "source": "book_fods",
    "section_title": "1 2"
  },
  {
    "instruction": "How is |s| different from online learning in data science?",
    "input": "",
    "output": "|s|: where size(h) denotes the number of bits needed to describe h in the given language. Corollary 5.8 gives us the tradeoff we were looking for. online learning: So far we have been considering what is often called the batch learning scenario. You are\ngiven a “batch” of data—the training sample S—and your goal is to use it to produce\na hypothesis h that will have low error on new data, under the assumption that both S\nand the new data are sampled from some fixed distribution D. We now switch to the\nmore challenging online learning scenario where we remove the assumption that data is\nsampled from a fixed probability distribution, or from any probabilistic process at all.",
    "source": "book_fods",
    "section_title": "2|S| vs 5.8 Online Learning"
  },
  {
    "instruction": "When or why would a data scientist use i?",
    "input": "",
    "output": "p ji 1 2 0.85π i π i = 0.85π j p ji + 0. 2 85π i\nj i\nπ = 1.48π p\ni j ji\n0.15π 0.15π\nj i\nFigure 4.13: Impact on pagerank of adding a self loop\nlist of webpages in response to each search query. To do this, they have to solve two\nproblems at query time: (i) find the set of all webpages containing the query term(s) and\n(ii) rank the webpages and display them (or the top subset of them) in ranked order. (i)\nis done by maintaining a “reverse index” which we do not discuss here. (ii) cannot be\ndone at query time since this would make the response too slow.",
    "source": "book_fods",
    "section_title": "2 i"
  },
  {
    "instruction": "When or why would a data scientist use 102?",
    "input": "",
    "output": "to true), the clause is now deleted, and if we included x (the literal we had set to false),\n102\nwe decrease the residual length of the clause by one. At a general stage, suppose the fates of i variables have already been decided and\nn−i remain. The residual length of each clause is known. Among the clauses that are\nnot yet satisfied, choose a random shortest length clause. Among the n − i variables\nremaining, pick one uniformly at random, then pick it or its negation as the new literal.",
    "source": "book_fods",
    "section_title": "102 102"
  },
  {
    "instruction": "Explain lab: deep learning 447 in simple terms for a beginner in data science.",
    "input": "",
    "output": "In[42]: mnist_results = pd.read_csv(mnist_logger.experiment. metrics_file_path)\nfig, ax = subplots(1, 1, figsize=(6, 6))\nsummary_plot(mnist_results,\nax,\ncol='accuracy',\nylabel='Accuracy')\nax.set_ylim([0.5, 1])\nax.set_ylabel('Accuracy')\nax.set_xticks(np.linspace(0, 30, 7).astype(int));\nOnce again we evaluate the accuracy using the test() method of our\ntrainer. This model achieves 97% accuracy on the test data.",
    "source": "book_islr",
    "section_title": "10.9 Lab: Deep Learning 447"
  },
  {
    "instruction": "Explain 2. statistical learning in simple terms for a beginner in data science.",
    "input": "",
    "output": "In[46]: axes[0,1].plot(x, y, 'o')\naxes[1,2].scatter(x, y, marker='+')\nfig\nType subplots? to learn more about subplots(). To save the output of fig, we call its savefig() method.",
    "source": "book_islr",
    "section_title": "50 2. Statistical Learning"
  },
  {
    "instruction": "How is 0 different from 9. support vector machines in data science?",
    "input": "",
    "output": "0:  \nand with P(β) = p β2 for ridge regression and P(β) = p β for\nj=1 j j=1| j |\nthe lasso. In the case of (9.25) the loss function instead takes the form\n) )\nn\nL(X,y,β)= max[0,1 y (β +β x + +β x )]. 9. support vector machines: −6 −4 −2 0 2\nssoL\n8\n6\n4\n2\n0\nSVM Loss\nLogistic Regression Loss\ny i(β0+β1x i1+...+β p x ip)\nFIGURE 9.12. The SVM and logistic regression loss functions are compared,\nasafunctionofy\ni\n(β\n0\n+β\n1\nx\ni1\n+ +β\np\nx\nip\n).Wheny\ni\n(β\n0\n+β\n1\nx\ni1\n+ +β\np\nx\nip\n)is\n··· ···\ngreaterthan1,thentheSVMlossiszero,sincethiscorrespondstoanobservation\nthatisonthecorrectsideofthemargin.Overall,thetwolossfunctionshavequite\nsimilar behavior.",
    "source": "book_islr",
    "section_title": "0 0 vs 386 9. Support Vector Machines"
  },
  {
    "instruction": "Describe the typical steps involved in lab: multiple testing 589 in a data science workflow.",
    "input": "",
    "output": "multipletests() function (abbreviated mult_test()) can be used to carry\nout the Benjamini–Hochberg procedure. In[18]: fund_qvalues = mult_test(fund_pvalues, method = \"fdr_bh\")[1]\nfund_qvalues[:10]\nOut[18]:array([0.09, 0.99, 0.12, 0.92, 0.96, 0.08, 0.08, 0.08, 0.08,\n0.08])\nThe q-values output by the Benjamini–Hochberg procedure can be inter-\nq-values\npretedasthesmallestFDRthresholdatwhichwewouldrejectaparticular\nnull hypothesis. For instance, a q-value of 0.1 indicates that we can reject\nthe corresponding null hypothesis at an FDR of 10% or greater, but that\nwe cannot reject the null hypothesis at an FDR below 10%. If we control the FDR at 10%, then for how many of the fund managers\ncan we reject H :µ =0? 0,j j\nIn[19]: (fund_qvalues <= 0.1).sum()\nOut[19]:146\nWe find that 146 of the 2,000 fund managers have a q-value below 0.1;\ntherefore, we are able to conclude that 146 of the fund managers beat the\nmarket at an FDR of 10%. Only about 15 (10% of 146) of these fund\nmanagers are likely to be false discoveries.",
    "source": "book_islr",
    "section_title": "13.6 Lab: Multiple Testing 589"
  },
  {
    "instruction": "How is 2 n different from i i i 1 in data science?",
    "input": "",
    "output": "2 n: diagonal. Rather than working with A, it is easier to work with the diagonal matrix D.\nThis will be an important technique that will simplify many proofs. i i i 1: i=1\ncolumn of P and is the first eigenvector of A. Similarly λ is the minimum value of xTAx\nn\nsubject to the same conditions.",
    "source": "book_fods",
    "section_title": "1 2 n vs 1 i i i 1"
  },
  {
    "instruction": "When or why would a data scientist use otherwise .?",
    "input": "",
    "output": "3. Pruning: Let R = {j|aˆ = 1}. If any R strictly contains another, set all\ni ij i\nˆ\nentries of row i of A to zero. Theorem 9.7 For i = 1,2,...,d, let R = {j|aˆ = 1} at the end of the algorithm. Then,\ni ij\neach nonempty R = T , with l(i) as in (9.9).",
    "source": "book_fods",
    "section_title": "0 otherwise ."
  },
  {
    "instruction": "What is 2 n in data science?",
    "input": "",
    "output": "n\nunique point x, namely their centroid, which minimizes (cid:80) |a − x|2. Show examples\ni\ni=1\nn\n(cid:80)\nwhere the x minimizing |a −x| is not unique. (Consider just points on the real line.)",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "What is pd.plotting.scatter_matrix(), in data science?",
    "input": "",
    "output": "NaturalSpline(), 317, 319 62\nndim, 42 pd.qcut(), 314, 315\nnn.RNN(), 461 pd.read_csv(), 55, 556\nnormal(), 132, 286, 555 pd.Series(), 62\nnp, see numpy Pipeline(), 275\nnp.all(), 54, 180 plot(), 48, 61, 356, 490\nnp.allclose(), 190 plot.scatter(), 120\nnp.any(), 54 plot_gam(), 321\nnp.arange(), 51 plot_svm(), 398\nnp.argmax(), 122 PLSRegression(), 282\nnp.array(), 42 poly(), 125, 313, 327\nnp.concatenate(), 133 predict(),175,178,181,216,\nnp.corrcoef(), 46, 554 218, 323, 358\nnp.empty(), 224 predict_survival_function(),\nnp.isnan(), 268 493\nnp.ix_(), 53 print(), 40\nnp.linalg.svd(), 539 pvalues, 175\nnp.linspace(), 50 pygam, 307, 317\nnp.logspace(), 318 pytorch_lightning, 435\nnp.mean(), 47, 176 QDA(),seeQuadraticDiscriminant-\nnp.nan, 60 Analysis()\nnp.nanmean(), 541 QuadraticDiscriminantAnalysis(),\nnp.percentile(), 228 174, 181\nnp.power(), 219 random(), 555\nnp.random.choice(), 553 RandomForestRegressor(),354,\nnp.random.default_rng(),46, 360",
    "source": "book_islr",
    "section_title": "496 pd.plotting.scatter_matrix(),"
  },
  {
    "instruction": "What is lab: linear models and regularization methods 271 in data science?",
    "input": "",
    "output": "ax.set_ylim([50000,250000]);\nNotice the expression None in Y[:,None] above. This adds an axis (dimen-\nsion) to the one-dimensional array Y, which allows it to be recycled when\nsubtracted from the two-dimensional Yhat_in. We are now ready to use cross-validation to estimate test error along\nthe model path.",
    "source": "book_islr",
    "section_title": "6.5 Lab: Linear Models and Regularization Methods 271"
  },
  {
    "instruction": "What is 2 s s+1 s+2 d+2 in data science?",
    "input": "",
    "output": "s\nx so (cid:80) |x | = 1. Let b (respectively a ) be the ith column of B (respectively A). Then,\ni i i\ni=1\ns d+2 s d+2 s\n(cid:80) (cid:80) (cid:80) (cid:80) (cid:80)\n|x |b = |x |b from which it follows that |x |a = |x |a and |x | =\ni i i i i i i i i\ni=1 i=s+1 i=1 i=s+1 i=1\nd+2 s d+2 s d+2\n(cid:80) (cid:80) (cid:80) (cid:80) (cid:80)\n|x |.",
    "source": "book_fods",
    "section_title": "1 2 s s+1 s+2 d+2"
  },
  {
    "instruction": "Describe the typical steps involved in 2 n in a data science workflow.",
    "input": "",
    "output": "partition into two subsets whose sums are equal. However, for that problem, there is\nan efficient dynamic programming algorithm that finds a near-optimal solution. We will\nbuild on that idea for the general rank k problem. For Item 2, we want to maximize (cid:80)k σ (xTu )(v T(1 − x)) over 0-1 vectors x. A\ni=1 i i i\npiece of notation will be useful. For any S ⊆ {1,2,...n}, write u (S) for the sum of coor-\ni\n(cid:80)\ndinates of the vector u corresponding to elements in the set S, that is, u (S) = u ,\ni i j∈S ij\nand similarly for v .",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "How is if h(b ) > 6m different from 1 1 1 in data science?",
    "input": "",
    "output": "if h(b ) > 6m: y = i d\ni 1 otherwise\nand let\nd\n(cid:88)\ny = y . i\ni=1\nWe want to show that with good probability, we will see a hash value in [0, 6M], i.e.,\nd\nthat Prob(y = 0) is small. 1 1 1: (cid:18) (cid:19) (cid:18) (cid:19)\nM d 6M\n(cid:0) (cid:1)\nProb < = Prob min > 6M = Prob ∀k h(b ) >\nmin 6 d k d\n= Prob(y = 0)\n≤ Prob(|y −E(y)| ≥ E(y))\nVar(y) 1 1\n≤ ≤ ≤\nE2(y) E(y) 6\nSince M > 6d with probability at most 1 + d and M < d with probability at most 1,\nmin 6 M min 6 6\nd ≤ M ≤ 6d with probability at least 2 − d .",
    "source": "book_fods",
    "section_title": "0 if h(b ) > 6M vs 1 1 1 1"
  },
  {
    "instruction": "Explain 10. deep learning in simple terms for a beginner in data science.",
    "input": "",
    "output": "process each element in the sequence, i.e. they are not functions of %. This\nis a form of weight sharing used by RNNs, and similar to the use of filters\nweight\nin convolutional neural networks (Section 10.3.1.)",
    "source": "book_islr",
    "section_title": "418 10. Deep Learning"
  },
  {
    "instruction": "Explain k in simple terms for a beginner in data science.",
    "input": "",
    "output": "data point assigned to whichever center is closest to it. Note that this assumption does\nnot yet tell whether one choice of centers is better than another. For this, one needs\nan objective, or optimization criterion.",
    "source": "book_fods",
    "section_title": "1 k"
  },
  {
    "instruction": "When or why would a data scientist use ranking documents and web pages?",
    "input": "",
    "output": "An important task for a document collection is to rank the documents according to\ntheir intrinsic relevance to the collection. A good candidate definition of “intrinsic rele-\nvance”isadocument’sprojectionontothebest-fitdirectionforthatcollection, namelythe\ntop left-singular vector of the term-document matrix. An intuitive reason for this is that\nthis direction has the maximum sum of squared projections of the collection and so can be\nthoughtofasasyntheticterm-documentvectorbestrepresentingthedocumentcollection. Ranking in order of the projection of each document’s term vector along the best fit\ndirection has a nice interpretation in terms of the power method. For this, we consider\na different example, that of the web with hypertext links.",
    "source": "book_fods",
    "section_title": "3.9.4 Ranking Documents and Web Pages"
  },
  {
    "instruction": "Describe the typical steps involved in cross-validation (statistics) part 2 in a data science workflow.",
    "input": "",
    "output": "Assume a model with one or more unknown parameters, and a data set to which the model can be fit (the training data set). The fitting process optimizes the model parameters to make the model fit the training data as well as possible. If an independent sample of validation data is taken from the same population as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data. The size of this difference is likely to be large especially when the size of the training data set is small, or when the number of parameters in the model is large. Cross-validation is a way to estimate the size of this effect. === Example: linear regression ===\nIn linear regression, there exist real response values \n\n y\n \n 1\n\n ,\n …\n ,\n \n y\n \n n\n\n {\\textstyle y_{1},\\ldots ,y_{n}}\n \n, and n p-dimensional vector covariates x1, ..., xn.",
    "source": "web_wikipedia",
    "section_title": "Cross-validation (statistics) part 2"
  },
  {
    "instruction": "How is 2 different from 2 in data science?",
    "input": "",
    "output": "2: that achieves the weak-learning guarantee for any distribution of data can be boosted to a\nstrong learner, using the technique of boosting. At the high level, the idea will be to take\nour training sample S, and then to run the weak-learner on different data distributions\nproduced by weighting the points in the training sample in different ways. 2: on the parts of the sample that previous hypotheses have performed poorly on. At the\nend we will combine the hypotheses together by a majority vote.",
    "source": "book_fods",
    "section_title": "2 2 vs 1 2"
  },
  {
    "instruction": "Describe the typical steps involved in active learning in a data science workflow.",
    "input": "",
    "output": "Active learning refers to algorithms that take an active role in the selection of which ex-\namples are labeled. The algorithm is given an initial unlabeled set U of data points drawn\nfrom distribution D and then interactively requests for the labels of a small number of\nthese examples. The aim is to reach a desired error rate (cid:15) using much fewer labels than\nwould be needed by just labeling random examples (i.e., passive learning). As a simple example, suppose that data consists of points on the real line and H =\n{f : f (x) = 1 iff x ≥ a} for a ∈ R. That is, H is the set of all threshold functions on\na a\nthe line. It is not hard to show (see Exercise 5.2) that a random labeled sample of size\nO(1 log(1)) is sufficient to ensure that with probability ≥ 1−δ, any consistent threshold\n(cid:15) δ\na(cid:48) has error at most (cid:15). Moreover, it is not hard to show that Ω(1) random examples are\n(cid:15)\nnecessary for passive learning.",
    "source": "book_fods",
    "section_title": "5.16.2 Active Learning"
  },
  {
    "instruction": "Explain finding low-error clusterings in simple terms for a beginner in data science.",
    "input": "",
    "output": "In the previous sections we saw algorithms for finding a local optimum to the k-means\nclusteringobjective, forfindingaglobaloptimumtothek-meansobjectiveontheline, and\nfor finding a factor 2 approximation to the k-center objective. But what about finding\na clustering that is close to the correct answer, such as the true clustering of proteins\nby function or a correct clustering of news articles by topic? For this we need some\nassumption about the data and what the correct answer looks like.",
    "source": "book_fods",
    "section_title": "7.4 Finding Low-Error Clusterings"
  },
  {
    "instruction": "How is results using a pretrained classifier different from document classification 413 in data science?",
    "input": "",
    "output": "results using a pretrained classifier: Here we use an industry-level pretrained classifier to predict the class of\nsomenewimages.Theresnet50classifierisaconvolutionalneuralnetwork\nthat was trained using the imagenet data set, which consists of millions of\nimagesthatbelongtoanever-growingnumberofcategories.10 Figure10.10\ndemonstratestheperformanceofresnet50onsixphotographs(privatecol-\nlectionofoneoftheauthors).11 TheCNNdoesareasonablejobclassifying\nthe hawk in the second image. If we zoom out as in the third image, it\ngets confused and chooses the fountain rather than the hawk. document classification 413: flamingo Cooper’s hawk Cooper’s hawk\nflamingo 0.83 kite 0.60 fountain 0.35\nspoonbill 0.17 great grey owl 0.09 nail 0.12\nwhite stork 0.00 robin 0.06 hook 0.07\nLhasa Apso cat Cape weaver\nTibetan terrier 0.56 Old English sheepdog 0.82 jacamar 0.28\nLhasa 0.32 Shih-Tzu 0.04 macaw 0.12\ncocker spaniel 0.03 Persian cat 0.04 robin 0.12\nFIGURE 10.10. Classification of six photographs using the resnet50 CNN\ntrained on the imagenet corpus.",
    "source": "book_islr",
    "section_title": "10.3.5 Results Using a Pretrained Classifier vs 10.4 Document Classification 413"
  },
  {
    "instruction": "How is y 52 different from t=−2.0936 in data science?",
    "input": "",
    "output": "y 52: −\nis virtually indistinguishable from a N(0,1) distribution.) If we instead\napply Algorithm 13.3 with B =10,000, then we obtain a p-value of 0.042. t=−2.0936: FIGURE 13.7. The 11th gene in the Khan dataset has a test statistic of\nT = 2.09.Itstheoreticalandre-samplingnulldistributionsarealmostidentical.",
    "source": "book_islr",
    "section_title": "Y 52 vs T=−2.0936"
  },
  {
    "instruction": "How is 2 different from ata in data science?",
    "input": "",
    "output": "2: Theorem 3.11 below states that even with ties, the power method converges to some\nvector in the span of those singular vectors corresponding to the “nearly highest” singular\nvalues. The theorem assumes it is given a vector x which has a component of magnitude\nat least δ along the first right singular vector v of A. ata: (cid:1)k\nx\nw = . (cid:12) (cid:12)\n(cid:12)(ATA)kx(cid:12)\n(cid:12) (cid:12)\n10E.g., suppose each entry in the first row of A is nonzero and the rest of A is zero.",
    "source": "book_fods",
    "section_title": "1 2 vs ATA"
  },
  {
    "instruction": "Explain 4. classification in simple terms for a beginner in data science.",
    "input": "",
    "output": "1. A person arrives at the emergency room with a set of symptoms\nthat could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?",
    "source": "book_islr",
    "section_title": "136 4. Classification"
  },
  {
    "instruction": "What is =1 , in data science?",
    "input": "",
    "output": "− ) ) 1p j=1 n i)=1 x2 ij 2 − TSS\nwhere TSS represents the)total)sum of squared elements of X, and RSS\nrepresents the residual sum of squares of the M-dimensional approxima-\ntiongivenbytheprincipalcomponents.RecallingthedefinitionofR2 from\n(3.17), this means that we can interpret the PVE as the R2 of the approx-\nimation for X given by the first M principal components.",
    "source": "book_islr",
    "section_title": "1 =1 ,"
  },
  {
    "instruction": "Describe the typical steps involved in machine learning part 5 in a data science workflow.",
    "input": "",
    "output": "Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n\nSupervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs. Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm works for all problems.",
    "source": "web_wikipedia",
    "section_title": "Machine learning part 5"
  },
  {
    "instruction": "How is 12. unsupervised learning different from lab: unsupervised learning 539 in data science?",
    "input": "",
    "output": "12. unsupervised learning: ax.arrow(0, 0, pcaUS.components_[i,k], pcaUS.components_[j,k])\nax.text(pcaUS.components_[i,k],\npcaUS.components_[j,k],\nUSArrests.columns[k])\nNotice that this figure is a reflection of Figure 12.1 through the y-axis. Recall that the principal components are only unique up to a sign change,\nso we can reproduce that figure by flipping the signs of the second set of\nscoresandloadings.Wealsoincreasethelengthofthearrowstoemphasize\nthe loadings. lab: unsupervised learning 539: ax.set_xlabel('Principal Component');\nax.set_ylabel('Proportion of Variance Explained')\nax.set_ylim([0,1])\nax.set_xticks(ticks)\nNoticetheuseof%%capture,whichsuppressesthedisplayingofthepartially\ncompleted figure. In[19]: ax = axes[1]\nax.plot(ticks,\npcaUS.explained_variance_ratio_.cumsum(),\nmarker='o')\nax.set_xlabel('Principal Component')\nax.set_ylabel('Cumulative Proportion of Variance Explained')\nax.set_ylim([0, 1])\nax.set_xticks(ticks)\nfig\nThe result is similar to that shown in Figure 12.3.",
    "source": "book_islr",
    "section_title": "538 12. Unsupervised Learning vs 12.5 Lab: Unsupervised Learning 539"
  },
  {
    "instruction": "What is a5 in data science?",
    "input": "",
    "output": "FIGURE 10.1. Neural network with a single hidden layer. The hidden layer\ncomputes activations A k = h k (X) that are nonlinear transformations of linear\ncombinations of the inputs X 1 ,X 2 ,...,X p. Hence these A k are not directly ob-\nserved.",
    "source": "book_islr",
    "section_title": "A5"
  },
  {
    "instruction": "Describe the typical steps involved in machine learning part 12 in a data science workflow.",
    "input": "",
    "output": "AAAI Conference on Artificial Intelligence\nAssociation for Computational Linguistics (ACL)\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)\nInternational Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)\nInternational Conference on Machine Learning (ICML)\nInternational Conference on Learning Representations (ICLR)\nInternational Conference on Intelligent Robots and Systems (IROS)\nConference on Knowledge Discovery and Data Mining (KDD)\nConference on Neural Information Processing Systems (NeurIPS)",
    "source": "web_wikipedia",
    "section_title": "Machine learning part 12"
  },
  {
    "instruction": "How is ∈s different from x 2 x 2 in data science?",
    "input": "",
    "output": "∈s: which typically involves far fewer terms than in (9.18).2\nTosummarize,inrepresentingthelinearclassifierf(x),andincomputing\nits coefficients, all we need are inner products. Now suppose that every time the inner product (9.17) appears in the\nrepresentation (9.18), or in a calculation of the solution for the support\nvector classifier, we replace it with a generalization of the inner product of\nthe form\nK(x ,x ), (9.20)\ni i! x 2 x 2: FIGURE 9.9. Left: An SVM with a polynomial kernel of degree 3 is applied to\nthe non-linear data from Figure 9.8, resulting in a far more appropriate decision\nrule.Right:AnSVMwitharadialkernelisapplied.Inthisexample,eitherkernel\nis capable of capturing the decision boundary.",
    "source": "book_islr",
    "section_title": "0∈S vs X 2 X 2"
  },
  {
    "instruction": "What is 2 1 2 in data science?",
    "input": "",
    "output": "×\npositive integers that are not necessarily equal. Figure10.7illustratestheapplicationoftwoconvolutionfilterstoa192\n×\n179 image of a tiger, shown on the left-hand side.8 Each convolution filter\nis a 15 15 image containing mostly zeros (black), with a narrow strip\n×\nof ones (white) oriented either vertically or horizontally within the image. Wheneachfilterisconvolvedwiththeimageofthetiger,areasofthetiger\nthatresemblethefilter(i.e.thathaveeitherhorizontalorverticalstripesor\nedges)aregivenlargevalues,andareasofthetigerthatdonotresemblethe\nfeature are given small values.",
    "source": "book_islr",
    "section_title": "1 2 1 2"
  },
  {
    "instruction": "What is 6. linear model selection and regularization in data science?",
    "input": "",
    "output": "We now add the cross-validation error estimates to our MSE plot. We\nincludethemeanerroracrossthefivefolds,andtheestimateofthestandard\nerror of the mean. In[16]: ax.errorbar(np.arange(n_steps),\ncv_mse.mean(1),\ncv_mse.std(1) / np.sqrt(K),\nlabel='Cross-validated',\nc='r') # color red\nax.set_ylim([50000,250000])\nax.legend()\nmse_fig\nTorepeattheaboveusingthevalidationsetapproach,wesimplychange\nour cv argument to a validation set: one random split of the data into a\ntest and training.",
    "source": "book_islr",
    "section_title": "272 6. Linear Model Selection and Regularization"
  },
  {
    "instruction": "How is 4. classification different from y = in data science?",
    "input": "",
    "output": "4. classification: convert a qualitative response variable with more than two levels into a\nquantitative response that is ready for linear regression. For a binary (two level) qualitative response, the situation is better. y =: =1 if drug overdose. We could then fit a linear regression to this binary response, and predict\ndrug overdose if Yˆ >0.5 and stroke otherwise.",
    "source": "book_islr",
    "section_title": "138 4. Classification vs Y ="
  },
  {
    "instruction": "How is 5. resampling methods different from exercises 227 in data science?",
    "input": "",
    "output": "5. resampling methods: (d) Nowconsideralogisticregressionmodelthatpredictstheprob-\nability of default using income, balance, and a dummy variable\nforstudent.Estimatethetesterrorforthismodelusingtheval-\nidation set approach. Comment on whether or not including a\ndummyvariableforstudentleadstoareductioninthetesterror\nrate. exercises 227: i. Fit a logistic regression model using all but the ith obser-\nvation to predict Direction using Lag1 and Lag2.",
    "source": "book_islr",
    "section_title": "226 5. Resampling Methods vs 5.4 Exercises 227"
  },
  {
    "instruction": "How is cross-validation (statistics) part 10 different from cross-validation (statistics) part 11 in data science?",
    "input": "",
    "output": "cross-validation (statistics) part 10: Due to correlations, cross-validation with random splits might be problematic for time-series models (if we are more interested in evaluating extrapolation, rather than interpolation). A more appropriate approach might be to use rolling cross-validation. cross-validation (statistics) part 11: Cross-validation can be used to compare the performances of different predictive modeling procedures. For example, suppose we are interested in optical character recognition, and we are considering using either a Support Vector Machine (SVM) or k-nearest neighbors (KNN) to predict the true character from an image of a handwritten character.",
    "source": "web_wikipedia",
    "section_title": "Cross-validation (statistics) part 10 vs Cross-validation (statistics) part 11"
  },
  {
    "instruction": "How is spectral clustering different from 1 (cid:88) in data science?",
    "input": "",
    "output": "spectral clustering: An important part of a clustering toolkit when data lies in Rd is Singular Value De-\ncomposition. Spectral Clustering refers to the following algorithm: Find the space V\nspanned by the top k right singular vectors of the matrix A whose rows are the data\npoints. 1 (cid:88): Prob(x) = e−|x−µi|2. (2π)d/2k\ni=1\nDenote by µ(x) the center nearest to x.",
    "source": "book_fods",
    "section_title": "7.1.3 Spectral Clustering vs 1 1 (cid:88)"
  },
  {
    "instruction": "How is generalized linear models 167 different from generalized linear models in data science?",
    "input": "",
    "output": "generalized linear models 167: Coefficient Std. error t-statistic p-value\nIntercept 73.60 5.13 14.34 0.00\nworkingday 1.27 1.78 0.71 0.48\ntemp 157.21 10.26 15.32 0.00\nweathersit[cloudy/misty] -12.89 1.96 -6.56 0.00\nweathersit[light rain/snow] -66.49 2.97 -22.43 0.00\nweathersit[heavy rain/snow] -109.75 76.67 -1.43 0.15\nTABLE 4.10. generalized linear models: In Chapter 3, we assumed that the response Y is quantitative, and ex-\nplored the use of least squares linear regression to predict Y. Thus far in\nthis chapter, we have instead assumed that Y is qualitative.",
    "source": "book_islr",
    "section_title": "4.6 Generalized Linear Models 167 vs 4.6 Generalized Linear Models"
  },
  {
    "instruction": "Explain 1 2 t−1 in simple terms for a beginner in data science.",
    "input": "",
    "output": "ending in state j times the probability of going from state j to state i and observing O . t\nThe time to compute the probability of a sequence of length T when there are n states is\nO(n2T). The factor n2 comes from the calculation for each time unit of the contribution\nfrom each possible previous state to the probability of each possible current state.",
    "source": "book_fods",
    "section_title": "0 1 2 t−1"
  },
  {
    "instruction": "When or why would a data scientist use lab: deep learning 441?",
    "input": "",
    "output": "recognized by torch using TensorDataset(). Tensor\nDataset()\nIn[21]: X_train_t = torch.tensor(X_train.astype(np.float32))\nY_train_t = torch.tensor(Y_train.astype(np.float32))\nhit_train = TensorDataset(X_train_t, Y_train_t)\nWe do the same for the test data. In[22]: X_test_t = torch.tensor(X_test.astype(np.float32))\nY_test_t = torch.tensor(Y_test.astype(np.float32))\nhit_test = TensorDataset(X_test_t, Y_test_t)\nFinally, this dataset is passed to a DataLoader() which ultimately passes\ndata into our network. While this may seem like a lot of overhead, this\nstructureishelpfulformorecomplextaskswheredatamayliveondifferent\nmachines, or where data must be passed to a GPU. We provide a helper\nfunction SimpleDataModule() in ISLP to make this task easier for standard\nSimpleData\nusage.",
    "source": "book_islr",
    "section_title": "10.9 Lab: Deep Learning 441"
  },
  {
    "instruction": "How is a re-sampling approach to p-values and false discovery rates 581 different from 0m in data science?",
    "input": "",
    "output": "a re-sampling approach to p-values and false discovery rates 581: However, the numerator E(V) on the right-hand side of (13.13) is more\nchallenging. This is the expected number of false positives associated with\nrejecting any null hypothesis for which the test statistic exceeds c in abso-\nlute value. 0m: therefore,thenumberofpermutedt-statisticsthatexceedthethresholdcin\nabsolutevalueprovidesanestimateforE(V).Thisestimatecanbefurther\nimproved by repeating the permutation process B times, for a large value\nof B, and averaging the results. Algorithm 13.4 details this procedure.18 It provides what is known as a\nplug-inestimateoftheFDR,becausetheapproximationin(13.13)allowsus\nto estimate the FDR by plugging R into the denominator and an estimate\nfor E(V) into the numerator.",
    "source": "book_islr",
    "section_title": "13.5 A Re-Sampling Approach to p-Values and False Discovery Rates 581 vs 01 0m"
  },
  {
    "instruction": "Explain 12. unsupervised learning in simple terms for a beginner in data science.",
    "input": "",
    "output": "the return type when using this method, so instances will still be of class\nAgglomerativeClustering. In the following example we use the data from\nthe previous lab to plot the hierarchical clustering dendrogram using com-\nplete, single, and averagelinkage clustering with Euclidean distance as the\ndissimilarity measure. We begin by clustering observations using complete\nlinkage.",
    "source": "book_islr",
    "section_title": "544 12. Unsupervised Learning"
  },
  {
    "instruction": "How is singular vectors and eigenvectors different from centering data in data science?",
    "input": "",
    "output": "singular vectors and eigenvectors: For a square matrix B, if Bx = λx, then x is an eigenvector of B and λ is the corre-\nsponding eigenvalue. We saw in Section 3.7, if B = ATA, then the right singular vectors\nv of A are eigenvectors of B with eigenvalues σ2. centering data: Singular value decomposition is used in many applications and for some of these ap-\nplications it is essential to first center the data by subtracting the centroid of the data\nfrom each data point.11 If you are interested in the statistics of the data and how it varies\nin relationship to its mean, then you would center the data. On the other hand, if you\nare interested in finding the best low rank approximation to a matrix, then you do not\ncenter the data.",
    "source": "book_fods",
    "section_title": "3.8 Singular Vectors and Eigenvectors vs 3.9.1 Centering Data"
  },
  {
    "instruction": "Explain generative models for classification 153 in simple terms for a beginner in data science.",
    "input": "",
    "output": "the default category. It is often of interest to determine which of these two\ntypes of errors are being made. A confusion matrix, shown for the Default\nconfusion\ndata in Table 4.4, is a convenient way to display this information.",
    "source": "book_islr",
    "section_title": "4.4 Generative Models for Classification 153"
  },
  {
    "instruction": "Explain 20 40 60 80 100 120 in simple terms for a beginner in data science.",
    "input": "",
    "output": "0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nMonths\ndehsilbuP\ngnieB\ntoN\nfo\nytilibaborP\nLow Risk\nMedium Risk\nHigh Risk\nFIGURE 11.8. For the Publication data introduced in Section 11.5.4, we\ncomputetertilesof“risk”inthetestsetusingcoefficientsestimatedonthetraining\nset. There is clear separation between the resulting survival curves.",
    "source": "book_islr",
    "section_title": "0 20 40 60 80 100 120"
  },
  {
    "instruction": "When or why would a data scientist use w = − . (11.4)?",
    "input": "",
    "output": "Var(X)\nTo construct the log-rank test stat5istic, we compute a quantity that takes\nexactly the form (11.4), with X = K q , where q is given in the top\nk=1 1k 1k\nleft of Table 11.1.\n) In greater detail, if there is no difference in survival between the two\ngroups, and conditioning on the row and column totals in Table 11.1, the\nexpected value of q is\n1k\nr\n1k\nµ = q . (11.5)\nk k\nr\nk\nSo the expected value of X = K q is µ = K r1kq . Furthermore,\nk=1 1k k=1 rk k\nit can be shown5 that the variance of q is\n) 1k )\nq (r /r )(1 r /r )(r q )\nk 1k k 1k k k k\nVar(q )= − − . (11.6)\n1k\nr 1\nk\n−\nThough q ,...,q may be correlated, we nonetheless estimate",
    "source": "book_islr",
    "section_title": "W = − . (11.4)"
  },
  {
    "instruction": "Explain an application to the heart disease data in simple terms for a beginner in data science.",
    "input": "",
    "output": "InChapter8weapplydecisiontreesandrelatedmethodstotheHeartdata. Theaimistouse13predictorssuchasAge,Sex,andCholinordertopredict\nwhether an individual has heart disease. We now investigate how an SVM\ncompares to LDA on this data.",
    "source": "book_islr",
    "section_title": "9.3.3 An Application to the Heart Disease Data"
  },
  {
    "instruction": "What is 2 n in data science?",
    "input": "",
    "output": "e−aν\n. (cid:20) (cid:21)n\n(cid:82)\ne−x2νdx\nx\nNow, a is fixed and ν is to be determined. Taking logs, the expression to maximize is\n \n(cid:90)\n−aν −nln e−νx2dx.",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "How is d different from c in data science?",
    "input": "",
    "output": "d: us that a sample size of 1 (10ln10+ln(2/δ)) would be sufficient. 2(cid:15)2\nHowever, what if we do not wish to discretize our concept class? c: (a) (b)\nFigure 5.5: (a) shows a set of four points that can be shattered by rectangles along with\nsome of the rectangles that shatter the set. Not every set of four points can be shattered\nas seen in (b).",
    "source": "book_fods",
    "section_title": "D vs C"
  },
  {
    "instruction": "Explain 2994.0 4.770e+06 1.0 1282.563 0.805 3.697e-01 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Notice the * in the anova_lm() line above. This function takes a variable\nnumber of non-keyword arguments, in this case fitted models. When these\nmodels are provided as a list (as is done here), it must be prefixed by *.",
    "source": "book_islr",
    "section_title": "4 2994.0 4.770e+06 1.0 1282.563 0.805 3.697e-01"
  },
  {
    "instruction": "Describe the typical steps involved in 2 2 in a data science workflow.",
    "input": "",
    "output": "unsatisfiable. It has been conjectured that there is a constant r independent of n such\nk\nthat r n is a sharp threshold. k\n279\nHere we derive upper and lower bounds on r . It is relatively easy to get an upper\nk\nbound on r . A fixed truth assignment satisfies a random k clause with probability\nk\n1 − 1 because of the 2k truth assignments to the k variables in the clause, only one\n2k\nfails to satisfy the clause. Thus, with probability 1 , the clause is not satisfied, and with\n2k\nprobability 1− 1 , the clause is satisfied.",
    "source": "book_fods",
    "section_title": "2 2 2"
  },
  {
    "instruction": "How is s different from occam’s razor in data science?",
    "input": "",
    "output": "s: disjunction. Proof: Suppose target concept c∗ is a disjunction. occam’s razor: Occam’s razor is the notion, stated by William of Occam around AD 1320, that in general\none should prefer simpler explanations over more complicated ones.18 Why should one\ndo this, and can we make a formal claim about why this is a good idea? What if each of\nus disagrees about precisely which explanations are simpler than others?",
    "source": "book_fods",
    "section_title": "S vs 5.6.2 Occam’s Razor"
  },
  {
    "instruction": "What is step functions in data science?",
    "input": "",
    "output": "Using polynomial functions of the features as predictors in a linear model\nimposes a global structure on the non-linear function of X. We can instead\nuse step functions in order to avoid imposing such a global structure. Here\nstep\nwe break the range of X into bins, and fit a different constant in each bin.",
    "source": "book_islr",
    "section_title": "7.2 Step Functions"
  },
  {
    "instruction": "When or why would a data scientist use e e?",
    "input": "",
    "output": "where the second equality follows from (11.10). Wenowconsiderthetaskofmodelingthesurvivaltimes.Ifweassumeex-\nponential survival, i.e. that the probability density function of the survival\ntime T takes the form f(t)=λexp( λt), then estimating the parameter λ\n−\nby maximizing the likelihood in (11.13) is straightforward.9 Alternatively,\nwe could assume that the survival times are drawn from a more flexible\nfamily of distributions, such as the Gamma or Weibull family. Another\npossibility is to model the survival times non-parametrically, as was done\nin Section 11.3 using the Kaplan–Meier estimator. However, what we would really like to do is model the survival time as\na function of the covariates.",
    "source": "book_islr",
    "section_title": "E E"
  },
  {
    "instruction": "How is mse different from cross-validation in data science?",
    "input": "",
    "output": "mse: Out[8]:array([20.76, 16.95, 16.97])\nUsing this split of the observations into a training set and a validation\nset, we find that the validation set error rates for the models with linear,\nquadratic, and cubic terms are 20.76, 16.95, and 16.97, respectively. These results are consistent with our previous findings: a model that\npredictsmpgusingaquadraticfunctionofhorsepowerperformsbetterthan\na model that involves only a linear function of horsepower, and there is no\nevidence of an improvement in using a cubic function of horsepower. cross-validation: In theory, the cross-validation estimate can be computed for any general-\nized linear model. In practice, however, the simplest way to cross-validate\nin Python is to use sklearn, which has a different interface or API than\nstatsmodels, the code we have been using to fit GLMs.",
    "source": "book_islr",
    "section_title": "MSE vs 5.3.2 Cross-Validation"
  },
  {
    "instruction": "When or why would a data scientist use y?",
    "input": "",
    "output": "FIGURE 3.3. A simulated data set. Left: The red line represents the true\nrelationship, f(X) = 2+3X, which is known as the population regression line. The blue line is the least squares line; it is the least squares estimate for f(X)\nbasedontheobserveddata,showninblack.Right:Thepopulationregressionline\nis again shown in red, and the least squares line in dark blue. In light blue, ten\nleast squares lines are shown, each computed on the basis of a separate random\nset of observations.",
    "source": "book_islr",
    "section_title": "Y"
  },
  {
    "instruction": "What is fitting a neural network in data science?",
    "input": "",
    "output": "Fittingneuralnetworksissomewhatcomplex,andwegiveabriefoverview\nhere. The ideas generalize to much more complex networks. Readers who\nfind this material challenging can safely skip it.",
    "source": "book_islr",
    "section_title": "10.7 Fitting a Neural Network"
  },
  {
    "instruction": "Explain 4 1 blue in simple terms for a beginner in data science.",
    "input": "",
    "output": "Sketch the observations. (b) Sketchtheoptimalseparatinghyperplane,andprovidetheequa-\ntion for this hyperplane (of the form (9.1)). (c) Describetheclassificationruleforthemaximalmarginclassifier.",
    "source": "book_islr",
    "section_title": "7 4 1 Blue"
  },
  {
    "instruction": "What is quadratic discriminant analysis in data science?",
    "input": "",
    "output": "We will now fit a QDA model to the Smarket data. QDA is implemented\nviaQuadraticDiscriminantAnalysis()inthesklearnpackage,whichweab-\nQuadratic\nbreviate to QDA(). The syntax is very similar to LDA().",
    "source": "book_islr",
    "section_title": "4.7.4 Quadratic Discriminant Analysis"
  },
  {
    "instruction": "Explain spectral clustering in simple terms for a beginner in data science.",
    "input": "",
    "output": "Let A be a n×d data matrix with each row a data point and suppose we want to partition\nthedatapointsintok clusters. Spectral Clusteringreferstoaclassofclusteringalgorithms\nwhich share the following outline:\n• Find the space V spanned by the top k (right) singular vectors of A. • Project data points into V.\n• Cluster the projected points.",
    "source": "book_fods",
    "section_title": "7.5 Spectral Clustering"
  },
  {
    "instruction": "How is 2. statistical learning different from rows x 2 columns in data science?",
    "input": "",
    "output": "2. statistical learning: To save space, we have omitted the output of the previous code block. We\nsee the culprit is the value ?, which is being used to encode missing values. rows x 2 columns: Since we did not specify an index column when we loaded our data frame,\nthe rows are labeled using integers 0 to 396. In[84]: Auto.index\nOut[84]:Int64Index([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n...\n387, 388, 389, 390, 391, 392, 393, 394, 395, 396],\ndtype='int64', length=392)\nWecanusetheset_index()methodtore-nametherowsusingthecontents\n.set_index()\nof Auto['name'].",
    "source": "book_islr",
    "section_title": "56 2. Statistical Learning vs 392 rows x 2 columns"
  },
  {
    "instruction": "How is 11. survival analysis and censored data different from shrinkage for the cox model in data science?",
    "input": "",
    "output": "11. survival analysis and censored data: Coefficient Std. error z-statistic p-value\nposres[Yes] 0.55 0.18 3.02 0.00\nmulti[Yes] 0.15 0.31 0.47 0.64\nclinend[Yes] 0.51 0.27 1.89 0.06\nmech[K01] 1.05 1.06 1.00 0.32\nmech[K23] -0.48 1.05 -0.45 0.65\nmech[P01] -0.31 0.78 -0.40 0.69\nmech[P50] 0.60 1.06 0.57 0.57\nmech[R01] 0.10 0.32 0.30 0.76\nmech[R18] 1.05 1.05 0.99 0.32\nmech[R21] -0.05 1.06 -0.04 0.97\nmech[R24,K24] 0.81 1.05 0.77 0.44\nmech[R42] -14.78 3414.38 -0.00 1.00\nmech[R44] -0.57 0.77 -0.73 0.46\nmech[RC2] -14.92 2243.60 -0.01 0.99\nmech[U01] -0.22 0.32 -0.70 0.48\nmech[U54] 0.47 1.07 0.44 0.66\nsampsize 0.00 0.00 0.19 0.85\nbudget 0.00 0.00 1.67 0.09\nimpact 0.06 0.01 8.23 0.00\nTABLE 11.3. shrinkage for the cox model: In this section, we illustrate that the shrinkage methods of Section 6.2\ncan be applied to the survival data setting. In particular, motivated by\nthe “loss+penalty” formulation of Section 6.2, we consider minimizing a\npenalized version of the negative log partial likelihood in (11.16),\nexp p x β\nj=1 ij j\nlog +λP(β), (11.17)\n−  1 e)xp p x 2 β \ni: E δi=1 i!",
    "source": "book_islr",
    "section_title": "484 11. Survival Analysis and Censored Data vs 11.6 Shrinkage for the Cox Model"
  },
  {
    "instruction": "Explain ··· k in simple terms for a beginner in data science.",
    "input": "",
    "output": "variance term across all K classes, which for simplicity we can denote by\nσ2. Plugging (4.16) into (4.15), we find that\nπ 1 exp 1 (x µ )2\np (x)= k√2πσ −2σ2 − k . (4.17)\nk K\nl=1\nπ\nl√2\n1\nπσ\ne’xp\n−2σ\n1\n2\n(x\n−\nµ l()2\n(Note that in (4.17), π)denotes the pr’ior probability (that an observation\nk\nbelongs to the kth class, not to be confused with π 3.14159, the math-\n≈\nematical constant.)",
    "source": "book_islr",
    "section_title": "1 ··· K"
  },
  {
    "instruction": "What is k k−i 1 0 in data science?",
    "input": "",
    "output": "polynomial. 1. If the equation has a set of r distinct roots, what is the most general form of the\nsolution?",
    "source": "book_fods",
    "section_title": "0 k k−i 1 0"
  },
  {
    "instruction": "How is 4 6 8 10 different from k-fold cross-validation in data science?",
    "input": "",
    "output": "4 6 8 10: 82\n62\n42\n22\n02\n81\n61\n10−fold CV\nDegree of Polynomial\nrorrE\nderauqS\nnaeM\nFIGURE 5.4. Cross-validation was used on the Auto data set in order to\nestimatethetesterrorthatresultsfrompredictingmpgusingpolynomialfunctions\nof horsepower. k-fold cross-validation: An alternative to LOOCV is k-fold CV. This approach involves randomly\nk-foldCV\ndividing the set of observations into k groups, or folds, of approximately\nequal size.",
    "source": "book_islr",
    "section_title": "2 4 6 8 10 vs 5.1.3 k-Fold Cross-Validation"
  },
  {
    "instruction": "Describe the typical steps involved in t in a data science workflow.",
    "input": "",
    "output": "| |\n(y yˆ )2+αT (8.4)\ni − Rm | |\nm 0 =1i:x 0i∈ Rm\nis as small as possible. Here T indicates the number of terminal nodes\n| |\nof the tree T, R is the rectangle (i.e. the subset of predictor space) cor-\nm\nresponding to the mth terminal node, and yˆ is the predicted response\nRm\nassociated with R —that is, the mean of the training observations in R . m m\nThe tuning parameter α controls a trade-off between the subtree’s com-\nplexity and its fit to the training data. When α = 0, then the subtree T\nwill simply equal T , because then (8.4) just measures the training error. 0\nHowever, as α increases, there is a price to pay for having a tree with\nmany terminal nodes, and so the quantity (8.4) will tend to be minimized\nfor a smaller subtree.",
    "source": "book_islr",
    "section_title": "T"
  },
  {
    "instruction": "How is 1 1 ij 2 kl different from nmf with anchor terms in data science?",
    "input": "",
    "output": "1 1 ij 2 kl: It remains to solve the nonlinear problem in 2r2 variables. There is a classical algo-\nrithm which solves such problems in time exponential only in r2 (polynomial in the other\nparameters). nmf with anchor terms: An important case of NMF, which can be solved efficiently, is the case where there are\nanchor terms. An anchor term for a topic is a term that occurs in the topic and does not\noccur in any other topic.",
    "source": "book_fods",
    "section_title": "2 1 1 ij 2 kl vs 9.4 NMF with Anchor Terms"
  },
  {
    "instruction": "Explain 1 2 t−1 in simple terms for a beginner in data science.",
    "input": "",
    "output": "probability of the transition from j to i producing O . We then select the j for which this\nt\nproduct is largest. Note that in the previous example, we added the probabilities of each\npossibility together.",
    "source": "book_fods",
    "section_title": "0 1 2 t−1"
  },
  {
    "instruction": "Explain lab: multiple testing in simple terms for a beginner in data science.",
    "input": "",
    "output": "We include our usual imports seen in earlier labs. In[1]: import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom ISLP import load_data\nWe also collect the new imports needed for this lab. In[2]: from scipy.stats import \\\n(ttest_1samp,\nttest_rel,\nttest_ind,\nt as t_dbn)\nfrom statsmodels.stats.multicomp import \\\npairwise_tukeyhsd\nfrom statsmodels.stats.multitest import \\\nmultipletests as mult_test",
    "source": "book_islr",
    "section_title": "13.6 Lab: Multiple Testing"
  },
  {
    "instruction": "When or why would a data scientist use spectral clustering?",
    "input": "",
    "output": "An important part of a clustering toolkit when data lies in Rd is Singular Value De-\ncomposition. Spectral Clustering refers to the following algorithm: Find the space V\nspanned by the top k right singular vectors of the matrix A whose rows are the data\npoints. Project data points to V and cluster in the projection. An obvious reason to do this is dimension reduction, clustering in the d dimensional\nspace where data lies is reduced to clustering in a k dimensional space (usually, k << d). A more important point is that under certain assumptions one can prove that spectral\nclustering gives a clustering close to the true clustering.",
    "source": "book_fods",
    "section_title": "7.1.3 Spectral Clustering"
  },
  {
    "instruction": "How is cross-validation different from 5. resampling methods in data science?",
    "input": "",
    "output": "cross-validation: In theory, the cross-validation estimate can be computed for any general-\nized linear model. In practice, however, the simplest way to cross-validate\nin Python is to use sklearn, which has a different interface or API than\nstatsmodels, the code we have been using to fit GLMs. 5. resampling methods: package, we provide a wrapper, sklearn_sm(), that enables us to easily use\nsklearn_sm()\nthe cross-validation tools of sklearn with models fit by statsmodels. Theclasssklearn_sm()hasasitsfirstargumentamodelfromstatsmodels.",
    "source": "book_islr",
    "section_title": "5.3.2 Cross-Validation vs 218 5. Resampling Methods"
  },
  {
    "instruction": "What is i 2 i k i in data science?",
    "input": "",
    "output": "we can use least squares to estimate the unknown regression coefficients\nin (7.7). Importantly, this means that all of the inference tools for linear\nmodels that are discussed in Chapter 3, such as standard errors for the\ncoefficient estimates and F-statistics for the model’s overall significance,\nare available in this setting. Thus far we have considered the use of polynomial functions and piece-\nwiseconstantfunctionsforourbasisfunctions;however,manyalternatives\narepossible.Forinstance,wecanusewaveletsorFourierseriestoconstruct\nbasis functions.",
    "source": "book_islr",
    "section_title": "1 i 2 i K i"
  },
  {
    "instruction": "When or why would a data scientist use 0 9 0 0?",
    "input": "",
    "output": "We see that the four clusters obtained using hierarchical clustering and\nK-means clustering are somewhat different. First we note that the labels\nin the two clusterings are arbitrary. That is, swapping the identifier of the\ncluster does not change the clustering. We see here Cluster 3 in K-means\nclustering is identical to cluster 2 in hierarchical clustering. However, the\nother clusters differ: for instance, cluster 0 in K-means clustering contains\na portion of the observations assigned to cluster 0 by hierarchical cluster-\ning, as well as all of the observations assigned to cluster 1 by hierarchical\nclustering.",
    "source": "book_islr",
    "section_title": "3 0 9 0 0"
  },
  {
    "instruction": "Explain 13. multiple testing in simple terms for a beginner in data science.",
    "input": "",
    "output": "T_vals[j] = ttest_ind(D2[col],\nD4[col],\nequal_var=True).statistic\nD_ = np.hstack([D2[col], D4[col]])\nD_null = D_.copy()\nfor b in range(B):\nrng.shuffle(D_null)\nttest_ = ttest_ind(D_null[:n_],\nD_null[n_:],\nequal_var=True)\nTnull_vals[j,b] = ttest_.statistic\nNext, we compute the number of rejected null hypotheses R, the esti-\nmated number of false positives V, and the estimated FDR, for a range\nof threshold values c in Algorithm 13.4. The threshold values are chosen\nusing the absolute values of the teWst statistics from the 100 genes. In[28]: cutoffs = np.sort(np.abs(T_vals))\nFDRs, Rs, Vs = np.empty((3, m))\nfor j in range(m):\nR = np.sum(np.abs(T_vals) >= cutoffs[j])\nV = np.sum(np.abs(Tnull_vals) >= cutoffs[j]) / B\nRs[j] = R\nVs[j] = V\nFDRs[j] = V / R\nNow, for any given FDR, we can find the genes that will be rejected.",
    "source": "book_islr",
    "section_title": "592 13. Multiple Testing"
  },
  {
    "instruction": "Explain 2. statistical learning in simple terms for a beginner in data science.",
    "input": "",
    "output": "Above, we summed over each combination of value and weight. We also\ntook advantage of the increment notation in Python: the expression a += b\nincrement\nis equivalent to a = a + b. Besides being a convenient notation, this can\nsave time in computationally heavy tasks in which the intermediate value\nof a+b need not be explicitly created.",
    "source": "book_islr",
    "section_title": "60 2. Statistical Learning"
  },
  {
    "instruction": "When or why would a data scientist use k+1?",
    "input": "",
    "output": "For a square symmetric matrix A and eigenvector v, Av = λv. We now prove the\nanalog for singular values and vectors we discussed in the introduction. Lemma 3.10 (Analog of eigenvalues and eigenvectors)\nAv = σ u and ATu = σ v . i i i i i i\nProof: The first equation follows from the definition of left singular vectors. For the\nsecond, note that from the SVD, we get ATu = (cid:80) σ v u Tu , where since the u are\ni j j j j i j\northonormal, all terms in the summation are zero except for j = i.",
    "source": "book_fods",
    "section_title": "2 k+1"
  },
  {
    "instruction": "What is 1 2 t−1 in data science?",
    "input": "",
    "output": "probability of the transition from j to i producing O . We then select the j for which this\nt\nproduct is largest. Note that in the previous example, we added the probabilities of each\npossibility together.",
    "source": "book_fods",
    "section_title": "0 1 2 t−1"
  },
  {
    "instruction": "Explain 1 p in simple terms for a beginner in data science.",
    "input": "",
    "output": "timatingthecoefficients,andsetsanumberofthemtoexactlyzero.Hence\nin this sense the lasso is a less flexible approach than linear regression. It is also more interpretable than linear regression, because in the final\nmodel the response variable will only be related to a small subset of the\npredictors—namely, those with nonzero coefficient estimates. Generalized\nadditive models (GAMs), discussed in Chapter 7, instead extend the lin-\ngeneralized\near model (2.4) to allow for certain non-linear relationships.",
    "source": "book_islr",
    "section_title": "0 1 p"
  },
  {
    "instruction": "How is esm different from 70 111 in data science?",
    "input": "",
    "output": "esm: tseT\nFIGURE 6.23. On a simulated example with n = 20 training observations,\nfeatures that are completely unrelated to the outcome are added to the model. 70 111: 5\n4\n3\n2\n1\n0\np=20 p=50 p=2000\nDegreesofFreedom DegreesofFreedom DegreesofFreedom\nFIGURE 6.24. The lasso was performed with n=100 observations and three\nvalues of p, the number of features.",
    "source": "book_islr",
    "section_title": "ESM vs 1 70 111"
  },
  {
    "instruction": "What is multiple linear regression 81 in data science?",
    "input": "",
    "output": "However,theapproachoffittingaseparatesimplelinearregressionmodel\nforeachpredictorisnotentirelysatisfactory.Firstofall,itisunclearhowto\nmakeasinglepredictionofsalesgiventhethreeadvertisingmediabudgets,\nsinceeachofthebudgetsisassociatedwithaseparateregressionequation. Second, each of the three regression equations ignores the other two media\ninformingestimatesfortheregressioncoefficients.Wewillseeshortlythat\nif the media budgets are correlated with each other in the 200 markets\nin our data set, then this can lead to very misleading estimates of the\nassociation between each media budget and sales. Instead of fitting a separate simple linear regression model for each pre-\ndictor, a better approach is to extend the simple linear regression model\n(3.5) so that it can directly accommodate multiple predictors.",
    "source": "book_islr",
    "section_title": "3.2 Multiple Linear Regression 81"
  },
  {
    "instruction": "What is lab: deep learning 439 in data science?",
    "input": "",
    "output": "param_grid,\ncv=cv,\nscoring='neg_mean_absolute_error')\ngrid.fit(X_train, Y_train);\nWeextractthelassomodelwithbestcross-validatedmeanabsoluteerror,\nand evaluate its performance on X_test and Y_test, which were not used\nin cross-validation. In[17]: trained_lasso = grid.best_estimator_\nYhat_test = trained_lasso.predict(X_test)\nnp.fabs(Yhat_test - Y_test).mean()\nOut[17]:257.2382\nThisissimilartotheresultswegotforthelinearmodelfitbyleastsquares. However, these results can vary a lot for different train/test splits; we en-\ncourage the reader to try a different seed in code block 12 and rerun the\nsubsequent code up to this point.",
    "source": "book_islr",
    "section_title": "10.9 Lab: Deep Learning 439"
  },
  {
    "instruction": "When or why would a data scientist use 21 22 23?",
    "input": "",
    "output": "Figure 11.1: Set of scale functions associated with the Haar wavelet. (cid:82)\nsense that φ (x)φ (x)dx = 0 for k (cid:54)= l.\nx jk jl\nNote that for each j, the set of functions φ , k = 0,1,2..., form a basis for a vector\njk\nspace V and are orthogonal. The set of basis vectors φ , for all j and k, form an over-\nj jk\ncomplete basis and for different values of j are not orthogonal. Since φ , φ , and\njk j+1,2k\nφ are linearly dependent, for each value of j delete φ for odd values of k to\nj+1,2k+1 j+1,k\nget a linearly independent set of basis vectors. To get an orthogonal set of basis vectors,\ndefine\n",
    "source": "book_fods",
    "section_title": "20 21 22 23"
  },
  {
    "instruction": "Explain a = bc +n, in simple terms for a beginner in data science.",
    "input": "",
    "output": "where, N stands for noise. In topic modeling, N can be high. But it will be useful to first\nlook at the problem when there is no noise.",
    "source": "book_fods",
    "section_title": "A = BC +N,"
  },
  {
    "instruction": "When or why would a data scientist use a i o t l c a?",
    "input": "",
    "output": "Boston 188 1282 271 2300 483 1038 2099 2699 2493 393\nBuffalo 292 883 279 1906 178 662 1699 2300 2117 292\nChicago 713 432 666 1453 410 262 1260 1858 1737 597\nDallas 1374 586 1299 887 1070 547 999 1483 1681 1185\nDenver 1631 488 1579 586 1320 796 371 949 1021 1494\nHouston 1420 794 1341 1017 1137 679 1200 1645 1891 1220\nLosAngeles 2451 1315 2394 357 2136 1589 579 347 959 2300\nMemphis 957 529 881 1263 660 240 1250 1802 1867 765\nMiami 1092 1397 1019 1982 1010 1061 2089 2594 2734 923\nMinneapolis 1018 290 985 1280 743 466 987 1584 1395 934\nNewYork - 1144 83 2145 317 875 1972 2571 2408 230\nOmaha 1144 - 1094 1036 836 354 833 1429 1369 1014\nPhiladelphia 83 1094 - 2083 259 811 1925 2523 2380 123\nPhoenix 2145 1036 2083 - 1828 1272 504 653 1114 1973\nPittsburgh 317 836 259 1828 - 559 1668 2264 2138 192\nSaintLouis 875 354 811 1272 559 - 1162 1744 1724 712\nSaltLakeCity 1972 833 1925 504 1668 1162 - 600 701 1848\nSanFrancisco 2571 1429 2523 653 2264 1744 600 - 678 2442\nSeattle 2408 1369 2380 1114 2138 1724 701 678 - 2329\nWashingtonD.C. 230 1014 123 1973 192 712 1848 2442 2329 -\nCity Bei- Tian- Shang- Chong- Hoh- Urum- Lha- Yin- Nan- Har- Chang- Shen-\njing jin hai qing hot qi sa chuan ning bin chun yang\nBeijing 0 125 1239 3026 480 3300 3736 1192 2373 1230 979 684\nTianjin 125 0 1150 1954 604 3330 3740 1316 2389 1207 955 661\nShanghai 1239 1150 0 1945 1717 3929 4157 2092 1892 2342 2090 1796\nChongqing 3026 1954 1945 0 1847 3202 2457 1570 993 3156 2905 2610\nHohhot 480 604 1717 1847 0 2825 3260 716 2657 1710 1458 1164\nUrumqi 3300 3330 3929 3202 2825 0 2668 2111 4279 4531 4279 3985\nLhasa 3736 3740 4157 2457 3260 2668 0 2547 3431 4967 4715 4421\nYinchuan 1192 1316 2092 1570 716 2111 2547 0 2673 2422 2170 1876\nNanning 2373 2389 1892 993 2657 4279 3431 2673 0 3592 3340 3046\nHarbin 1230 1207 2342 3156 1710 4531 4967 2422 3592 0 256 546\nChangchun 979 955 2090 2905 1458 4279 4715 2170 3340 256 0 294\nShenyang 684 661 1796 2610 1164 3985 4421 1876 3046 546 294 0\nExercise 3.32 One’s data in a high dimensional space may lie on a lower dimensional\nsheath. To test for this one might for each data point find the set of closest data points\nand calculate the vector distance from the data point to each of the close points. If the set\nof these distance vectors is a lower dimensional space than the number of distance points,\nthen it is likely that the data is on a low dimensional sheath. To test the dimension of\nthe space of the distance vectors one might use the singular value decomposition to find\nthe singular values.",
    "source": "book_fods",
    "section_title": "A I O T L C A"
  },
  {
    "instruction": "What is 6. linear model selection and regularization in data science?",
    "input": "",
    "output": "the BIC statistic generally places a heavier penalty on models with many\nvariables, and hence results in the selection of smaller models than C . p\nIn Figure 6.2, we see that this is indeed the case for the Credit data set;\nBIC chooses a model that contains only the four predictors income, limit,\ncards, and student. In this case the curves are very flat and so there does\nnotappeartobemuchdifferenceinaccuracybetweenthefour-variableand\nsix-variable models.",
    "source": "book_islr",
    "section_title": "238 6. Linear Model Selection and Regularization"
  },
  {
    "instruction": "Explain 4 6 8 10 in simple terms for a beginner in data science.",
    "input": "",
    "output": "10−e1\n20−e1\n30−e1\n40−e1\n50−e1\nOrdering of p−values\n)elacs\ngol(\nseulav−p\nFIGURE 13.3. Each panel displays, for a separate simulation, the sorted\np-values for tests of m = 10 null hypotheses. The p-values corresponding to\nthe m 0 = 2 true null hypotheses are displayed in black, and the rest are in red.",
    "source": "book_islr",
    "section_title": "2 4 6 8 10"
  },
  {
    "instruction": "Explain n in simple terms for a beginner in data science.",
    "input": "",
    "output": "as a function of n? Exercise 8.27 Theorem 8.13 shows that for some c > 0 and p = clnn/n, G(n,p) has\ndiameter O(lnn). Tighten the argument to pin down as low a value as possible for c.\nExercise 8.28 What is diameter of G(n,p) for various values of p?",
    "source": "book_fods",
    "section_title": "2 n"
  },
  {
    "instruction": "When or why would a data scientist use 4 6 8 10?",
    "input": "",
    "output": "022\n002\n081\n061\n041\n021\n001\nNumber of Predictors\nrorrE\nnoitadilaV−ssorC\nFIGURE 6.3. For the Credit data set, three quantities are displayed for the\nbest model containing d predictors, for d ranging from 1 to 11. The overall best\nmodel, based on each of these quantities, is shown as a blue cross. Left: Square\nroot of BIC. Center: Validation set errors.",
    "source": "book_islr",
    "section_title": "2 4 6 8 10"
  },
  {
    "instruction": "What is 4. classification in data science?",
    "input": "",
    "output": "10. Equation 4.32 derived an expression for log Pr(Y=k | X=x) in the\nPr(Y=KX=x)\n|\nsetting where p > 1, so that the mean for th1e kth class, µ k2, is a p-\ndimensional vector, and the shared covariance Σ is a p p matrix. ×\nHowever,inthesettingwith p=1,(4.32)takesasimplerform,since\nthemeansµ ,...,µ andthevarianceσ2 arescalars.Inthissimpler",
    "source": "book_islr",
    "section_title": "196 4. Classification"
  },
  {
    "instruction": "How is i different from bibliographic notes in data science?",
    "input": "",
    "output": "i: p ji 1 2 0.85π i π i = 0.85π j p ji + 0. 2 85π i\nj i\nπ = 1.48π p\ni j ji\n0.15π 0.15π\nj i\nFigure 4.13: Impact on pagerank of adding a self loop\nlist of webpages in response to each search query. bibliographic notes: The material on the analogy between random walks on undirected graphs and electrical\nnetworks is from [DS84] as is the material on random walks in Euclidean space. Addi-\n116\ntional material on Markov chains can be found in [MR95b], [MU05], and [per10].",
    "source": "book_fods",
    "section_title": "2 i vs 4.9 Bibliographic Notes"
  },
  {
    "instruction": "How is j different from p in data science?",
    "input": "",
    "output": "j: intervals associated with the coefficients. Connection With The Log-Rank Test\nSuppose we have just a single predictor (p = 1), which we assume to be\nbinary, i.e. p: However, at times we may also wish to estimate the baseline hazard\nh (t),forinstancesothatwecanestimatethesurvivalcurveS(tx)for\n0\n|\nanindividualwithfeaturevectorx.Thedetailsarebeyondthescope\nof this book. Estimation of h 0 (t) is implemented in the lifelines\npackage in Python, which we will see in Section 11.8.",
    "source": "book_islr",
    "section_title": "0 j vs 1 p"
  },
  {
    "instruction": "How is exercises 285 different from 2 in data science?",
    "input": "",
    "output": "exercises 285: 5. It is well-known that ridge regression tends to give similar coefficient\nvalues to correlated variables, whereas the lasso may give quite dif-\nferent coefficient values to correlated variables. 2: not unique—in other words, there are many possible solutions\nto the optimization problem in (c). Describe these solutions.",
    "source": "book_islr",
    "section_title": "6.6 Exercises 285 vs 1 2"
  },
  {
    "instruction": "What is c in data science?",
    "input": "",
    "output": "(a) (b)\nFigure 5.5: (a) shows a set of four points that can be shattered by rectangles along with\nsome of the rectangles that shatter the set. Not every set of four points can be shattered\nas seen in (b). Any rectangle containing points A, B, and C must contain D. No set of five\npoints can be shattered by rectangles with axis-parallel edges.",
    "source": "book_fods",
    "section_title": "C"
  },
  {
    "instruction": "Describe the typical steps involved in when to use deep learning 425 in a data science workflow.",
    "input": "",
    "output": "The RNN we used scanned the document from beginning to end; alter-\nnative bidirectional RNNs scan the sequences in both directions. bidirectional\nIn language translation the target is also a sequence of words, in a\nlanguage different from that of the input sequence. Both the input se-\nquence and the target sequence are represented by a structure similar to\nFigure 10.12, and they share the hidden units. In this so-called Seq2Seq\nSeq2Seq\nlearning, the hidden units are thought to capture the semantic meaning\nof the sentences. Some of the big breakthroughs in language modeling and\ntranslationresultedfromtherelativelyrecentimprovementsinsuchRNNs. AlgorithmsusedtofitRNNscanbecomplexandcomputationallycostly.",
    "source": "book_islr",
    "section_title": "10.6 When to Use Deep Learning 425"
  },
  {
    "instruction": "What is 7.007 in data science?",
    "input": "",
    "output": "FIGURE8.4. RegressiontreeanalysisfortheHittersdata.Theunprunedtree\nthat results from top-down greedy splitting on the training data is shown. binary splitting to grow a classification tree.",
    "source": "book_islr",
    "section_title": "6.459 7.007"
  },
  {
    "instruction": "What is 4. classification in data science?",
    "input": "",
    "output": "The confusion_table() function from the ISLP package summarizes these\nconfusion_\npredictions, showing how many observations were correctly or incorrectly\ntable()\nclassified. Our function, which is adapted from a similar function in the\nmodule sklearn.metrics, transposes the resulting matrix and includes row\nandcolumnlabels.Theconfusion_table()functiontakesasfirstargument\nthe predicted labels, and second argument the true labels. In[12]: confusion_table(labels, Smarket.Direction)\nOut[12]: Truth Down Up\nPredicted\nDown 145 141\nUp 457 507\nThediagonalelementsoftheconfusionmatrixindicatecorrectpredictions,\nwhile the off-diagonals represent incorrect predictions.",
    "source": "book_islr",
    "section_title": "176 4. Classification"
  },
  {
    "instruction": "Explain 2 n in simple terms for a beginner in data science.",
    "input": "",
    "output": "finite variance, then better bounds are possible. Here we focus on the case where the\nn independent variables are binomial. In the next section we consider the more general\ncase where we have independent random variables from any distribution that has a finite\nvariance.",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "When or why would a data scientist use 10. deep learning?",
    "input": "",
    "output": "We’ll use a two-layer model for our first model. In[72]: class IMDBModel(nn.Module):\ndef __init__(self, input_size):\nsuper(IMDBModel, self).__init__()\nself.dense1 = nn.Linear(input_size, 16)\nself.activation = nn.ReLU()\nself.dense2 = nn.Linear(16, 16)\nself.output = nn.Linear(16, 1)\ndef forward(self, x):\nval = x\nfor _map in [self.dense1,\nself.activation,\nself.dense2,\nself.activation,\nself.output]:\nval = _map(val)\nreturn torch.flatten(val)\nWe now instantiate our model and look at a summary (not shown). In[73]: imdb_model = IMDBModel(imdb_test.tensors[0].size()[1])\nsummary(imdb_model,\ninput_size=imdb_test.tensors[0].size(),\ncol_names=['input_size',\n'output_size',\n'num_params'])\nWe’ll again use a smaller learning rate for these data, hence we pass an\noptimizertotheSimpleModule.Sincethereviewsareclassifiedintopositive\nor negative sentiment, we use SimpleModule.binary_classification().28\nIn[74]: imdb_optimizer = RMSprop(imdb_model.parameters(), lr=0.001)\nimdb_module = SimpleModule.binary_classification(\nimdb_model,\noptimizer=imdb_optimizer)\nHavingloadedthedatasetsintoadatamoduleandcreatedaSimpleModule,\nthe remaining steps are familiar. In[75]: imdb_logger = CSVLogger('logs', name='IMDB')\nimdb_trainer = Trainer(deterministic=True,\nmax_epochs=30,\nlogger=imdb_logger,\ncallbacks=[ErrorTracker()])\nimdb_trainer.fit(imdb_module,\ndatamodule=imdb_dm)\nEvaluating the test error yields roughly 86% accuracy. In[76]: test_results = imdb_trainer.test(imdb_module, datamodule=imdb_dm)\ntest_results\n28Our use of binary_classification() instead of classification() is due to\nsome subtlety in how torchmetrics.Accuracy() works, as well as the data type of\nthetargets.",
    "source": "book_islr",
    "section_title": "456 10. Deep Learning"
  },
  {
    "instruction": "How is j different from x 2x in data science?",
    "input": "",
    "output": "j: (y yˆ )2, (8.1)\ni − Rj\n0\nj=1i\n0∈\nRj\nwhere yˆ is the mean response for the training observations within the\nRj\njth box. Unfortunately, it is computationally infeasible to consider every\npossiblepartitionofthefeaturespaceintoJ boxes.Forthisreason,wetake\natop-down,greedyapproachthatisknownasrecursivebinarysplitting.The\nrecursive\napproachistop-downbecauseitbeginsatthetopofthetree(atwhichpoint\nbinary\nall observations belong to a single region) and then successively splits the splitting\npredictor space; each split is indicated via two new branches further down\non the tree. x 2x: X1≤t1\nX2≤t2 X1≤t3\nX2≤t4\nFIGURE 8.3.TopLeft:Apartitionoftwo-dimensionalfeaturespacethatcould\nnot result from recursive binary splitting. Top Right: The output of recursive\nbinarysplittingonatwo-dimensionalexample.BottomLeft:Atreecorresponding\nto the partition in the top right panel.",
    "source": "book_islr",
    "section_title": "J vs 2X 2X"
  },
  {
    "instruction": "Explain 2 ··· k nk i=1 i in simple terms for a beginner in data science.",
    "input": "",
    "output": "2. Compute fˆ1(x)= K fˆ1(x)= 1 n )y . k=1 k n i=1 i\n3.",
    "source": "book_islr",
    "section_title": "1 2 ··· K nK i=1 i"
  },
  {
    "instruction": "When or why would a data scientist use ! 1 2 3 l-1 l?",
    "input": "",
    "output": "FIGURE10.12.Schematicofasimplerecurrentneuralnetwork.Theinputisa\nsequence of vectors { X \" } L 1 , and here the target is a single response. The network\nprocesses the input sequence X sequentially; each X \" feeds into the hidden layer,\nwhich also has as input the activation vector A \" 1 from the previous element in\n−\nthesequence,andproducesthecurrentactivationvectorA \".Thesamecollections\nofweightsW,UandBareusedaseachelementofthesequenceisprocessed.The\noutput layer produces a sequence of predictions O \" from the current activation\nA \",buttypicallyonlythelastofthese,O L,isofrelevance.Totheleftoftheequal\nsign is a concise representation of the network, which is unrolled into a more\nexplicit version on the right. Figure10.12illustratesthestructureofaverybasicRNNwithasequence\nX = X ,X ,...,X as input, a simple output Y, and a hidden-layer",
    "source": "book_islr",
    "section_title": "! 1 2 3 L-1 L"
  },
  {
    "instruction": "What is a in data science?",
    "input": "",
    "output": "Reject H Type I Error Correct\nDecision 0\nDo Not Reject H Correct Type II Error\n0\nTABLE 13.1. A summary of the possible scenarios associated with testing the\nnull hypothesis H 0. Type I errors are also known as false positives, and Type II\nerrors as false negatives.",
    "source": "book_islr",
    "section_title": "0 a"
  },
  {
    "instruction": "Explain k − z in simple terms for a beginner in data science.",
    "input": "",
    "output": "5. Reject all null hypotheses H for which p <p . 0j j (L)\nalways reject at least as many null hypotheses as Bonferroni — and so it\nshould always be preferred.",
    "source": "book_islr",
    "section_title": "K − Z"
  },
  {
    "instruction": "When or why would a data scientist use , -0.95, 0.56, 0.35, 0.87, 0.88, -1.66, -0.32,?",
    "input": "",
    "output": "-0.3 , -1.36, 0.92, -0.31, 1.28, -1.94, 1.07, 0.07,\n0.79, -0.46, 2.19, -0.27, -0.64, 0.85, 0.13, 0.46,\n-0.09, 0.7 ])\nWe create an array y by adding an independent N(50,1) random variable\nto each element of x. In[29]: y = x + np.random.normal(loc=50, scale=1, size=50)\nThenp.corrcoef()functioncomputesthecorrelationmatrixbetweenxand\nnp.corrcoef()\ny. The off-diagonal elements give the correlation between x and y. In[30]: np.corrcoef(x, y)\nOut[30]:array([[1. , 0.69],\n[0.69, 1. ]]) If you’refollowingalong in yourown Jupyter notebook, then youprobably\nnoticed that you got a different set of results when you ran the past few\ncommands.Inparticular,eachtimewecallnp.random.normal(),wewillget\na different answer, as shown in the following example.",
    "source": "book_islr",
    "section_title": "1.7 , -0.95, 0.56, 0.35, 0.87, 0.88, -1.66, -0.32,"
  },
  {
    "instruction": "Explain lab: cross-validation and the bootstrap 219 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Above we introduced the outer() method of the np.power() function. .outer()\nThe outer() method is applied to an operation that has two arguments,\nnp.power()\nsuch as add(), min(), or power(). It has two arrays as arguments, and then\nformsalargerarraywheretheoperationisappliedtoeachpairofelements\nof the two arrays.",
    "source": "book_islr",
    "section_title": "5.3 Lab: Cross-Validation and the Bootstrap 219"
  },
  {
    "instruction": "What is k in data science?",
    "input": "",
    "output": "familyofdistributions(e.g.multivariatenormal),weinsteadmakeasingle\nassumption:\nWithin the kth class, the p predictors are independent. Stated mathematically, this assumption means that for k =1,...,K,\nf (x)=f (x ) f (x ) f (x ), (4.29)\nk k1 1 k2 2 kp p\n× ×···×\nwhere f is the density function of the jth predictor among observations\nkj\nin the kth class. Why is this assumption so powerful?",
    "source": "book_islr",
    "section_title": "K"
  },
  {
    "instruction": "How is convolutional neural networks 407 different from 10. deep learning in data science?",
    "input": "",
    "output": "convolutional neural networks 407: FIGURE10.6.Schematicshowinghowaconvolutionalneuralnetworkclassifies\nan image of a tiger. The network takes in the image and identifies local features. 10. deep learning: ofwhichisatemplatethatdetermineswhetheraparticularlocalfeatureis\npresent in an image. A convolution filter relies on a very simple operation,\ncalled a convolution, which basically amounts to repeatedly multiplying\nmatrix elements and then adding the results.",
    "source": "book_islr",
    "section_title": "10.3 Convolutional Neural Networks 407 vs 408 10. Deep Learning"
  },
  {
    "instruction": "How is 0.2 0.4 0.6 0.8 1.0 different from 4. classification in data science?",
    "input": "",
    "output": "0.2 0.4 0.6 0.8 1.0: 0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nFIGURE 4.8. A ROC curve for the LDA classifier on the Default data. 4. classification: True class\nor Null + or Non-null Total\n−\nPredicted or Null True Neg. (TN) False Neg.",
    "source": "book_islr",
    "section_title": "0.0 0.2 0.4 0.6 0.8 1.0 vs 156 4. Classification"
  },
  {
    "instruction": "Describe the typical steps involved in i in a data science workflow.",
    "input": "",
    "output": "process, we have left the left hand side the same, but have not increased the right hand\nside. So it suffices to prove the inequality at the end which clearly holds. This method of\nproof is called the variational method. 413\n1+x ≤ ex for all real x\n(1−x)n ≥ 1−nx for 0 ≤ x ≤ 1\n(x+y)2 ≤ 2x2 +2y2\nTriangle Inequality |x+y| ≤ |x|+|y|. Cauchy-Schwartz Inequality |x||y| ≥ xTy\nYoung’s Inequality For positive real numbers p and q where 1 + 1 = 1 and\np q\npositive reals x and y,",
    "source": "book_fods",
    "section_title": "2 i"
  },
  {
    "instruction": "Explain poisson regression on the bikeshare data in simple terms for a beginner in data science.",
    "input": "",
    "output": "ToovercometheinadequaciesoflinearregressionforanalyzingtheBikeshare\ndata set, we will make use of an alternative approach, called Poisson\nregression. Before we can talk about Poisson regression, we must first in-\nPoisson\ntroduce the Poisson distribution. regression\nSuppose that a random variable Y takes on nonnegative integer values,\nPoisson\ni.e.",
    "source": "book_islr",
    "section_title": "4.6.2 Poisson Regression on the Bikeshare Data"
  },
  {
    "instruction": "Explain b in simple terms for a beginner in data science.",
    "input": "",
    "output": "Figure 6.6: Samples of overlapping sets A and B.\nelements from each of A and B to also overlap. One difficulty that might arise is that the\nsmall integers might be used for some special purpose and appear in essentially all sets\nand thus distort the results. To overcome this potential problem, rename all elements\nusing a random permutation.",
    "source": "book_fods",
    "section_title": "B"
  },
  {
    "instruction": "How is 3. linear regression different from 1 in data science?",
    "input": "",
    "output": "3. linear regression: iii. ForafixedvalueofIQandGPA,highschoolgraduatesearn\nmore, on average, than college graduates provided that the\nGPA is high enough. 1: squares (RSS) for the linear regression, and also the training\nRSS for the cubic regression. Would we expect one to be lower\nthantheother,wouldweexpectthemtobethesame,oristhere\nnot enough information to tell?",
    "source": "book_islr",
    "section_title": "128 3. Linear Regression vs 0 1"
  },
  {
    "instruction": "Explain 1 in simple terms for a beginner in data science.",
    "input": "",
    "output": "original data set, the noisier data set, and the less noisy data\nset? Comment on your results. 14.",
    "source": "book_islr",
    "section_title": "0 1"
  },
  {
    "instruction": "How is alert-transformer: bridging asynchronous and synchronous machine learning for real-time event-based spatio-temporal data different from on-device machine learning: an algorithms and learning theory perspective in data science?",
    "input": "",
    "output": "alert-transformer: bridging asynchronous and synchronous machine learning for real-time event-based spatio-temporal data: We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method. on-device machine learning: an algorithms and learning theory perspective: The predominant paradigm for using machine learning models on a device is to train a model in the cloud and perform inference using the trained model on the device. However, with increasing number of smart devices and improved hardware, there is interest in performing model training on the device.",
    "source": "web_arxiv",
    "section_title": "ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data vs On-Device Machine Learning: An Algorithms and Learning Theory Perspective"
  },
  {
    "instruction": "How is exercises different from 32 32 16 in data science?",
    "input": "",
    "output": "exercises: Exercise 4.1 The Fundamental Theorem of Markov chains says that for a connected\nMarkov chain, the long-term average distribution a(t) converges to a stationary distribu-\ntion. Does the t step distribution p(t) also converge for every connected Markov Chain? 32 32 16: Note by symmetry there are only three types of vertices and only two types of rows or\ncolumns. Exercise 4.9 How would you integrate a high dimensional multivariate polynomial dis-\ntribution over some convex region?",
    "source": "book_fods",
    "section_title": "4.10 Exercises vs 16 32 32 16"
  },
  {
    "instruction": "What is ··· k in data science?",
    "input": "",
    "output": "4. When the number of features p is large, there tends to be a deteri-\noration in the performance of KNN and other local approaches that\nperformpredictionusingonlyobservationsthatarenear thetestob-\nservation for which a prediction must be made. This phenomenon is\nknown as the curse of dimensionality, and it ties into the fact that\ncurseofdi-\nnon-parametric approaches often perform poorly when p is large.",
    "source": "book_islr",
    "section_title": "1 ··· K"
  },
  {
    "instruction": "When or why would a data scientist use introduction?",
    "input": "",
    "output": "High dimensional data has become very important. However, high dimensional space\nis very different from the two and three dimensional spaces we are familiar with. Generate\nn points at random in d-dimensions where each coordinate is a zero mean, unit variance\nGaussian. For sufficiently large d, with high probability the distances between all pairs\nof points will be essentially the same. Also the volume of the unit ball in d-dimensions,\nthe set of all points x such that |x| ≤ 1, goes to zero as the dimension goes to infinity.",
    "source": "book_fods",
    "section_title": "2.1 Introduction"
  },
  {
    "instruction": "Explain d in simple terms for a beginner in data science.",
    "input": "",
    "output": "Conversion procedure 2: Controlled Testing. A second natural approach to us-\ning an online learning algorithm A in the distributional setting is to just run a series of\ncontrolled tests. Specifically, suppose that the initial hypothesis produced by algorithm\nA is h .",
    "source": "book_fods",
    "section_title": "D"
  },
  {
    "instruction": "Explain 2 1 2 3 2 3 in simple terms for a beginner in data science.",
    "input": "",
    "output": "it will have error at most 10% with respect to D. What is perhaps surprising about this\ntheorem is that it means that we can each have different ways of describing rules and yet\nall use Occam’s razor. Note that the theorem does not say that complicated rules are\nnecessarily bad, or even that given two rules consistent with the data that the complicated\nrule is necessarily worse. What it does say is that Occam’s razor is a good policy in that\nsimple rules are unlikely to fool us since there are just not that many simple rules.",
    "source": "book_fods",
    "section_title": "1 2 1 2 3 2 3"
  },
  {
    "instruction": "Explain 6. linear model selection and regularization in simple terms for a beginner in data science.",
    "input": "",
    "output": "repeatedthe validationsetapproachusing adifferentsplit ofthe data into\na training set and a validation set, or if we repeated cross-validation using\na different set of cross-validation folds, then the precise model with the\nlowest estimated test error would surely change. In this setting, we can\nselect a model using the one-standard-error rule. We first calculate the\none-\nstandard error of the estimated test MSE for each model size, and then standard-\nselect the smallest model for which the estimated test error is within one error\nstandard error of the lowest point on the curve.",
    "source": "book_islr",
    "section_title": "240 6. Linear Model Selection and Regularization"
  },
  {
    "instruction": "How is online learning different from 2 d in data science?",
    "input": "",
    "output": "online learning: So far we have been considering what is often called the batch learning scenario. You are\ngiven a “batch” of data—the training sample S—and your goal is to use it to produce\na hypothesis h that will have low error on new data, under the assumption that both S\nand the new data are sampled from some fixed distribution D. We now switch to the\nmore challenging online learning scenario where we remove the assumption that data is\nsampled from a fixed probability distribution, or from any probabilistic process at all. 2 d: it for prediction. We will maintain the invariant that every variable in the target disjunc-\ntion is also in our hypothesis, which is clearly true at the start.",
    "source": "book_fods",
    "section_title": "5.8 Online Learning vs 1 2 d"
  },
  {
    "instruction": "How is n different from 2clnn in data science?",
    "input": "",
    "output": "n: nents except for isolated vertices. Theorem 8.11 For p = clnn with c > 1/2, almost surely there are only isolated vertices\nn\nand a giant component. 2clnn: f(cid:48)(cid:48)(k) = + > 0.\nk2 n\nThus, the function f(k) attains its maximum over the range [2,n/2] at one of the extreme\npoints 2 or n/2. At k = 2, f(2) ≈ (1−2c)lnn and at k = n/2, f(n/2) ≈ −cn lnn.",
    "source": "book_fods",
    "section_title": "2 n vs 2 2clnn"
  },
  {
    "instruction": "Explain cross-validation (statistics) part 3 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Two types of cross-validation can be distinguished: exhaustive and non-exhaustive cross-validation. === Exhaustive cross-validation ===\nExhaustive cross-validation methods are cross-validation methods which learn and test on all possible ways to divide the original sample into a training and a validation set. ==== Leave-p-out cross-validation ====\nLeave-p-out cross-validation (LpO CV) involves using p observations as the validation set and the remaining observations as the training set.",
    "source": "web_wikipedia",
    "section_title": "Cross-validation (statistics) part 3"
  },
  {
    "instruction": "Explain document classification 413 in simple terms for a beginner in data science.",
    "input": "",
    "output": "flamingo Cooper’s hawk Cooper’s hawk\nflamingo 0.83 kite 0.60 fountain 0.35\nspoonbill 0.17 great grey owl 0.09 nail 0.12\nwhite stork 0.00 robin 0.06 hook 0.07\nLhasa Apso cat Cape weaver\nTibetan terrier 0.56 Old English sheepdog 0.82 jacamar 0.28\nLhasa 0.32 Shih-Tzu 0.04 macaw 0.12\ncocker spaniel 0.03 Persian cat 0.04 robin 0.12\nFIGURE 10.10. Classification of six photographs using the resnet50 CNN\ntrained on the imagenet corpus. The table below the images displays the true\n(intended)labelatthetopofeachpanel,andthetopthreechoicesoftheclassifier\n(outof100).Thenumbersaretheestimatedprobabilitiesforeachchoice.",
    "source": "book_islr",
    "section_title": "10.4 Document Classification 413"
  },
  {
    "instruction": "Explain 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Use axline() method to overlay those multiple linear regression\ncoefficient estimates on the plot obtained in (e). (h) On this data set, how many backfitting iterations were required\nin order to obtain a “good” approximation to the multiple re-\ngression coefficient estimates? 12.",
    "source": "book_islr",
    "section_title": "1 2"
  },
  {
    "instruction": "How is w different from the log-rank test in data science?",
    "input": "",
    "output": "w: For times t between d and d , we set S(t) = S(d ). Consequently, the\nk k+1 k\nKaplan–Meier survival curve has a step-like shape. the log-rank test: We now continue our analysis of the BrainCancer data introduced in Sec-\ntion 11.3. We wish to compare the survival of males to that of females.",
    "source": "book_islr",
    "section_title": "W vs 11.4 The Log-Rank Test"
  },
  {
    "instruction": "How is 2 d √ different from d 1 d √ in data science?",
    "input": "",
    "output": "2 d √: from a unit variance Gaussian centered at the origin, and let r = |x|. d − β ≤ |y| ≤\n√ √ √\nd + β is equivalent to |r − d| ≥ β. d 1 d √: of variables: y = x2 −1. We want to bound the probability that |y +...+y | ≥ β d.\ni i 1 d\nNotice that E(y ) = E(x2) − 1 = 0.",
    "source": "book_fods",
    "section_title": "1 2 d √ vs 1 d 1 d √"
  },
  {
    "instruction": "What is 5. resampling methods in data science?",
    "input": "",
    "output": "Out[13]:array([23.6166])\nOnecanestimatethevariabilityinthetesterrorbyrunningthefollowing:\nIn[14]: validation = ShuffleSplit(n_splits=10,\ntest_size=196,\nrandom_state=0)\nresults = cross_validate(hp_model,\nAuto.drop(['mpg'], axis=1),\nAuto['mpg'],\ncv=validation)\nresults['test_score'].mean(), results['test_score'].std()\nOut[14]:(23.8022, 1.4218)\nNote that this standard deviation is not a valid estimate of the sam-\npling variability of the mean test score or the individual scores, since the\nrandomly-selected training samples overlap and hence introduce correla-\ntions. But it does give an idea of the Monte Carlo variation incurred by\npicking different random folds.",
    "source": "book_islr",
    "section_title": "220 5. Resampling Methods"
  },
  {
    "instruction": "How is vc-dimension of combinations of concepts different from 2 k in data science?",
    "input": "",
    "output": "vc-dimension of combinations of concepts: Often one wants to create concepts out of other concepts. For example, given several\nlinear separators, one could take their intersection to create a convex polytope. 2 k: comb (h ,...,h ) = {x ∈ X : f(h (x),...,h (x)) = 1}, where here we are using h (x) to\nf 1 k 1 k i\ndenotetheindicatorforwhetherornotx ∈ h . Forexample, f mightbetheANDfunction\ni\nto take the intersection of the sets h , or f might be the majority-vote function.",
    "source": "book_fods",
    "section_title": "5.11.4 VC-Dimension of Combinations of Concepts vs 1 2 k"
  },
  {
    "instruction": "How is 2994.0 4.770e+06 1.0 1282.563 0.805 3.697e-01 different from 7. moving beyond linearity in data science?",
    "input": "",
    "output": "2994.0 4.770e+06 1.0 1282.563 0.805 3.697e-01: Notice the * in the anova_lm() line above. This function takes a variable\nnumber of non-keyword arguments, in this case fitted models. 7. moving beyond linearity: AsanalternativetousinghypothesistestsandANOVA,wecouldchoose\nthe polynomial degree using cross-validation, as discussed in Chapter 5. Nextweconsiderthetaskofpredictingwhetheranindividualearnsmore\nthan $250,000 per year.",
    "source": "book_islr",
    "section_title": "4 2994.0 4.770e+06 1.0 1282.563 0.805 3.697e-01 vs 314 7. Moving Beyond Linearity"
  },
  {
    "instruction": "How is subset selection methods different from lab: linear models and regularization methods 269 in data science?",
    "input": "",
    "output": "subset selection methods: Here we implement methods that reduce the number of parameters in a\nmodel by restricting the model to a subset of the input variables. Forward Selection\nWewillapplytheforward-selectionapproachtotheHittersdata.Wewish\nto predict a baseball player’s Salary on the basis of various statistics asso-\nciated with performance in the previous year. lab: linear models and regularization methods 269: We need to estimate the residual variance σ2, which is the first argument\nin our scoring function above. We will fit the biggest model, using all the\nvariables, and estimate σ2 based on its MSE.",
    "source": "book_islr",
    "section_title": "6.5.1 Subset Selection Methods vs 6.5 Lab: Linear Models and Regularization Methods 269"
  },
  {
    "instruction": "Explain logistic regression in simple terms for a beginner in data science.",
    "input": "",
    "output": "Next, we will fit a logistic regression model in order to predict Direction\nusing Lag1 through Lag5 and Volume. The sm.GLM() function fits gener-\nsm.GLM()\nalized linear models, a class of models that includes logistic regression. generalized\nAlternatively, the function sm.Logit() fits a logistic regression model di-\nlinearmodel\nrectly.",
    "source": "book_islr",
    "section_title": "4.7.2 Logistic Regression"
  },
  {
    "instruction": "What is 1. introduction in data science?",
    "input": "",
    "output": "while\nxT = x x x . i i1 i2 ··· ip\nWe use y\ni\nto denote the i’th observation of t(he variable on which we\nwish to make predictions, such as wage. Hence, we write the set of all n\nobservations in vector form as\ny\n1\ny\n2\ny=\n.",
    "source": "book_islr",
    "section_title": "10 1. Introduction"
  },
  {
    "instruction": "How is ieee, 1998. different from journal of the acm, 46(5):604–632, 1999. in data science?",
    "input": "",
    "output": "ieee, 1998.: [CBFH+97] N. Cesa-Bianchi, Y. Freund, D.P. Helmbold, D. Haussler, R.E. journal of the acm, 46(5):604–632, 1999.: [Kle00] Jon M. Kleinberg. The small-world phenomenon: an algorithm perspective.",
    "source": "book_fods",
    "section_title": "IEEE, 1998. vs JOURNAL OF THE ACM, 46(5):604–632, 1999."
  },
  {
    "instruction": "When or why would a data scientist use { $ }1 { 1 2 l } $?",
    "input": "",
    "output": "example X could represent a one-hot encoding for the %th word based on\n$\nthe language dictionary for the corpus (see the top panel in Figure 10.13\nfor a simple example). As the sequence is processed one vector X at a\n$\ntime, the network updates the activations A in the hidden layer, taking\n$\nas input the vector X and the activation vector A from the previous\n$ $ 1\n−\nstep in the sequence. Each A feeds into the output layer and produces a\n$\nprediction O for Y. O , the last of these, is the most relevant.",
    "source": "book_islr",
    "section_title": "{ $ }1 { 1 2 L } $"
  },
  {
    "instruction": "When or why would a data scientist use 2 3?",
    "input": "",
    "output": "and find early approximation to the first three singular vectors. Then find the minimum\n1-norm vector in the space spanned by the early approximations. Hidden structure\nIn the previous section we discussed overlapping communities. Another issue is hidden\nstructure. Suppose the vertices of a social network could be partitioned into a number of\nstrongly connected communities.",
    "source": "book_fods",
    "section_title": "1 2 3"
  },
  {
    "instruction": "How is k 0 =1 ’ 0 j=1 (2 different from regularization and stochastic gradient descent in data science?",
    "input": "",
    "output": "k 0 =1 ’ 0 j=1 (2: To simplify the expressions to follow, we write z = w + p w x . ik k0 j=1 kj ij\nFirst we take the derivative with respect to β :\nk\n)\n∂R (θ) ∂R (θ) ∂f (x )\ni i θ i\n=\n∂β ∂f (x ) · ∂β\nk θ i k\n= (y f (x )) g(z ). regularization and stochastic gradient descent: Gradient descent usually takes many steps to reach a local minimum. In\npractice, there are a number of approaches for accelerating the process.",
    "source": "book_islr",
    "section_title": "1 k 0 =1 ’ 0 j=1 (2 vs 10.7.2 Regularization and Stochastic Gradient Descent"
  },
  {
    "instruction": "How is 2 1 2 different from 1 1 in data science?",
    "input": "",
    "output": "2 1 2: Hence, V is at least as good as W and so is a best-fit 2-dimensional subspace. 2\nFor general k, proceed by induction. 1 1: 44\nNote that the n-dimensional vector Av is a list of lengths (with signs) of the projec-\ni\ntions of the rows of A onto v . Think of |Av | = σ (A) as the component of the matrix\ni i i\nA along v .",
    "source": "book_fods",
    "section_title": "1 2 1 2 vs 2 1 1"
  },
  {
    "instruction": "When or why would a data scientist use warning propagation?",
    "input": "",
    "output": "Significant progress has been made using methods similar to belief propagation in\nfinding satisfying assignments for 3-CNF formulas. Thus, we include a section on a\nversion of belief propagation, called warning propagation, that is quite effective in finding\nassignments. ConsiderafactorgraphforaSATproblem(Figure9.10). Indexthevariables\nbyi, j, andk andthefactorsbya, b, andc. Factorasendsamessagem toeachvariablei\nai\nthat appears in the factor a called a warning.",
    "source": "book_fods",
    "section_title": "9.20 Warning Propagation"
  },
  {
    "instruction": "What is implementing length squared sampling in two passes in data science?",
    "input": "",
    "output": "Traditional matrix algorithms often assume that the input matrix is in random access\nmemory (RAM) and so any particular entry of the matrix can be accessed in unit time. For massive matrices, RAM may be too small to hold the entire matrix, but may be able\nto hold and compute with the sampled columns and rows. Consider a high-level model where the input matrix or matrices have to be read from\nexternal memory using one pass in which one can read sequentially all entries of the ma-\ntrix and sample.",
    "source": "book_fods",
    "section_title": "6.3.2 Implementing Length Squared Sampling in Two Passes"
  },
  {
    "instruction": "How is number of occurrences of a given element. different from 2 n i in data science?",
    "input": "",
    "output": "number of occurrences of a given element.: To count the number of occurrences of a given element in a stream requires at most\nlogn space where n is the length of the stream. Clearly, for any length stream that occurs\nin practice, one can afford logn space. 2 n i: {1,2,...,m}, and want to determine whether there is some s ∈ {1,2,...,m} which oc-\ncurs more than n/2 times and if so which s. It is easy to see that to solve the problem\nexactlyonread-oncestreamingdatawithadeterministicalgorithm,requiresΩ(min(n,m))\nspace. Suppose n is even and the last n/2 items are identical.",
    "source": "book_fods",
    "section_title": "6.2.2 Number of Occurrences of a Given Element. vs 1 2 n i"
  },
  {
    "instruction": "What is s in data science?",
    "input": "",
    "output": "but err (h) ≥ (cid:15)/2. Prob(B∗) = Prob(B) since drawing 2n points from D and randomly\nS(cid:48)\npartitioning them into two sets of size n produces the same distribution on (S,S(cid:48)) as does\ndrawing S and S(cid:48) directly. The advantage of this new experiment is that we can now\nargue that Prob(B∗) is low by arguing that for any set S(cid:48)(cid:48) of size 2n, Prob(B∗|S(cid:48)(cid:48)) is low,\nwith probability now taken over just the random partition of S(cid:48)(cid:48) into S and S(cid:48).",
    "source": "book_fods",
    "section_title": "S"
  },
  {
    "instruction": "What is /k in data science?",
    "input": "",
    "output": "etaR\nrorrE\nTraining Errors\nTest Errors\nFIGURE 2.17. The KNN training error rate (blue, 200 observations) and test\nerrorrate(orange,5,000observations)onthedatafromFigure2.13,asthelevel\nof flexibility (assessed using 1/K on the log scale) increases, or equivalently as\nthe number of neighbors K decreases. The black dashed line indicates the Bayes\nerror rate.",
    "source": "book_islr",
    "section_title": "1/K"
  },
  {
    "instruction": "What is k in data science?",
    "input": "",
    "output": "the first hidden layer as inputs and computes new activations\nA(2) = h(2)(X)\n$ $ (10.11)\n= g(w(2)+ K1 w(2)A(1))\n$0 k=1 $k k\nfor % = 1,...,K . Notice that each of )the activations in the second layer\n2\nA(2) =h(2)(X)isafunctionoftheinputvectorX.Thisisthecasebecause\n$ $\nwhile they are explicitly a function of the activations A(1) from layer L ,\nk 1\nthese in turn are functions of X. This would also be the case with more\nhidden layers.",
    "source": "book_islr",
    "section_title": "1 k"
  },
  {
    "instruction": "Explain 3 in simple terms for a beginner in data science.",
    "input": "",
    "output": "be 1 if x is a laptop and 0 otherwise. These recognition tasks are related in that image\n174\nfeatures that are good for one task are likely to be helpful for the others as well. Thus,\none approach to multi-task learning is to try to learn a common representation under\nwhich each of the target functions can be described as a simple function.",
    "source": "book_fods",
    "section_title": "2 3"
  },
  {
    "instruction": "Explain images drawn from the cifar100 database.4 this database consists of in simple terms for a beginner in data science.",
    "input": "",
    "output": "60,000imageslabeledaccordingto20superclasses(e.g.aquaticmammals),\nwith five classes per superclass (beaver, dolphin, otter, seal, whale). Each\nimage has a resolution of 32 32 pixels, with three eight-bit numbers per\n×\npixel representing red, green and blue. The numbers for each image are\norganized in a three-dimensional array called a feature map.",
    "source": "book_islr",
    "section_title": "75 images drawn from the CIFAR100 database.4 This database consists of"
  },
  {
    "instruction": "What is proportional hazards in data science?",
    "input": "",
    "output": "The Proportional Hazards Assumption\nThe proportional hazards assumption states that\nproportional\nhazards\np\nassumption\nh(tx )=h (t)exp x β , (11.14)\ni 0 ij j\n|  \nj=1\n0\n \nwhere h (t) 0 is an unspecified function, known as the baseline hazard. 0\n≥ baseline\nIt is the hazard function for an individual with features x = =x =\ni1 ··· ip hazard\n0. The name “proportional hazards” arises from the fact that the hazard\nfunctionforanindividualwithfeaturevectorx issomeunknownfunction\ni\n9SeeExercise9.",
    "source": "book_islr",
    "section_title": "11.5.2 Proportional Hazards"
  },
  {
    "instruction": "How is 3. linear regression different from simple linear regression in data science?",
    "input": "",
    "output": "3. linear regression: 2. How strong is the relationship between advertising budget and sales? simple linear regression: Simple linear regression lives up to its name: it is a very straightforward\nsimplelinear\napproach for predicting a quantitative response Y on the basis of a sin-\nregression\ngle predictor variable X. It assumes that there is approximately a linear\nrelationship between X and Y.",
    "source": "book_islr",
    "section_title": "70 3. Linear Regression vs 3.1 Simple Linear Regression"
  },
  {
    "instruction": "Describe the typical steps involved in mcmc, 78 in a data science workflow.",
    "input": "",
    "output": "Random projection, 25\nMean value theorem, 457\ntheorem, 25\nMedian, 423\nRandom variable, 420\nMetropolis-Hastings algorithm, 83\nRandom walk\nMixing time, 80\nEucleadean space, 109\nModel\nin three dimensions, 110\nrandom graph, 245\nin two dimensions, 110\nMolloy Reed, 285\non lattice, 109\nMoment generating function, 455\nundirected graph, 102\nMutually independent, 421\nweb, 112\nRapid Mixing, 82\nNearest neighbor problem, 27\nReal spectral theorem, 439\nNonuniform Random Graphs, 284\nReplication, 270\nNormalized conductance, 80, 89\nResistance, 97, 108\nNumber of triangles in G(n,p), 251\nefffective, 101\nOhm’s law, 99 Restart, 113\nOrthonormal, 445 value, 113\nReturn time, 113\nPage rank, 113\npersonalized , 116 Sample space, 420\nPersistent, 77 Sampling\nPhase transition, 253 length squared, 194\nCNF-sat, 279 Satisfying assignments\nnonfinite components, 291 expected number of, 280\nPoisson distribution, 426 Scale function, 386\nPolar coordinates, 17 Scale vector, 386\nPolynomial interpolation, 190 Second moment method, 251, 254\nPositive semidefinite, 451 Sharp threshold, 253\nPower iteration, 62 Similar matrices, 437\n468\nSingular value decomposition, 40 Variance, 422\nSingular vector, 42 variational method, 413\nfirst, 43 VC-dimension, 148\nleft, 45 convex polygons, 151\nright, 45 finite sets, 153\nsecond, 43 half spaces, 151\nSix-degrees separation, 295 intervals, 151\nSketch pairs of intervals, 151\nmatrix, 197 rectangles, 151\nSketches spheres, 152\ndocuments, 201 Viterbi algorithm, 334\nSmall world, 294 Voltage\nSmallest-clause heuristic, 280 probabilistic interpretation, 99\nSpam, 115\nWavelet, 385\nSpectral clustering, 216\nWorld Wide Web, 112\nSperner’s lemma, 459\nStanley Milgram, 294\nYoung’s inequality, 414, 416\nState, 82\nStirling approximation, 414\nStreaming model, 181\nSymmetric matrices, 439\nTail bounds, 430, 433\nTail of Gaussian, 419\nTaylor series, 409\nThreshold, 252\nCNF-sat, 277\ndiameter O(lnn), 269\ndisappearance of isolated vertices, 259\nemergence of cycles, 265\nemergence of diameter two, 256\ngiant component plus isolated vertices,\n267\nTime domain, 371\nTotal variation distance, 82\nTrace, 448\nTriangle inequality, 414\nTriangles, 250\nUnion bound, 422\nUnit-clause heuristic, 280\nUnitary matrix, 445\nUnsatisfiability, 279\n469\nReferences\n[AB15] PranjalAwasthiandMaria-FlorinaBalcan. Centerbasedclustering: Afoun-\ndational perspective. In Christian Hennig, Marina Meila, Fionn Murtagh,\nand Roberto Rocci, editors, Handbook of cluster analysis. CRC Press, 2015. [ACORT11] Dimitris Achlioptas, Amin Coja-Oghlan, and Federico Ricci-Tersenghi. On\nthe solution-space geometry of random constraint satisfaction problems.",
    "source": "book_fods",
    "section_title": "MCMC, 78"
  },
  {
    "instruction": "When or why would a data scientist use 2?",
    "input": "",
    "output": "x=−∞\ni=0 j=0 i=0\nmalized so that\n(cid:90) ∞ d−1\n(cid:88)\nφ(x)φ(x−k)dx = δ(k), it follows that c c = 2δ(k). i i−2k\n−∞\ni=0\n392\nScale and wavelet coefficients equations\nd−1\nφ(x) =\n(cid:80)d−1c\nφ(2x−k) ψ(x) =\n(cid:80)\nb φ(x−k)\nk=0 k k\nk=0\n∞ ∞\n(cid:82) (cid:82)\nφ(x)φ(x−k)dx = δ(k) φ(x)ψ(x−k) = 0\n−∞ x=−∞\nd (cid:80) −1 c = 2 (cid:82) ∞ ψ(x)dx = 0\nj\nj=0 x=−∞\n∞\nd−1 (cid:82)\n(cid:80) c c = 2δ(k) ψ(x)ψ(x−k)dx = δ(k)\nj j−2k\nj=0 x=−∞\nd−1\nc = 0 unless 0 ≤ k ≤ d−1 (cid:80) (−1)kb b = 2δ(k)\nk i i−2k\ni=0\nd even d (cid:80) −1\nc b = 0\nj j−2k\nj=0\nd−1 d−1\n(cid:80) c 2j = (cid:80) c 2j+1 d (cid:80) −1 b = 0\nj=0 j=0 j\nj=0\nb = (−1)kc\nk d−1−k\nOne designs wavelet systems so the above conditions are satisfied. Lemma 11.2 provides a necessary but not sufficient condition on the coefficients of\nthe dilation equation for shifts of the scale function to be orthogonal. One should note\nthat the conditions of Lemma 11.2 are not true for the triangular or piecewise quadratic\nsolutions to",
    "source": "book_fods",
    "section_title": "2 2"
  },
  {
    "instruction": "Explain 11. survival analysis and censored data in simple terms for a beginner in data science.",
    "input": "",
    "output": "Regardless of which test we use, we see that there is no clear evidence for\na difference in survival between males and females. As we learned in this\nchapter, the score test from the Cox model is exactly equal to the log rank\ntest statistic! Nowwefitamodelthatmakesuseofadditionalpredictors.Wefirstnote\nthatoneofourdiagnosisvaluesismissing,hencewedropthatobservation\nbefore continuing.",
    "source": "book_islr",
    "section_title": "492 11. Survival Analysis and Censored Data"
  },
  {
    "instruction": "What is 0m in data science?",
    "input": "",
    "output": "therefore,thenumberofpermutedt-statisticsthatexceedthethresholdcin\nabsolutevalueprovidesanestimateforE(V).Thisestimatecanbefurther\nimproved by repeating the permutation process B times, for a large value\nof B, and averaging the results. Algorithm 13.4 details this procedure.18 It provides what is known as a\nplug-inestimateoftheFDR,becausetheapproximationin(13.13)allowsus\nto estimate the FDR by plugging R into the denominator and an estimate\nfor E(V) into the numerator. We apply the re-sampling approach to the FDR from Algorithm 13.4,\nas well as the Benjamini–Hochberg approach from Algorithm 13.2 using\ntheoreticalp-values,tothem=2,308genesintheKhandataset.Resultsare\nshowninFigure13.9.Weseethatforagivennumberofrejectedhypotheses,\nthe estimated FDRs are almost identical for the two methods.",
    "source": "book_islr",
    "section_title": "01 0m"
  },
  {
    "instruction": "Describe the typical steps involved in 1 in a data science workflow.",
    "input": "",
    "output": "(3,0)\n(2,0)\nFigure 3.6 a Figure 3.6 b\nFigure 3.6: SVD problem\n1. Run the power method starting from x =\n(cid:0)1(cid:1)\nfor k = 3 steps. What does this give\n1\nas an estimate of v ? 1\n2. What actually are the v ’s, σ ’s, and u ’s? It may be easiest to do this by computing\ni i i\nthe eigenvectors of B = ATA.",
    "source": "book_fods",
    "section_title": "3 1"
  },
  {
    "instruction": "When or why would a data scientist use ≤ ≤ e?",
    "input": "",
    "output": "Vol(S ∩R)\ni−1\nbecause Vol(S )/Vol(S ) =\n(cid:0)\n1+ 1\n(cid:1)d\n< e, and the fraction of S occupied by R is less\ni i−1 d i\nthan or equal to the fraction of S occupied by R (due to the convexity of R and the\ni−1\nfact that the center of the spheres lies in R). This implies that the ratio Vol(Si∩R) can\nVol(Si−1∩R)\nbe estimated by rejection sampling, i.e., selecting points in S ∩R uniformly at random\ni\nand computing the fraction in S ∩R, provided one can select points at random from a\ni−1\nd-dimensional convex region. The number of spheres is at most\nO(log r) = O(rd)\n1+(1/d)\nwhere r is the ratio of the radius of S to the radius of S . This means that it suffices\nk 1\nto estimate each ratio to a factor of (1± (cid:15) ) in order to estimate the overall volume to\nerd\nerror 1±(cid:15). Itremainstoshowhowtodrawauniformrandomsamplefromad-dimensionalconvex\nset.",
    "source": "book_fods",
    "section_title": "1 ≤ ≤ e"
  },
  {
    "instruction": "How is why estimate f? different from p in data science?",
    "input": "",
    "output": "why estimate f?: There are two main reasons that we may wish to estimate f: prediction\nand inference. We discuss each in turn. p: blood sample that can be easily measured in a lab, and Y is a variable\nencoding the patient’s risk for a severe adverse reaction to a particular\ndrug. It is natural to seek to predict Y using X, since we can then avoid\ngiving the drug in question to patients who are at high risk of an adverse\nreaction—that is, patients for whom the estimate of Y is high.",
    "source": "book_islr",
    "section_title": "2.1.1 Why Estimate f? vs 1 p"
  },
  {
    "instruction": "What is cross-validation on classification problems in data science?",
    "input": "",
    "output": "In this chapter so far, we have illustrated the use of cross-validation in the\nregression setting where the outcome Y is quantitative, and so have used\nMSE to quantify test error. But cross-validation can also be a very useful\napproachintheclassificationsettingwhenY isqualitative.Inthissetting,\ncross-validation works just as described earlier in this chapter, except that\nrather than using MSE to quantify test error, we instead use the number\nof misclassified observations. For instance, in the classification setting, the\nLOOCV error rate takes the form\nn\n1\nCV = Err , (5.4)\n(n) n i\ni=1\n0\nwhere Err =I(y =yˆ).",
    "source": "book_islr",
    "section_title": "5.1.5 Cross-Validation on Classification Problems"
  },
  {
    "instruction": "Explain l = in simple terms for a beginner in data science.",
    "input": "",
    "output": "i\n=S(y\ni\n) if the ith observation is censored\n=f(y i )δiS(y i )1 − δi. (11.12)\nTheintuitionbehind(11.12)isasfollows:ifY =y andtheithobservation\ni\nis not censored, then the likelihood is the probability of dying in a tiny in-\ntervalaroundtimey .Iftheithobservationiscensored,thenthelikelihood\ni\n7Duetothe∆tinthedenominatorof (11.9),thehazardfunctionisarateofdeath,\nrather than a probability of death. However, higher values of h(t) directly correspond\nto a higher probability of death, just as higher values of a probability density function\ncorrespondtomorelikelyoutcomesforarandomvariable.Infact,h(t)istheprobability\ndensityfunctionforT conditionalonT >t.",
    "source": "book_islr",
    "section_title": "L ="
  },
  {
    "instruction": "When or why would a data scientist use n1?",
    "input": "",
    "output": "page 254 displays the first principal component loading vector (green solid\nline) on an advertising data set. In these data, there are only two features,\nand so the observations as well as the first principal component loading\nvector can be easily displayed. As can be seen from (6.19), in that data set\nφ =0.839 and φ =0.544.",
    "source": "book_islr",
    "section_title": "11 n1"
  },
  {
    "instruction": "How is the challenge of multiple testing 563 different from a 0 in data science?",
    "input": "",
    "output": "the challenge of multiple testing 563: Table13.1summarizesthepossiblescenariosassociatedwithtestingthe\nnullhypothesisH .10 Oncethehypothesistestisperformed,therow ofthe\n0\ntable is known (based on whether or not we have rejected H ); however, it\n0\nis impossible for us to know which column we are in. If we reject H when\n0\nH isfalse(i.e.,whenH istrue),orifwedonotrejectH whenitistrue, a 0: then we arrived at the correct result. However, if we erroneously reject H\n0\nwhenH isinfacttrue,thenwehavecommittedaTypeIerror.TheTypeI\n0\nTypeIerror\nerror rate is defined as the probability of making a Type I error given that\nTypeIerror\nH holds, i.e., the probability of incorrectly rejecting H .",
    "source": "book_islr",
    "section_title": "13.2 The Challenge of Multiple Testing 563 vs 0 a 0"
  },
  {
    "instruction": "How is otherwise ∂x 0 otherwise different from 2 1 2 3 in data science?",
    "input": "",
    "output": "otherwise ∂x 0 otherwise: seems to work well even though its derivative at x = 0 is undefined. An advantage of\nReLU over sigmoid is that ReLU does not saturate far from the origin. 2 1 2 3: way one trains one set of weights at a time. Training a deep learning network of 7 or 8 levels using gradient descent can be compu-\ntationally expensive.26 To address this issue one can train one level at a time on unlabeled\ndata using an idea called autoencoding.",
    "source": "book_fods",
    "section_title": "0 otherwise ∂x 0 otherwise vs 1 2 1 2 3"
  },
  {
    "instruction": "Explain x 2 x 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "FIGURE 9.9. Left: An SVM with a polynomial kernel of degree 3 is applied to\nthe non-linear data from Figure 9.8, resulting in a far more appropriate decision\nrule.Right:AnSVMwitharadialkernelisapplied.Inthisexample,eitherkernel\nis capable of capturing the decision boundary. classifier in a higher-dimensional space involving polynomials of degree d,\nratherthanintheoriginalfeaturespace.Whenthesupportvectorclassifier\niscombinedwithanon-linearkernelsuchas(9.22),theresultingclassifieris\nknownasasupportvectormachine.Notethatinthiscasethe(non-linear)\nfunction has the form\nf(x)=β + α K(x,x ).",
    "source": "book_islr",
    "section_title": "X 2 X 2"
  },
  {
    "instruction": "What is application to gene expression data in data science?",
    "input": "",
    "output": "We now examine the Khan data set, which consists of a number of tissue\nsamples corresponding to four distinct types of small round blue cell tu-\nmors. For each tissue sample, gene expression measurements are available. Thedatasetconsistsoftrainingdata,xtrainandytrain,andtestingdata,\nxtest and ytest.",
    "source": "book_islr",
    "section_title": "9.6.5 Application to Gene Expression Data"
  },
  {
    "instruction": "When or why would a data scientist use p?",
    "input": "",
    "output": "hood gives us (up to a proportionality constant) the posterior distribution,\nposterior\nwhich takes the form\ndistribution\np(β X,Y) f(Y X,β)p(β X)=f(Y X,β)p(β),\n| ∝ | | |\nwhere the proportionality above follows from Bayes’ theorem, and the\nequality above follows from the assumption that X is fixed. We assume the usual linear model,\nY =β +X β + +X β +\",",
    "source": "book_islr",
    "section_title": "1 p"
  },
  {
    "instruction": "Explain 4. classification in simple terms for a beginner in data science.",
    "input": "",
    "output": "We recall that the logistic regression model had very underwhelming p-\nvalues associated with all of the predictors, and that the smallest p-value,\nthough not very small, corresponded to Lag1. Perhaps by removing the\nvariables that appear not to be helpful in predicting Direction, we can\nobtain a more effective model. After all, using predictors that have no\nrelationship with the response tends to cause a deterioration in the test\nerror rate (since such predictors cause an increase in variance without a\ncorresponding decrease in bias), and so removing such predictors may in\nturnyieldanimprovement.Belowwerefitthelogisticregressionusingjust\nLag1 and Lag2, which seemed to have the highest predictive power in the\noriginal logistic regression model.",
    "source": "book_islr",
    "section_title": "178 4. Classification"
  },
  {
    "instruction": "What is the bootstrap in data science?",
    "input": "",
    "output": "WeillustratetheuseofthebootstrapinthesimpleexampleofSection5.2,\nas well as on an example involving estimating the accuracy of the linear\nregression model on the Auto data set. Estimating the Accuracy of a Statistic of Interest\nOne of the great advantages of the bootstrap approach is that it can be\nappliedinalmostallsituations.Nocomplicatedmathematicalcalculations\nare required. While there are several implementations of the bootstrap in\nPython,itsuseforestimatingstandarderrorissimpleenoughthatwewrite\nourownfunctionbelowforthecasewhenourdataisstoredinadataframe.",
    "source": "book_islr",
    "section_title": "5.3.3 The Bootstrap"
  },
  {
    "instruction": "What is logistic regression part 7 in data science?",
    "input": "",
    "output": "=== Maximum likelihood estimation (MLE) ===\nThe regression coefficients are usually estimated using maximum likelihood estimation. Unlike linear regression with normally distributed residuals, it is not possible to find a closed-form expression for the coefficient values that maximize the likelihood function so an iterative process must be used instead; for example Newton's method. This process begins with a tentative solution, revises it slightly to see if it can be improved, and repeats this revision until no more improvement is made, at which point the process is said to have converged.",
    "source": "web_wikipedia",
    "section_title": "Logistic regression part 7"
  },
  {
    "instruction": "Explain 1 4 4 in simple terms for a beginner in data science.",
    "input": "",
    "output": "y x\nn n\nFigure 9.7: A Markov random field with a single loop. and thus J(cid:48) might decrease in value. As long as J(cid:48) decreases by some finite amount, we\ncan increase J by increasing k sufficiently.",
    "source": "book_fods",
    "section_title": "1 1 4 4"
  },
  {
    "instruction": "Explain ata in simple terms for a beginner in data science.",
    "input": "",
    "output": "(cid:1)k\nx\nw = . (cid:12) (cid:12)\n(cid:12)(ATA)kx(cid:12)\n(cid:12) (cid:12)\n10E.g., suppose each entry in the first row of A is nonzero and the rest of A is zero. 52\nThen w has a component of at most ε perpendicular to V.\nProof: Let\nr\n(cid:88)\nA = σ u vT\ni i i\ni=1\nbe the SVD of A.",
    "source": "book_fods",
    "section_title": "ATA"
  },
  {
    "instruction": "When or why would a data scientist use 13. multiple testing?",
    "input": "",
    "output": "acumen. She tells 1,024 (1,024 = 210) potential new clients that she can\ncorrectlypredictwhetherApple’sstockpricewillincreaseordecreasefor10\ndays running. There are 210 possibilities for how Apple’s stock price might\nchange over the course of these 10 days. Therefore, she emails each client\none of these 210 possibilities. The vast majority of her potential clients\nwill find that the stockbroker’s predictions are no better than chance (and\nmany will find them to be even worse than chance).",
    "source": "book_islr",
    "section_title": "564 13. Multiple Testing"
  },
  {
    "instruction": "What is t in data science?",
    "input": "",
    "output": "Let us define the distance between these two clusterings to be the fraction of points that\nwould have to be re-clustered in C to make C match C(cid:48), where by “match” we mean that\nthere should be a bijection between the clusters of C and the clusters of C(cid:48). We can write\nthis distance mathematically as:\nk",
    "source": "book_fods",
    "section_title": "T"
  },
  {
    "instruction": "How is |s| different from l s l |s| in data science?",
    "input": "",
    "output": "|s|: rate of the algorithm on S (the total number of mistakes of the algorithm divided by\n177\n(cid:113)\n|S|) will be at most err (h ) + O( Llog(L(T))), where h = argmin err (h)\nS L |S| L h:L(h)=L S\nis the pruning of T with L leaves of lowest error on S.\n3. In the above question, we assumed L was given. l s l |s|: L(T) copies of the above algorithm (one for each value of L) and then combining\nthese algorithms using the experts algorithm (in this case, none of them will be\nsleeping). Exercise 5.9 Kernels; (Section 5.3) Prove Theorem 5.2.",
    "source": "book_fods",
    "section_title": "|S| vs L S L |S|"
  },
  {
    "instruction": "Explain an overview of classification in simple terms for a beginner in data science.",
    "input": "",
    "output": "generalized\nlinear\nClassification problems occur often, perhaps even more so than regression models\nPoisson\nproblems. Some examples include:\nregression\n© Springer Nature Switzerland AG 2023 135\nG. James et al., An Introduction to Statistical Learning, Springer Texts in Statistics,\nhttps://doi.org/10.1007/978-3-031-38747-0_4",
    "source": "book_islr",
    "section_title": "4.1 An Overview of Classification"
  },
  {
    "instruction": "When or why would a data scientist use n?",
    "input": "",
    "output": "and let S denote the subset of examples on which h makes a prediction (e.g., this could\ni i\nbe articles with the word “football” in them). We consider the online learning model,\nand let mistakes(A,S) denote the number of mistakes of an algorithm A on a sequence\nof examples S. Then the guarantee of our algorithm A will be that for all i",
    "source": "book_fods",
    "section_title": "1 n"
  },
  {
    "instruction": "What is s2 in data science?",
    "input": "",
    "output": "Out[68]: coef std err t P>|t|\nintercept 73.5974 5.132 14.340 0.000\nmnth[Jan] -46.0871 4.085 -11.281 0.000\nmnth[Feb] -39.2419 3.539 -11.088 0.000\nmnth[March] -29.5357 3.155 -9.361 0.000\nmnth[April] -4.6622 2.741 -1.701 0.089\nmnth[May] 26.4700 2.851 9.285 0.000\nmnth[June] 21.7317 3.465 6.272 0.000\nmnth[July] -0.7626 3.908 -0.195 0.845\nmnth[Aug] 7.1560 3.535 2.024 0.043\nmnth[Sept] 20.5912 3.046 6.761 0.000\nmnth[Oct] 29.7472 2.700 11.019 0.000\nmnth[Nov] 14.2229 2.860 4.972 0.000\nhr[0] -96.1420 3.955 -24.307 0.000\nhr[1] -110.7213 3.966 -27.916 0.000\nhr[2] -117.7212 4.016 -29.310 0.000\n..... ....... ..... ...... .....\nWhatisthedifferencebetweenthetwocodings?InM2_lm,acoefficientesti-\nmateisreportedforallbutlevel23ofhrandlevelDecofmnth.Importantly,\nin M2_lm, the (unreported) coefficient estimate for the last level of mnth is\nnot zero: instead, it equals the negative of the sum of the coefficient esti-\nmatesforalloftheotherlevels.Similarly,inM2_lm,thecoefficientestimate\nforthelastlevelofhristhenegativeofthesumofthecoefficientestimates\nfor all of the other levels. This means that the coefficients of hr and mnth\nin M2_lm will always sum to zero, and can be interpreted as the difference\nfrom the mean level. For example, the coefficient for January of 46.087\n−\nindicates that, holding all other variables constant, there are typically 46\nfewer riders in January relative to the yearly average.",
    "source": "book_islr",
    "section_title": "S2"
  },
  {
    "instruction": "When or why would a data scientist use selecting the tuning parameter?",
    "input": "",
    "output": "Just as the subset selection approaches considered in Section 6.1 require\na method to determine which of the models under consideration is best,\nimplementingridgeregressionandthelassorequiresamethodforselecting\na value for the tuning parameter λ in (6.5) and (6.7), or equivalently, the\nvalueoftheconstraintsin(6.9)and(6.8).Cross-validationprovidesasim-\nple way to tackle this problem. We choose a grid of λ values, and compute\nthecross-validationerrorforeachvalueofλ,asdescribedinChapter5.We\nthen select the tuning parameter value for which the cross-validation error\nissmallest.Finally,themodelisre-fitusingalloftheavailableobservations\nand the selected value of the tuning parameter. Figure 6.12 displays the choice of λ that results from performing leave-\none-out cross-validation on the ridge regression fits from the Credit data\nset. The dashed vertical lines indicate the selected value of λ. In this case\nthe value is relatively small, indicating that the optimal fit only involves a\nsmallamountofshrinkagerelativetotheleastsquaressolution.Inaddition,\nthe dip is not very pronounced, so there is rather a wide range of values\nthatwouldgiveaverysimilarerror.Inacaselikethiswemightsimplyuse\nthe least squares solution.",
    "source": "book_islr",
    "section_title": "6.2.3 Selecting the Tuning Parameter"
  },
  {
    "instruction": "When or why would a data scientist use 1.0 1.5 2.0?",
    "input": "",
    "output": "0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nTime\nytilibaborP\nlavivruS\nFIGURE 11.4. Top: In a simple example with p = 1 and a binary covariate\nx i 0,1 , the log hazard and the survival function under the model (11.14)\n∈ { }\nare shown (green for x i = 0 and black for x i = 1). Because of the proportional\nhazardsassumption (11.14),theloghazardfunctionsdifferbyaconstant,andthe\nsurvival functions do not cross. Bottom: Again we have a single binary covariate\nx i 0,1 . However, the proportional hazards assumption (11.14) does not hold.",
    "source": "book_islr",
    "section_title": "0.5 1.0 1.5 2.0"
  },
  {
    "instruction": "What is 0 in data science?",
    "input": "",
    "output": "distributionofT in(13.1)followsapproximatelyaN(0,1)distribution7 —\nthatis,anormaldistributionwithmean0andvariance1.Thisdistribution\nis displayed in Figure 13.1. We see that the vast majority — 98% — of the\nN(0,1) distribution falls between 2.33 and 2.33. This means that under\n−\nH , we would expect to see such a large value of T only 2% of the time.",
    "source": "book_islr",
    "section_title": "0 0"
  },
  {
    "instruction": "Explain 4. classification in simple terms for a beginner in data science.",
    "input": "",
    "output": "True default status\nNo Yes Total\nPredicted No 9432 138 9570\ndefault status Yes 235 195 430\nTotal 9667 333 10000\nTABLE 4.5. A confusion matrix compares the LDA predictions to the true\ndefault statuses for the 10,000 training observations in the Default data set,\nusing a modified threshold value that predicts default for any individuals whose\nposterior default probability exceeds 20%.",
    "source": "book_islr",
    "section_title": "154 4. Classification"
  },
  {
    "instruction": "How is nwonknu nairavo different from lab: unsupervised learning 551 in data science?",
    "input": "",
    "output": "nwonknu nairavo: CLCSN CLCSN ETATSORP AMONALEM NOLOC NAIRAVO CLCSN LANER NOLOC ETATSORP NOLOC NAIRAVO NOLOC NOLOC CLCSN CLCSN LANER CLCSN LANER LANER LANER LANER LANER SNC SNC SNC\nSingle Linkage\nFIGURE 12.19. The NCI60 cancer cell line microarray data, clustered with\naverage,complete,andsinglelinkage,andusingEuclideandistanceasthedissim-\nilarity measure. lab: unsupervised learning 551: We can cut the dendrogram at the height that will yield a particular\nnumber of clusters, say four:\nIn[55]: linkage_comp = compute_linkage(hc_comp)\ncomp_cut = cut_tree(linkage_comp, n_clusters=4).reshape(-1)\npd.crosstab(nci_labs['label'],\npd.Series(comp_cut.reshape(-1), name='Complete'))\nThere are some clear patterns. All the leukemia cell lines fall in one\ncluster,whilethebreastcancercelllinesarespreadoutoverthreedifferent\nclusters.",
    "source": "book_islr",
    "section_title": "NWONKNU NAIRAVO vs 12.5 Lab: Unsupervised Learning 551"
  },
  {
    "instruction": "When or why would a data scientist use classification trees?",
    "input": "",
    "output": "A classification tree is very similar to a regression tree, except that it is\nclassification\nused to predict a qualitative response rather than a quantitative one. Re-\ntree\ncall that for a regression tree, the predicted response for an observation is\ngiven by the mean response of the training observations that belong to the\nsame terminal node. In contrast, for a classification tree, we predict that\neach observation belongs to the most commonly occurring class of training\nobservationsintheregiontowhichitbelongs.Ininterpretingtheresultsof\naclassificationtree,weareofteninterestednotonlyintheclassprediction\ncorresponding to a particular terminal node region, but also in the class\nproportions among the training observations that fall into that region. The task of growing a classification tree is quite similar to the task of\ngrowingaregressiontree.Justasintheregressionsetting,weuserecursive\n2Although CV error is computed as a function of α, it is convenient to display the\nresultasafunctionof T ,thenumberofleaves;thisisbasedontherelationshipbetween\n| |\nαand T intheoriginaltreegrowntoallthetrainingdata. | |",
    "source": "book_islr",
    "section_title": "8.1.2 Classification Trees"
  },
  {
    "instruction": "What is r2 in data science?",
    "input": "",
    "output": "statistic. 4In Table 3.1, a small p-value for the intercept indicates that we can reject the null\nhypothesisthatβ0=0,andasmallp-valueforTVindicatesthatwecanrejectthenull\nhypothesis that β1 =0. Rejecting the latter null hypothesis allows us to conclude that\nthereisarelationshipbetweenTVandsales.Rejectingtheformerallowsustoconclude\nthatintheabsenceofTVexpenditure,salesarenon-zero.",
    "source": "book_islr",
    "section_title": "R2"
  },
  {
    "instruction": "Explain x in simple terms for a beginner in data science.",
    "input": "",
    "output": "FIGURE 3.13. Left: Observation 41 is a high leverage point, while 20 is not. The red line is the fit to all the data, and the blue line is the fit with observation\n41 removed.",
    "source": "book_islr",
    "section_title": "2X"
  },
  {
    "instruction": "Explain logistic regression part 3 in simple terms for a beginner in data science.",
    "input": "",
    "output": "=== Problem ===\nAs a simple example, we can use a logistic regression with one explanatory variable and two categories to answer the following question:\n\nA group of 20 students spends between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability of the student passing the exam? The reason for using logistic regression for this problem is that the values of the dependent variable, pass and fail, while represented by \"1\" and \"0\", are not cardinal numbers.",
    "source": "web_wikipedia",
    "section_title": "Logistic regression part 3"
  },
  {
    "instruction": "How is principal components analysis different from principal components analysis 505 in data science?",
    "input": "",
    "output": "principal components analysis: Principal components are discussed in Section 6.3.1 in the context of\nprincipal components regression. When faced with a large set of corre-\nlated variables, principal components allow us to summarize this set with\na smaller number of representative variables that collectively explain most\nof the variability in the original set. principal components analysis 505: the observations or visualization of the variables). It can also be used as a\ntool for data imputation — that is, for filling in missing values in a data\nmatrix.",
    "source": "book_islr",
    "section_title": "12.2 Principal Components Analysis vs 12.2 Principal Components Analysis 505"
  },
  {
    "instruction": "When or why would a data scientist use 12. unsupervised learning?",
    "input": "",
    "output": "Socks Computers\n01\n8\n6\n4\n2\n0\nSocks Computers\n2.1\n0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nSocks Computers\n0051\n0001\n005\n0\nFIGURE12.16.Aneclecticonlineretailersellstwoitems:socksandcomputers. Left:thenumberofpairsofsocks,andcomputers,purchasedbyeightonlineshop-\npers is displayed. Each shopper is shown in a different color. If inter-observation\ndissimilarities are computed using Euclidean distance on the raw variables, then\nthe number of socks purchased by an individual will drive the dissimilarities ob-\ntained, and the number of computers purchased will have little effect. This might\nbe undesirable, since (1) computers are more expensive than socks and so the\nonline retailer may be more interested in encouraging shoppers to buy computers\nthan socks, and (2) a large difference in the number of socks purchased by two\nshoppersmaybelessinformativeabouttheshoppers’overallshoppingpreferences\nthan a small difference in the number of computers purchased.",
    "source": "book_islr",
    "section_title": "534 12. Unsupervised Learning"
  },
  {
    "instruction": "Explain 1 in simple terms for a beginner in data science.",
    "input": "",
    "output": "xTΣ − 1µ k − 2 µT k Σ − 1µ k =xTΣ − 1µ l − 2 µT l Σ − 1µ l (4.25)\nfor k = l. (The logπ term from (4.24) has disappeared because each of\nk\n%\nthe three classes has the same number of training observations; i.e. π is\nk\nthe same for each class.) Note that there are three lines representing the\nBayes decision boundaries because there are three pairs of classes among\nthe three classes.",
    "source": "book_islr",
    "section_title": "1 1"
  },
  {
    "instruction": "Explain random forest part 5 in simple terms for a beginner in data science.",
    "input": "",
    "output": "As part of their construction, random forest predictors naturally lead to a dissimilarity measure among observations. One can analogously define dissimilarity between unlabeled data, by training a forest to distinguish original \"observed\" data from suitably generated synthetic data drawn from a reference distribution. A random forest dissimilarity is attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations.",
    "source": "web_wikipedia",
    "section_title": "Random forest part 5"
  },
  {
    "instruction": "What is 2 in data science?",
    "input": "",
    "output": "proportionofwomenandmeninthecity. Theparameter estimation problemforamixture\nmodel is the problem: given access to samples from the overall density p (e.g., heights of\npeople in the city, but without being told whether the person with that height is male\nor female), reconstruct the parameters for the distribution (e.g., good approximations to\nthe means and variances of p and p , as well as the mixture weights).",
    "source": "book_fods",
    "section_title": "1 2"
  },
  {
    "instruction": "What is best rank-k approximations in data science?",
    "input": "",
    "output": "Let A be an n × d matrix and think of the rows of A as n points in d-dimensional\nspace. Let\nr\n(cid:88)\nA = σ u vT\ni i i\ni=1\nbe the SVD of A. For k ∈ {1,2,...,r}, let\nk\n(cid:88)\nA = σ u vT\nk i i i\ni=1\nbe the sum truncated after k terms.",
    "source": "book_fods",
    "section_title": "3.5 Best Rank-k Approximations"
  },
  {
    "instruction": "How is 1− g different from 1− g(x) in data science?",
    "input": "",
    "output": "1− g: g(cid:48) = x. 2δ 1−g\n290\nPhase transition for nonfinite components\nThe generating function g(x) contains information about the finite components of the\ngraph. 1− g(x): g(cid:48)(x) = x . 2δ1−g(x)\nIf δ is greater than some δ , then g(1) (cid:54)= 1.",
    "source": "book_fods",
    "section_title": "1 1− g vs 1 1− g(x)"
  },
  {
    "instruction": "When or why would a data scientist use 500 1000 1500 2000 2500?",
    "input": "",
    "output": "00006\n00004\n00002\n0\nBalance\nemocnI\nNo Yes\n0052\n0002\n0051\n0001\n005\n0\nDefault\necnalaB\nNo Yes\n00006\n00004\n00002\n0\nDefault\nemocnI\nFIGURE 4.1. The Default data set. Left: The annual incomes and monthly\ncredit card balances of a number of individuals. The individuals who defaulted on\ntheircreditcardpaymentsareshowninorange,andthosewhodidnotareshown\nin blue. Center: Boxplots of balance as a function of default status.",
    "source": "book_islr",
    "section_title": "0 500 1000 1500 2000 2500"
  },
  {
    "instruction": "How is multilayer neural networks different from multilayer neural networks 403 in data science?",
    "input": "",
    "output": "multilayer neural networks: Modern neural networks typically have more than one hidden layer, and\noften many units per layer. In theory a single hidden layer with a large\nnumber of units has the ability to approximate most functions. multilayer neural networks 403: FIGURE 10.3. Examples of handwritten digits from the MNIST corpus.",
    "source": "book_islr",
    "section_title": "10.2 Multilayer Neural Networks vs 10.2 Multilayer Neural Networks 403"
  },
  {
    "instruction": "How is 30 40 50 60 70 80 different from 1 1 i k k i in data science?",
    "input": "",
    "output": "30 40 50 60 70 80: 02.0\n51.0\n01.0\n50.0\n00.0\n| ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| |\n||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||||\nAge\n)egA|052>egaW(rP\nFIGURE7.2.TheWagedata.Left:Thesolidcurvedisplaysthefittedvaluefrom\na least squares regression of wage (in thousands of dollars) using step functions\nof age. The dashed curves indicate an estimated 95% confidence interval. 1 1 i k k i: ···\ninordertopredicttheprobabilitythatanindividualisahighearneronthe\nbasisofage.Theright-handpanelofFigure7.2displaysthefittedposterior\nprobabilities obtained using this approach. Unfortunately, unless there are natural breakpoints in the predictors,\npiecewise-constant functions can miss the action.",
    "source": "book_islr",
    "section_title": "20 30 40 50 60 70 80 vs 0 1 1 i K K i"
  },
  {
    "instruction": "Explain exercises 287 in simple terms for a beginner in data science.",
    "input": "",
    "output": "(g) Comment on the results obtained. How accurately can we pre-\ndict the number of college applications received? Is there much\ndifference among the test errors resulting from these five ap-\nproaches?",
    "source": "book_islr",
    "section_title": "6.6 Exercises 287"
  },
  {
    "instruction": "How is lab: linear models and regularization methods 279 different from y, in data science?",
    "input": "",
    "output": "lab: linear models and regularization methods 279: In[41]: outer_valid = skm.ShuffleSplit(n_splits=1,\ntest_size=0.25,\nrandom_state=1)\ninner_cv = skm.KFold(n_splits=5,\nshuffle=True,\nrandom_state=2)\nridgeCV = skl.ElasticNetCV(alphas=lambdas,\nl1_ratio=0,\ncv=inner_cv)\npipeCV = Pipeline(steps=[('scaler', scaler),\n('ridge', ridgeCV)]);\nIn[42]: results = skm.cross_validate(pipeCV, y,: cv=outer_valid,\nscoring='neg_mean_squared_error')\n-results['test_score']\nOut[42]:array([132393.84])\nThe Lasso\nWe saw that ridge regression with a wise choice of λ can outperform least\nsquares as well as the null model on the Hitters data set. We now ask\nwhether the lasso can yield either a more accurate or a more interpretable\nmodel than ridge regression.",
    "source": "book_islr",
    "section_title": "6.5 Lab: Linear Models and Regularization Methods 279 vs Y,"
  },
  {
    "instruction": "What is 0 in data science?",
    "input": "",
    "output": "We’dliketoselectthemodelforwhichthisquantityisassmallaspossible. How can we go about trying to select a method that minimizes the test\nMSE? In some settings, we may have a test data set available—that is,\nwe may have access to a set of observations that were not used to train\nthe statistical learning method.",
    "source": "book_islr",
    "section_title": "0 0"
  },
  {
    "instruction": "When or why would a data scientist use n?",
    "input": "",
    "output": "Let α = k and f(α) = 1−e−cαlnn −400α. By differentiation f(cid:48)(cid:48)(α) ≤ 0, so f is concave\nn\nand the minimum value of f over the interval [0,1/1000] is attained at one of the end\npoints. It is easy to check that both f(0) and f(1/1000) are greater than or equal to\nzero for sufficiently large n. Thus, f is nonnegative throughout the interval proving that\nE(|S |) ≥ 200|S |. The lemma follows from Chernoff bounds. i+1 i\nTheorem 8.13 For p ≥ clnn/n, where c is a sufficiently large constant, almost surely,\nG(n,p) has diameter O(lnn).",
    "source": "book_fods",
    "section_title": "2 n"
  },
  {
    "instruction": "How is m different from 6. linear model selection and regularization in data science?",
    "input": "",
    "output": "m: best size k is chosen, we find the best model of that size on the full data\nset. Inthepast,performingcross-validationwascomputationallyprohibitive\nfor many problems with large p and/or large n, and so AIC, BIC, C ,\np\nand adjusted R2 were more attractive approaches for choosing among a\nset of models. 6. linear model selection and regularization: repeatedthe validationsetapproachusing adifferentsplit ofthe data into\na training set and a validation set, or if we repeated cross-validation using\na different set of cross-validation folds, then the precise model with the\nlowest estimated test error would surely change. In this setting, we can\nselect a model using the one-standard-error rule.",
    "source": "book_islr",
    "section_title": "M vs 240 6. Linear Model Selection and Regularization"
  },
  {
    "instruction": "What is a representation cannot be sparse in both time and frequency in data science?",
    "input": "",
    "output": "Domains\nThere is an uncertainty principle that states that a time signal cannot be sparse in\nboth the time domain and the frequency domain. If the signal is of length n, then the\nproduct of the number of nonzero coordinates in the time domain and the number of\nnonzero coordinates in the frequency domain must be at least n. This is the mathemati-\ncal version of Heisenberg’s uncertainty principle. Before proving the uncertainty principle\nwe first prove a technical lemma.",
    "source": "book_fods",
    "section_title": "10.4.2 A Representation Cannot be Sparse in Both Time and Frequency"
  },
  {
    "instruction": "What is 2 n in data science?",
    "input": "",
    "output": "i with probability proportional to the value of a . When we see an element, we do not\ni\nknow the probability with which to select it since the normalizing constant depends on\nall of the elements including those we have not yet seen. However, the following method\nworks.",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "What is data science part 1 in data science?",
    "input": "",
    "output": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data. Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.",
    "source": "web_wikipedia",
    "section_title": "Data science part 1"
  },
  {
    "instruction": "When or why would a data scientist use machine learning part 3?",
    "input": "",
    "output": "=== Artificial intelligence ===\n\nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning.",
    "source": "web_wikipedia",
    "section_title": "Machine learning part 3"
  },
  {
    "instruction": "When or why would a data scientist use m?",
    "input": "",
    "output": "x z φ . (12.5)\nij im jm\n≈\nm=1\n0\nWe can state this more formally by writing down an optimization prob-\nlem.SupposethedatamatrixXiscolumn-centered.Outofallapproxima-\ntions of the form x M a b , we could ask for the one with the\nij ≈ m=1 im jm\nsmallest residual sum of squares:\n)\np n M 2\nminimize x a b . (12.6)\nA ∈ Rn × M,B ∈ Rp × M 0 j=1 0 i=1 > ij − m 0 =1 im jm ?  \nHere, A is an n M matrix whose (i,m) element is a , and B is a p M\n im \n× ×\nelement whose (j,m) element is b . jm\nIt can be shown that for any value of M, the columns of the matrices\nAˆ and Bˆ that solve (12.6) are in fact the first M principal components\nscore and loading vectors.",
    "source": "book_islr",
    "section_title": "M"
  },
  {
    "instruction": "How is n n 2 2 n 5 1 different from 2 3 in data science?",
    "input": "",
    "output": "n n 2 2 n 5 1: all n.\nMeans and standard deviations of sequences\nGenerating functions are useful for calculating the mean and standard deviation of a\nsequence. Let z be an integral valued random variable where p is the probability that\ni\n∞ ∞\nz equals i. 2 3: ∞\nd (cid:88)\nx p(x) = ip xi. i\ndx\ni=0\nThus, the expected value of the random variable z is m = xp(cid:48)(x)| = p(cid:48)(1).",
    "source": "book_fods",
    "section_title": "2 n n 2 2 n 5 1 vs 1 2 3"
  },
  {
    "instruction": "Explain an uncertainty principle in simple terms for a beginner in data science.",
    "input": "",
    "output": "Given a function x(t), one can represent the function by the composition of sinusoidal\nfunctions. Basically one is representing the time function by its frequency components. The transformation from the time representation of a function to it frequency represen-\ntation is accomplished by a Fourier transform.",
    "source": "book_fods",
    "section_title": "10.4 An Uncertainty Principle"
  },
  {
    "instruction": "What is sparse vector in some coordinate basis in data science?",
    "input": "",
    "output": "Consider Ax = b where A is a square n×n matrix. The vectors x and b can be con-\nsidered as two representations of the same quantity. For example, x might be a discrete\ntime sequence, b the frequency spectrum of x, and the matrix A the Fourier transform.",
    "source": "book_fods",
    "section_title": "10.4.1 Sparse Vector in Some Coordinate Basis"
  },
  {
    "instruction": "When or why would a data scientist use threshold for o(lnn) diameter?",
    "input": "",
    "output": "We now show that within a constant factor of the threshold for graph connectivity, not\nonly is the graph connected, but its diameter is O(lnn). That is, if p > clnn for sufficiently\nn\nlarge constant c, the diameter of G(n,p) is O(lnn) with high probability. Consider a particular vertex v. Let S be the set of vertices at distance i from v. We\ni\nargue that as i increases, with high probability |S |+|S |+···+|S | grows by at least a",
    "source": "book_fods",
    "section_title": "8.4.3 Threshold for O(lnn) Diameter"
  },
  {
    "instruction": "When or why would a data scientist use 11. survival analysis and censored data?",
    "input": "",
    "output": "Regardless of which test we use, we see that there is no clear evidence for\na difference in survival between males and females. As we learned in this\nchapter, the score test from the Cox model is exactly equal to the log rank\ntest statistic! Nowwefitamodelthatmakesuseofadditionalpredictors.Wefirstnote\nthatoneofourdiagnosisvaluesismissing,hencewedropthatobservation\nbefore continuing. In[12]: cleaned = BrainCancer.dropna()\nall_MS = MS(cleaned.columns, intercept=False)\nall_df = all_MS.fit_transform(cleaned)\nfit_all = coxph().fit(all_df,\n'time',\n'status')\nfit_all.summary[['coef', 'se(coef)', 'p']]\nOut[12]: coef se(coef) p\ncovariate\nsex[Male] 0.183748 0.360358 0.610119\ndiagnosis[LG glioma] -1.239541 0.579557 0.032454\ndiagnosis[Meningioma] -2.154566 0.450524 0.000002\ndiagnosis[Other] -1.268870 0.617672 0.039949\nloc[Supratentorial] 0.441195 0.703669 0.530664\nki -0.054955 0.018314 0.002693\ngtv 0.034293 0.022333 0.124660\nstereo[SRT] 0.177778 0.601578 0.767597\nThe diagnosis variable has been coded so that the baseline corresponds to\nHG glioma. The results indicate that the risk associated with HG glioma\nis more than eight times (i.e.",
    "source": "book_islr",
    "section_title": "492 11. Survival Analysis and Censored Data"
  },
  {
    "instruction": "What is n, in data science?",
    "input": "",
    "output": "replace=True)\nD = pd.DataFrame({'Operators': Operators,\n'Center': pd.Categorical(Center),\n'Time': pd.Categorical(Time)})\nWe then build a model matrix (omitting the intercept)\nIn[22]: model = MS(['Operators',\n'Center',\n'Time'],\nintercept=False)\nX = model.fit_transform(D)\nIt is worthwhile to take a peek at the model matrix X, so that we can be\nsure that we understand how the variables have been coded. By default,\nthe levels of categorical variables are sorted and, as usual, the first column\nof the one-hot encoding of the variable is dropped. In[23]: X[:5]\nOut[23]: Operators Center[B] Center[C] Time[Even.]",
    "source": "book_islr",
    "section_title": "N,"
  },
  {
    "instruction": "What is 2. statistical learning in data science?",
    "input": "",
    "output": "thetruef givenbytheblackcurve.Theorange,blueandgreencurvesillus-\ntratethreepossibleestimatesforf obtainedusingmethodswithincreasing\nlevelsofflexibility.Theorangelineisthelinearregressionfit,whichisrela-\ntivelyinflexible.Theblueandgreencurveswereproducedusingsmoothing\nsplines, discussed in Chapter 7, with different levels of smoothness. It is\nsmoothing\nclear that as the level of flexibility increases, the curves fit the observed\nspline\ndata more closely. The green curve is the most flexible and matches the\ndata very well; however, we observe that it fits the true f (shown in black)\npoorly because it is too wiggly.",
    "source": "book_islr",
    "section_title": "30 2. Statistical Learning"
  },
  {
    "instruction": "Explain lstm: 1-2 [10, 500, 32] [10, 500, 32] 8,448 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Linear: 1-3 [10, 32] [10, 1] 33\n====================================================================\nTotal params: 328,577\nTrainable params: 328,577\nThe 10,003 is suppressed in the summary, but we see it in the parameter\ncount, since 10,003 32=320,096. ×\nIn[88]: lstm_module = SimpleModule.binary_classification(lstm_model)\nlstm_logger = CSVLogger('logs', name='IMDB_LSTM')\nIn[89]: lstm_trainer = Trainer(deterministic=True,\nmax_epochs=20,\nlogger=lstm_logger,\ncallbacks=[ErrorTracker()])\nlstm_trainer.fit(lstm_module,\ndatamodule=imdb_seq_dm)\nThe rest is now similar to other networks we have fit. We track the test\nperformance as the network is fit, and see that it attains 85% accuracy.",
    "source": "book_islr",
    "section_title": "LSTM: 1-2 [10, 500, 32] [10, 500, 32] 8,448"
  },
  {
    "instruction": "Explain l s l |s| in simple terms for a beginner in data science.",
    "input": "",
    "output": "L(T) copies of the above algorithm (one for each value of L) and then combining\nthese algorithms using the experts algorithm (in this case, none of them will be\nsleeping). Exercise 5.9 Kernels; (Section 5.3) Prove Theorem 5.2. Exercise 5.10 What is the VC-dimension of right corners with axis aligned edges that\nare oriented with one edge going to the right and the other edge going up?",
    "source": "book_fods",
    "section_title": "L S L |S|"
  },
  {
    "instruction": "How is the hazard function different from regression models with a survival response 477 in data science?",
    "input": "",
    "output": "the hazard function: The hazard function or hazard rate — also known as the force of mortality\nhazard\n— is formally defined as\nfunction\nPr(t<T t+∆tT >t)\nh(t)= lim ≤ | , (11.9)\n∆t 0 ∆t\n→\n6Alternatively, we can estimate the p-value via permutations, using ideas that will\nbe presented in Section 13.5. The permutation distribution is obtained by randomly\nswappingthelabelsfortheobservationsinthetwogroups. regression models with a survival response 477: whereT isthe(unobserved)survivaltime.Itisthedeathrateintheinstant\nafter time t, given survival past that time.7 In (11.9), we take the limit as\n∆t approaches zero, so we can think of ∆t as being an extremely tiny\nnumber. Thus, more informally, (11.9) implies that\nPr(t<T t+∆tT >t)\nh(t) ≤ |\n≈ ∆t\nfor some arbitrarily small ∆t.",
    "source": "book_islr",
    "section_title": "11.5.1 The Hazard Function vs 11.5 Regression Models With a Survival Response 477"
  },
  {
    "instruction": "What is x in data science?",
    "input": "",
    "output": "1\n-1.06 0.21\n-1.80 0.63\nFIGURE 8.14. Left: A partition of the predictor space corresponding to Exer-\ncise 4a. Right: A tree corresponding to Exercise 4b.",
    "source": "book_islr",
    "section_title": "X"
  },
  {
    "instruction": "What is 2 in data science?",
    "input": "",
    "output": "on whether the majority of leaves have value +1 or -1. For β < β the probability that\n0\nthe root has value +1 is 1/ independent of the values at the leaves of the tree. 2\n352\nConsider a height one tree of degree d. If i of the leaves have spin +1 and d−i have\nspin -1, then the probability of the root having spin +1 is proportional to\neiβ−(d−i)β = e(2i−d)β.",
    "source": "book_fods",
    "section_title": "0 2"
  },
  {
    "instruction": "What is k in data science?",
    "input": "",
    "output": "whereξ istheknot.Onecanshowthataddingatermoftheformβ h(x,ξ)\n4\nto the model (7.8) for a cubic polynomial will lead to a discontinuity in\nonly the third derivative at ξ; the function will remain continuous, with\ncontinuous first and second derivatives, at each of the knots. Inotherwords,inordertofitacubicsplinetoadatasetwithK knots,we\nperformleast squares regression with aninterceptand 3+K predictors,of\nthe form X,X2,X3,h(X,ξ ),h(X,ξ ),...,h(X,ξ ), where ξ ,...,ξ are",
    "source": "book_islr",
    "section_title": "K"
  },
  {
    "instruction": "When or why would a data scientist use d?",
    "input": "",
    "output": "Conversion procedure 2: Controlled Testing. A second natural approach to us-\ning an online learning algorithm A in the distributional setting is to just run a series of\ncontrolled tests. Specifically, suppose that the initial hypothesis produced by algorithm\nA is h . Define δ = δ/(i + 2)2 so we have (cid:80)∞ δ = (π2 − 1)δ ≤ δ. We draw a set of",
    "source": "book_fods",
    "section_title": "D"
  },
  {
    "instruction": "When or why would a data scientist use 10. deep learning?",
    "input": "",
    "output": "In[107]: nyse_trainer = Trainer(deterministic=True,\nmax_epochs=200,\ncallbacks=[ErrorTracker()])\nnyse_trainer.fit(nyse_module,\ndatamodule=nyse_dm)\nnyse_trainer.test(nyse_module,\ndatamodule=nyse_dm)\nOut[107]:[{'test_loss': 0.6141, 'test_r2': 0.4172}]\nWe could also fit a model without the nn.RNN() layer by just using a\nnn.Flatten() layer instead. This would be a nonlinear AR model. If in\naddition we excluded the hidden layer, this would be equivalent to our\nearlier linear AR model. InsteadwewillfitanonlinearARmodelusingthefeaturesetX_daythat\nincludes the day_of_week indicators. To do so, we must first create our test\nand training datasets and a corresponding data module.",
    "source": "book_islr",
    "section_title": "464 10. Deep Learning"
  },
  {
    "instruction": "When or why would a data scientist use |s|+1?",
    "input": "",
    "output": "an estimate of |S|. Converting the intuition into an algorithm via hashing\nIn general, the set S might not have been chosen uniformly at random. If the el-\nements of S were obtained by selecting the |S| smallest elements of {1,2,...,m}, the\nabove technique would give a very bad answer. However, we can convert our intuition\ninto an algorithm that works well with high probability on every sequence via hashing. Specifically, we will use a hash function h where\nh : {1,2,...,m} → {0,1,2,...,M −1},\nand then instead of keeping track of the minimum element a ∈ S, we will keep track of\ni\nthe minimum hash value.",
    "source": "book_fods",
    "section_title": "|S|+1"
  },
  {
    "instruction": "How is 1 p different from 1000 1500 2000 in data science?",
    "input": "",
    "output": "1 p: Table 4.3 shows the coefficient estimates for a logistic regression model\nthat uses balance, income (in thousands of dollars), and student status to\npredict probability of default. There is a surprising result here. 1000 1500 2000: 8.0\n6.0\n4.0\n2.0\n0.0\nCredit Card Balance\netaR\ntluafeD\nNo Yes\n0052\n0002\n0051\n0001\n005\n0\nStudent Status\necnalaB\ndraC\ntiderC\nFIGURE 4.3. Confounding in the Default data.",
    "source": "book_islr",
    "section_title": "0 1 p vs 500 1000 1500 2000"
  },
  {
    "instruction": "How is 1 4 8 different from s in data science?",
    "input": "",
    "output": "1 4 8: whether an email message is spam or not, and our features correspond to the presence\nor absence of different possible indicators of spam-ness, then this would correspond to\nthe belief that there is some subset of these indicators such that every spam email has at\nleast one of them and every non-spam email has none of them. Formally, let H denote\nthe class of disjunctions, and notice that |H| = 2d. s: disjunction. Proof: Suppose target concept c∗ is a disjunction.",
    "source": "book_fods",
    "section_title": "8 1 4 8 vs S"
  },
  {
    "instruction": "What is logistic regression part 1 in data science?",
    "input": "",
    "output": "In statistics, a logistic model (or logit model) is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) estimates the parameters of a logistic model (the coefficients in the linear or non linear combinations). In binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \"0\" and \"1\", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value).",
    "source": "web_wikipedia",
    "section_title": "Logistic regression part 1"
  },
  {
    "instruction": "When or why would a data scientist use 3. linear regression?",
    "input": "",
    "output": "20 40 60 80 100 5 10 15 20 2000 8000 14000\nBalance\n001\n08\n06\n04\n02\n0051\n005\n0\nAge\nCards\n02\n51\n01\n5\n8\n6\n4\n2\nEducation\nIncome\n00041\n0008\n0002\n051\n001\n05\nLimit\n0 500 1500 2 4 6 8 50 100 150 200 600 1000\n0001\n006\n002\nRating\nFIGURE 3.6. The Credit data set contains information about balance, age,\ncards, education, income, limit, and rating for a number of potential cus-\ntomers. notice that the p-value for the dummy variable is very high. This indicates\nthat there is no statistical evidence of a difference in average credit card\nbalance based on house ownership. The decision to code owners as 1 and non-owners as 0 in (3.27) is ar-\nbitrary, and has no effect on the regression fit, but does alter the inter-\npretation of the coefficients.",
    "source": "book_islr",
    "section_title": "92 3. Linear Regression"
  },
  {
    "instruction": "How is lab: linear models and regularization methods 277 different from 6. linear model selection and regularization in data science?",
    "input": "",
    "output": "lab: linear models and regularization methods 277: Alternatively, we can use 5-fold cross-validation. In[33]: grid = skm.GridSearchCV(pipe,\nparam_grid,\ncv=kfold,\nscoring='neg_mean_squared_error')\ngrid.fit(X, Y)\ngrid.best_params_['ridge__alpha']\ngrid.best_estimator_\nRecall we set up the kfold object for 5-fold cross-validation on page 271. 6. linear model selection and regularization: ('ridge', ridgeCV)])\npipeCV.fit(X, Y)\nLet’s produce a plot again of the cross-validation error to see that it is\nsimilar to using skm.GridSearchCV. In[38]: tuned_ridge = pipeCV.named_steps['ridge']\nridgeCV_fig, ax = subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\ntuned_ridge.mse_path_.mean(1),\nyerr=tuned_ridge.mse_path_.std(1) / np.sqrt(K))\nax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')\nax.set_ylim([50000,250000])\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20);\nWe see that the value of λ that results in the smallest cross-validation\nerror is 1.19e-02, available as the value tuned_ridge.alpha_.",
    "source": "book_islr",
    "section_title": "6.5 Lab: Linear Models and Regularization Methods 277 vs 278 6. Linear Model Selection and Regularization"
  },
  {
    "instruction": "What is missing values and matrix completion 515 in data science?",
    "input": "",
    "output": "proach,andisreflectiveofthefactthatPCAisgenerallyusedasatoolfor\nexploratory data analysis. On the other hand, if we compute principal components for use in a\nsupervised analysis, such as the principal components regression presented\ninSection6.3.1,thenthereisasimpleandobjectivewaytodeterminehow\nmany principal components to use: we can treat the number of principal\ncomponentscorevectorstobeusedintheregressionasatuningparameter\nto be selected via cross-validation or a related approach. The comparative\nsimplicityofselectingthenumberofprincipalcomponentsforasupervised\nanalysis is one manifestation of the fact that supervised analyses tend to\nbe more clearly defined and more objectively evaluated than unsupervised\nanalyses.",
    "source": "book_islr",
    "section_title": "12.3 Missing Values and Matrix Completion 515"
  },
  {
    "instruction": "Explain 20 30 40 50 60 70 in simple terms for a beginner in data science.",
    "input": "",
    "output": "53\n03\n52\n02\n51\n01\n5\n0\nPopulation\ngnidnepS\ndA\nFIGURE6.14. Thepopulationsize(pop)andadspending(ad)for100different\ncitiesareshownaspurplecircles.Thegreensolidlineindicatesthefirstprincipal\ncomponent, and the blue dashed line indicates the second principal component. simpler problem of estimating the M +1 coefficients θ ,θ ,...,θ , where",
    "source": "book_islr",
    "section_title": "10 20 30 40 50 60 70"
  },
  {
    "instruction": "Explain j=1 ij j j=1 ij j in simple terms for a beginner in data science.",
    "input": "",
    "output": "iscalledtherelativeriskfor1thefeature2vectorx =(x ,...,x1 )T,relativ2e\n) i i1 i)p\nto that for the feature vector x =(0,...,0)T.\ni\nWhat does it mean that the baseline hazard function h (t) in (11.14) is\n0\nunspecified? Basically, we make no assumptions about its functional form. We allow the instantaneous probability of death at time t, given that one\nhas survived at least until time t, to take any form.",
    "source": "book_islr",
    "section_title": "0 j=1 ij j j=1 ij j"
  },
  {
    "instruction": "What is 50 100 150 in data science?",
    "input": "",
    "output": "0041\n0001\n006\n002\nIncome\necnalaB\nstudent\nnon−student\nFIGURE 3.7. For the Credit data, the least squares lines are shown for pre-\ndiction of balance from income for students and non-students. Left: The model\n(3.34) was fit.",
    "source": "book_islr",
    "section_title": "0 50 100 150"
  },
  {
    "instruction": "Explain machine learning part 9 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data into a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.",
    "source": "web_wikipedia",
    "section_title": "Machine learning part 9"
  },
  {
    "instruction": "What is 4 1 blue in data science?",
    "input": "",
    "output": "Sketch the observations. (b) Sketchtheoptimalseparatinghyperplane,andprovidetheequa-\ntion for this hyperplane (of the form (9.1)). (c) Describetheclassificationruleforthemaximalmarginclassifier.",
    "source": "book_islr",
    "section_title": "7 4 1 Blue"
  },
  {
    "instruction": "Explain kernel methods in simple terms for a beginner in data science.",
    "input": "",
    "output": "Kernel methods combine aspects of both center-based and density-based clustering. In center-based approaches like k-means or k-center, once the cluster centers are fixed, the\n228\nVoronoi diagram of the cluster centers determines which cluster each data point belongs\nto. This implies that clusters are pairwise linearly separable.",
    "source": "book_fods",
    "section_title": "7.8 Kernel Methods"
  },
  {
    "instruction": "How is 20 40 60 80 different from exercises 501 in data science?",
    "input": "",
    "output": "20 40 60 80: 0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nTime in Days\nlavivruS\nfo\nytilibaborP\ndetamitsE\nFIGURE 11.9. A Kaplan-Meier survival curve used in Exercise 4. exercises 501: (b) Sketch the Kaplan-Meier survival curve corresponding to this\ndataset. (Youdonotneedtouseanysoftwaretodothis—you\ncan sketch it by hand using the results obtained in (a).)",
    "source": "book_islr",
    "section_title": "0 20 40 60 80 vs 11.9 Exercises 501"
  },
  {
    "instruction": "How is 1 03 3 different from (l) in data science?",
    "input": "",
    "output": "1 03 3: However,asdiscussedinprevioussections,thisproceduredoesnotaccount\nfor the fact that we have tested multiple hypotheses, and therefore it will\nleadtoaFWERgreaterthan0.05.IfweinsteadwishtocontroltheFWER\natlevel0.05,then,usingaBonferronicorrection,wemustcontroltheType\nI error for each individual manager at level α/m = 0.05/5 = 0.01. Conse-\nquently, we will reject the null hypothesis only for the first manager, since\nthe p-values for all other managers exceed 0.01. (l): inStep5—actuallydependsonthevaluesofall mofthep-values. (Seethe\ndefinition of L in (13.7).)",
    "source": "book_islr",
    "section_title": "01 1 03 3 vs (L)"
  },
  {
    "instruction": "How is φ(s,t) = . different from s⊆w in data science?",
    "input": "",
    "output": "φ(s,t) = .: Total number of edges incident to S in G\nΦ(S,T) measures the relative strength of similarities between S and T. Let d(i) be the\n(cid:80)\ndegree of vertex i and for a subset S of vertices, let d(S) = d(i). Let m be the total\ni∈S\nnumber of edges in the graph. s⊆w: hardproblem. Sothetheoremcannotbeimplementedrightaway.",
    "source": "book_fods",
    "section_title": "Φ(S,T) = . vs S⊆W"
  },
  {
    "instruction": "How is x different from 12. unsupervised learning in data science?",
    "input": "",
    "output": "x: 2\n2\nFIGURE 12.13. An illustration of the first few steps of the hierarchical\nclustering algorithm, using the data from Figure 12.12, with complete linkage\nand Euclidean distance. 12. unsupervised learning: Average Linkage Complete Linkage Single Linkage\nFIGURE 12.14. Average, complete, and single linkage applied to an example\ndata set.",
    "source": "book_islr",
    "section_title": "X vs 532 12. Unsupervised Learning"
  },
  {
    "instruction": "When or why would a data scientist use 1?",
    "input": "",
    "output": "resulting line is as close as possible to the n = 200 data points. There are\na number of ways of measuring closeness. However, by far the most com-\nmon approach involves minimizing the least squares criterion, and we take\nleastsquares\nthatapproachinthischapter.Alternativeapproacheswillbeconsideredin\nChapter 6. Let yˆ =βˆ +βˆ x be the prediction for Y based on the ith value of X.\ni 0 1 i\nThene =y yˆ representstheithresidual—thisisthedifferencebetween\ni i i\n− residual\ntheithobservedresponsevalueandtheithresponsevaluethatispredicted\nby our linear model. We define the residual sum of squares (RSS) as\nresidualsum\nRSS=e2+e2+ +e2, ofsquares",
    "source": "book_islr",
    "section_title": "0 1"
  },
  {
    "instruction": "Describe the typical steps involved in 2 in a data science workflow.",
    "input": "",
    "output": "There are taller women and shorter men, so even if one solved the parameter estima-\ntion problem for heights perfectly, given a data point, one couldn’t necessarily tell which\npopulation it came from. That is, given a height, one couldn’t necessarily tell if it came\nfrom a man or a woman. In this section, we will look at a problem that is in some ways\neasier and some ways harder than this problem of heights. It will be harder in that we\nwill be interested in a mixture of two Gaussians in high-dimensions as opposed to the\nd = 1 case of heights. But it will be easier in that we will assume the means are quite\nwell-separated compared to the variances. Specifically, our focus will be on a mixture of\ntwo spherical unit-variance Gaussians whose means are separated by a distance Ω(d1/4).",
    "source": "book_fods",
    "section_title": "1 2"
  },
  {
    "instruction": "How is a b c different from 2 n2 i in data science?",
    "input": "",
    "output": "a b c: (b)\nFigure 4.1: (a) A directed graph with vertices having no out out edges and a strongly\nconnected component A with no in edges. (b) A directed graph with three strongly connected components. 2 n2 i: figurations. The energy of a configuration is a function f(v) of the configuration, not of\nanysinglespin.",
    "source": "book_fods",
    "section_title": "A B C vs 1 2 n2 i"
  },
  {
    "instruction": "How is graphs with a single cycle different from a b c a b c a b c in data science?",
    "input": "",
    "output": "graphs with a single cycle: The message passing algorithm gives the correct answers on trees and on certain other\ngraphs. One such situation is graphs with a single cycle which we treat here. a b c a b c a b c: (b) Segment of unrolled graph\nFigure 9.6: Unwrapping a graph with a single cycle\nloopy version will eventually converge to, assuming convergence, are the same messages\nthat occur in the unwrapped version provided that the nodes are sufficiently far in from\nthe ends. The beliefs in the unwrapped version are correct for the unwrapped graph since\nit is a tree.",
    "source": "book_fods",
    "section_title": "9.17 Graphs with a Single Cycle vs A B C A B C A B C"
  },
  {
    "instruction": "Explain assessing model accuracy 27 in simple terms for a beginner in data science.",
    "input": "",
    "output": "measurement. For the remaining n m observations, we have predictor\n−\nmeasurements but no response measurement. Such a scenario can arise if\nthe predictors can be measured relatively cheaply but the corresponding\nresponses are much more expensive to collect.",
    "source": "book_islr",
    "section_title": "2.2 Assessing Model Accuracy 27"
  },
  {
    "instruction": "What is 4. classification in data science?",
    "input": "",
    "output": "function works similarly to the other model-fitting functions that we have\nencountered thus far. As is the case for LDA and QDA, we fit the classifier using the fit\nmethod.Newpredictionsareformedusingthepredictmethodoftheobject\nreturned by fit(). In[47]: knn1 = KNeighborsClassifier(n_neighbors=1)\nknn1.fit(X_train, L_train)\nknn1_pred = knn1.predict(X_test)\nconfusion_table(knn1_pred, L_test)\nOut[47]: Truth Down Up\nPredicted\nDown 43 58\nUp 68 83\nThe results using K =1 are not very good, since only 50% of the observa-\ntionsarecorrectlypredicted.Ofcourse,itmaybethatK =1resultsinan\noverly-flexible fit to the data.",
    "source": "book_islr",
    "section_title": "184 4. Classification"
  },
  {
    "instruction": "Explain x 2x in simple terms for a beginner in data science.",
    "input": "",
    "output": "X1≤t1\nX2≤t2 X1≤t3\nX2≤t4\nFIGURE 8.3.TopLeft:Apartitionoftwo-dimensionalfeaturespacethatcould\nnot result from recursive binary splitting. Top Right: The output of recursive\nbinarysplittingonatwo-dimensionalexample.BottomLeft:Atreecorresponding\nto the partition in the top right panel. Bottom Right: A perspective plot of the\nprediction surface corresponding to that tree.",
    "source": "book_islr",
    "section_title": "2X 2X"
  },
  {
    "instruction": "When or why would a data scientist use 3. linear regression?",
    "input": "",
    "output": "ii. Which predictors appear to have a statistically significant\nrelationship to the response? iii. What does the coefficient for the year variable suggest? (d) Produce some of diagnostic plots of the linear regression fit as\ndescribedinthelab.Commentonanyproblemsyouseewiththe\nfit.",
    "source": "book_islr",
    "section_title": "130 3. Linear Regression"
  },
  {
    "instruction": "What is the support vector machine in data science?",
    "input": "",
    "output": "The support vector machine (SVM) is an extension of the support vector\nsupport\nclassifier that results from enlarging the feature space in a specific way,\nvector\nusing kernels. We will now discuss this extension, the details of which are machine\nsomewhat complex and beyond the scope of this book. However, the main kernel\nideaisdescribedinSection9.3.1:wemaywanttoenlargeourfeaturespace\nin order to accommodate a non-linear boundary between the classes.",
    "source": "book_islr",
    "section_title": "9.3.2 The Support Vector Machine"
  },
  {
    "instruction": "How is e different from 6 in data science?",
    "input": "",
    "output": "e: (cid:0)\neλxi\n(cid:1)\ni=1 i=1\nn n\n(cid:89)(cid:0) (cid:1) (cid:89)(cid:0) (cid:1)\n= eλp+1−p = p(eλ −1)+1 . i=1 i=1\nUsing the inequality 1+x < ex with x = p(eλ −1) yields\nn\nE (cid:0) eλs (cid:1) < (cid:89) ep(eλ−1). 6: For small δ the probability drops exponentially with δ2. When δ is large another simplification is possible.",
    "source": "book_fods",
    "section_title": "E vs 2 6"
  },
  {
    "instruction": "Describe the typical steps involved in 13. multiple testing in a data science workflow.",
    "input": "",
    "output": "T_vals[j] = ttest_ind(D2[col],\nD4[col],\nequal_var=True).statistic\nD_ = np.hstack([D2[col], D4[col]])\nD_null = D_.copy()\nfor b in range(B):\nrng.shuffle(D_null)\nttest_ = ttest_ind(D_null[:n_],\nD_null[n_:],\nequal_var=True)\nTnull_vals[j,b] = ttest_.statistic\nNext, we compute the number of rejected null hypotheses R, the esti-\nmated number of false positives V, and the estimated FDR, for a range\nof threshold values c in Algorithm 13.4. The threshold values are chosen\nusing the absolute values of the teWst statistics from the 100 genes. In[28]: cutoffs = np.sort(np.abs(T_vals))\nFDRs, Rs, Vs = np.empty((3, m))\nfor j in range(m):\nR = np.sum(np.abs(T_vals) >= cutoffs[j])\nV = np.sum(np.abs(Tnull_vals) >= cutoffs[j]) / B\nRs[j] = R\nVs[j] = V\nFDRs[j] = V / R\nNow, for any given FDR, we can find the genes that will be rejected. For example, with FDR controlled at 0.1, we reject 15 of the 100 null\nhypotheses. On average, we would expect about one or two of these genes\n(i.e. 10% of 15) to be false discoveries.",
    "source": "book_islr",
    "section_title": "592 13. Multiple Testing"
  },
  {
    "instruction": "When or why would a data scientist use e−d−?",
    "input": "",
    "output": "2\n1x2\n1V(d−1)dx\n1\n= V(d−1) x\n1\ne−d−\n2\n1x2\n1dx\n1\nc c\n√c √c\nd−1 d−1\nNow\n(cid:90) √ ∞ c x 1 e−d− 2 1x2 1dx 1 = − d− 1 1 e−d− 2 1x2 1 (cid:12) (cid:12) (cid:12) ∞ √ c = d− 1 1 e−c 2 2\nd−1 (d−1)\nThus, an upper bound on volume(A) is V √ (d−1)e−c 2 2 . c d−1\nThe volume of the hemisphere below the plane x = √ 1 is a lower bound on the entire\n1\nd−1\nvolumeoftheupperhemisphereandthisvolumeisatleastthatofacylinderofheight √ 1\nd−1\n(cid:113)\nand radius 1− 1 . The volume of the cylinder is V(d−1)(1− 1 )d− 2 1 √ 1 . Using the\nd−1 d−1 d−1\nfact that (1−x)a ≥ 1−ax for a ≥ 1, the volume of the cylinder is at least V √ (d−1) for d ≥ 3.",
    "source": "book_fods",
    "section_title": "1 e−d−"
  },
  {
    "instruction": "When or why would a data scientist use cross-validation 207?",
    "input": "",
    "output": "! \"#\"$\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"%\"\n! ! \"&’\"(\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\")&\"\n! !",
    "source": "book_islr",
    "section_title": "5.1 Cross-Validation 207"
  },
  {
    "instruction": "Explain read_image(), 436 in simple terms for a beginner in data science.",
    "input": "",
    "output": "np.random.normal(), 45 reindex(), 461\nnp.sqrt(), 45 reshape(), 43\nIndex 605\nreturn, 198 StandardScaler(),185,438,\nRF(),seeRandomForestRegressor() 537, 555\nrng,seenp.random.default_rng() statsmodels, 116, 173\nrng.choice(), 60 std(), 186\nrng.standard_normal(),60 Stepwise(), 269\nroc_curve(), 392 str.contains(), 59\nRocCurveDisplay.from_estimator(), subplots(), 48",
    "source": "book_islr",
    "section_title": "47 read_image(), 436"
  },
  {
    "instruction": "When or why would a data scientist use 1?",
    "input": "",
    "output": "β ,whichwedousingleastsquareslinearregression.ComparingFigure2.3\n2\nto Figure 2.4, we can see that the linear fit given in Figure 2.4 is not quite\nright: the true f has some curvature that is not captured in the linear fit. However,the linearfit stillappears todo areasonable jobof capturing the\npositiverelationshipbetweenyears of educationandincome,aswellasthe\nslightly less positive relationship between seniority and income. It may be\nthat with such a small number of observations, this is the best we can do. Non-Parametric Methods\nNon-parametricmethodsdonotmakeexplicitassumptionsaboutthefunc-\ntionalformoff.Insteadtheyseekanestimateoff thatgetsasclosetothe\ndatapointsaspossiblewithoutbeingtooroughorwiggly.Suchapproaches\ncan have a major advantage over parametric approaches: by avoiding the\nassumption of a particular functional form for f, they have the potential\nto accurately fit a wider range of possible shapes for f. Any parametric\napproach brings with it the possibility that the functional form used to\nestimate f is very different from the true f, in which case the resulting\nmodel will not fit the data well. In contrast, non-parametric approaches\ncompletely avoid this danger, since essentially no assumption about the\nform of f is made.",
    "source": "book_islr",
    "section_title": "0 1"
  },
  {
    "instruction": "How is 2 k i different from i in data science?",
    "input": "",
    "output": "2 k i: where the set of vectors H is pairwise independent and |H| < kk. We say that a set of vec-\ntors is pairwise independent if for any subset of two of the coordinates, all of the k2 possible\npairsofvaluesthatcouldappearinthosecoordinatessuchas(0,0),(0,1),...,(1,0),(1,1),...\noccur the exact same number of times. i: ones by the coin-flip approximate counting algorithm, in Section 6.2.2. Repeat the process\nfor p=1/4, 1/8, and 1/16.",
    "source": "book_fods",
    "section_title": "1 2 k i vs 2 i"
  },
  {
    "instruction": "When or why would a data scientist use n?",
    "input": "",
    "output": "nents except for isolated vertices. Theorem 8.11 For p = clnn with c > 1/2, almost surely there are only isolated vertices\nn\nand a giant component. For c > 1, almost surely the graph is connected. Proof: We prove that almost surely for c > 1/2, there is no connected component with\nk vertices for any k, 2 ≤ k ≤ n/2. This proves the first statement of the theorem since, if\nthere were two or more components that are not isolated vertices, both of them could not\nbe of size greater than n/2.",
    "source": "book_fods",
    "section_title": "2 n"
  },
  {
    "instruction": "How is n i j different from 2 n i=1 i i=1 i in data science?",
    "input": "",
    "output": "n i j: in addition |x | = 1 for all i. A matrix A is orthonormal if ATA = I. 2 n i=1 i i=1 i: lemma is proved. 445\nThere are two important matrix norms, the matrix p-norm\n||A|| = max (cid:107)Ax(cid:107)\np p\n|x|=1\nand the Frobenius norm\n(cid:115)\n(cid:88)\n||A|| = a2 .",
    "source": "book_fods",
    "section_title": "1 n i j vs 1 2 n i=1 i i=1 i"
  },
  {
    "instruction": "How is 2 different from 6.74 in data science?",
    "input": "",
    "output": "2: Use axline() method to overlay those multiple linear regression\ncoefficient estimates on the plot obtained in (e). (h) On this data set, how many backfitting iterations were required\nin order to obtain a “good” approximation to the multiple re-\ngression coefficient estimates? 6.74: FIGURE 8.1. For the Hitters data, a regression tree for predicting the log\nsalary of a baseball player, based on the number of years that he has played in\nthe major leagues and the number of hits that he made in the previous year.",
    "source": "book_islr",
    "section_title": "1 2 vs 6.00 6.74"
  },
  {
    "instruction": "Describe the typical steps involved in 2 in a data science workflow.",
    "input": "",
    "output": "two clauses is picked by SC. So, the literals which are set to true during this period are\nchosen independent of these clauses. Say the two clauses were w +x+y and w¯ +u+v\nat the start. x,y,u, and v must all be negations of literals set to true during steps t to\n1\n(cid:0) (cid:1)\nt. So, there are only O (lnn)4 choices for x,y,u, and v for a given value of t. There are\nO(n) choices of w, O(n2) choices of which two clauses i and j of the input become these\n(cid:0) (cid:1)\nw and w¯, and n choices for t. Thus, there are O n4(lnn)4 choices for what these clauses\ncontain and which clauses they are in the input. On the other hand, for any given i and j,\nthe probability that clauses i and j both match a given set of literals is O(1/n6). Thus the\n(cid:0) (cid:1)\nprobability that these choices are actually realized is therefore O n4(lnn)4/n6 = o(1),\nas required.",
    "source": "book_fods",
    "section_title": "1 2"
  },
  {
    "instruction": "When or why would a data scientist use i = j?",
    "input": "",
    "output": "6. vTv =\ni j 0 i (cid:54)= j\n7. l = (cid:80) u σ vT\ni i i j\n44To minimize the absolute value of x write x = u−v and using linear programming minimize u+v\nsubject to u≥0 and v ≥0. 369\nConditions (5) and (6) insure that UΣVT is the svd of some matrix. The solution is\nobtained when ∇(l) = 0 which can be found by gradient descent using ∇2(l). An example where low rank matrices that have been corrupted might occur is aerial\nphotographs of an intersection. Given a long sequence of such photographs, they will be\nthe same except for cars and people.",
    "source": "book_fods",
    "section_title": "1 i = j"
  },
  {
    "instruction": "Explain 10. deep learning in simple terms for a beginner in data science.",
    "input": "",
    "output": "We’ll use a two-layer model for our first model. In[72]: class IMDBModel(nn.Module):\ndef __init__(self, input_size):\nsuper(IMDBModel, self).__init__()\nself.dense1 = nn.Linear(input_size, 16)\nself.activation = nn.ReLU()\nself.dense2 = nn.Linear(16, 16)\nself.output = nn.Linear(16, 1)\ndef forward(self, x):\nval = x\nfor _map in [self.dense1,\nself.activation,\nself.dense2,\nself.activation,\nself.output]:\nval = _map(val)\nreturn torch.flatten(val)\nWe now instantiate our model and look at a summary (not shown). In[73]: imdb_model = IMDBModel(imdb_test.tensors[0].size()[1])\nsummary(imdb_model,\ninput_size=imdb_test.tensors[0].size(),\ncol_names=['input_size',\n'output_size',\n'num_params'])\nWe’ll again use a smaller learning rate for these data, hence we pass an\noptimizertotheSimpleModule.Sincethereviewsareclassifiedintopositive\nor negative sentiment, we use SimpleModule.binary_classification().28\nIn[74]: imdb_optimizer = RMSprop(imdb_model.parameters(), lr=0.001)\nimdb_module = SimpleModule.binary_classification(\nimdb_model,\noptimizer=imdb_optimizer)\nHavingloadedthedatasetsintoadatamoduleandcreatedaSimpleModule,\nthe remaining steps are familiar.",
    "source": "book_islr",
    "section_title": "456 10. Deep Learning"
  },
  {
    "instruction": "Explain regression versus classification problems in simple terms for a beginner in data science.",
    "input": "",
    "output": "Variables can be characterized as either quantitative or qualitative (also\nquantitative\nknownascategorical).Quantitativevariablestakeonnumericalvalues.Ex-\nqualitative\namples include a person’s age, height, or income, the value of a house, and\ncategorical\nthe price of a stock. In contrast, qualitative variables take on values in\none of K different classes, or categories. Examples of qualitative variables\nclass\ninclude a person’s marital status (married or not), the brand of product\npurchased (brand A, B, or C), whether a person defaults on a debt (yes\nor no), or a cancer diagnosis (Acute Myelogenous Leukemia, Acute Lym-\nphoblastic Leukemia, or No Leukemia).",
    "source": "book_islr",
    "section_title": "2.1.5 Regression Versus Classification Problems"
  },
  {
    "instruction": "How is ata different from m+1 d 1 in data science?",
    "input": "",
    "output": "ata: (cid:1)k\nx\nw = . (cid:12) (cid:12)\n(cid:12)(ATA)kx(cid:12)\n(cid:12) (cid:12)\n10E.g., suppose each entry in the first row of A is nonzero and the rest of A is zero. m+1 d 1: Now\n(cid:12) (cid:12)2\n(cid:12)(cid:88) d (cid:12) (cid:88) d\n|(ATA)kx|2 = (cid:12) σ2kc v (cid:12) = σ4kc2 ≥ σ4kc2 ≥ σ4kδ2. (cid:12) i i i(cid:12) i i 1 1 1\n(cid:12) (cid:12)\ni=1 i=1\nThe component of |(ATA)kx|2 perpendicular to the space V is\nd d\n(cid:88) (cid:88)\nσ4kc2 ≤ (1−ε)4kσ4k c2 ≤ (1−ε)4kσ4k\ni i 1 i 1\ni=m+1 i=m+1\nsince (cid:80)d c2 = |x| = 1.",
    "source": "book_fods",
    "section_title": "ATA vs 1 m+1 d 1"
  },
  {
    "instruction": "How is 2 n different from 2 3 in data science?",
    "input": "",
    "output": "2 n: that the y should generally be close to the corresponding x and the second, a term\ni i\ncorrecting possible observation errors, stipulating that y should generally be close to the\ni\nvalues of y for j ∼ i. This can be formulated as\nj\n(cid:32) (cid:33)\n(cid:88) (cid:88)\nmin |x −y |+ |y −y | ,\ni i i j\ny\ni i∼j\nwhere the values of x are constrained to be the observed values. 2 3: Figure 9.3: The factor graph for the function\nf(x ,x ,x ) = (x +x +x )(x +x¯ )(x +x¯ )(x¯ +x¯ ). 1 2 3 1 2 3 1 2 1 3 2 3\nusing say sum of squares instead of sum of absolute values can be used and there are\npolynomial time algorithms as long as the function to be minimized is convex.",
    "source": "book_fods",
    "section_title": "1 2 n vs 1 2 3"
  },
  {
    "instruction": "When or why would a data scientist use 1?",
    "input": "",
    "output": "Maximum likelihood is a very general approach that is used to fit many\nof the non-linear models that we examine throughout this book. In the\nlinearregressionsetting,theleastsquaresapproachisinfactaspecialcase\nof maximum likelihood. The mathematical details of maximum likelihood\nare beyond the scope of this book. However, in general, logistic regression\nand other models can be easily fit using statistical software such as R, and\nso we do not need to concern ourselves with the details of the maximum\nlikelihood fitting procedure. Table 4.1 shows the coefficient estimates and related information that\nresult from fitting a logistic regression model on the Default data in order\nto predict the probability of default=Yes using balance.",
    "source": "book_islr",
    "section_title": "0 1"
  },
  {
    "instruction": "Explain 4δ in simple terms for a beginner in data science.",
    "input": "",
    "output": "Thetwosolutionsgivenby(8.7)becomecomplexforδ > 1/8andthuscanbevalidonly\nfor 0 ≤ δ ≤ 1/8. For δ > 1/8, the only solution is g(cid:48)(1) = 1 and an infinite component\n2δ\nexists. As δ is decreased, at δ = 1/8 there is a singular point where for δ < 1/8 there are\nthree possible solutions, one from (8.6) which implies a giant component and two from\n(8.7) which imply no giant component.",
    "source": "book_fods",
    "section_title": "2 4δ"
  },
  {
    "instruction": "How is 22 32 n2 different from warning propagation in data science?",
    "input": "",
    "output": "22 32 n2: Figure 9.9: Tree for MWM problem. matching. warning propagation: Significant progress has been made using methods similar to belief propagation in\nfinding satisfying assignments for 3-CNF formulas. Thus, we include a section on a\nversion of belief propagation, called warning propagation, that is quite effective in finding\nassignments.",
    "source": "book_fods",
    "section_title": "12 22 32 n2 vs 9.20 Warning Propagation"
  },
  {
    "instruction": "How is 1 different from the conceptual idea in data science?",
    "input": "",
    "output": "1: An interesting question is how to determine the quality of a community found. Many\nresearchers use an existing standard of what the communities are. the conceptual idea: We now consider another condition that will allow us to produce accurate clusters\nfrom data. To think about this condition, imagine that we are given a few thousand news\narticles that we want to cluster by topic.",
    "source": "book_fods",
    "section_title": "1 1 vs 7.6.1 The Conceptual Idea"
  },
  {
    "instruction": "When or why would a data scientist use relationship to logistic regression 385?",
    "input": "",
    "output": "classifier. Thus, a small value of λ in (9.25) amounts to a small value of C\nin (9.15). Note that the λ p β2 term in (9.25) is the ridge penalty term\nj=1 j\nfrom Section 6.2.1, and plays a similar role in controlling the bias-variance\n)\ntrade-off for the support vector classifier. Now(9.25)takesthe“Loss+Penalty”formthatwehaveseenrepeatedly\nthroughout this book:\nminimize L(X,y,β)+λP(β) . (9.26)\nβ0,β1,...,βp { }\nIn (9.26), L(X,y,β) is some loss function quantifying the extent to which\nthe model, parametrized by β, fits the data (X,y), and P(β) is a penalty\nfunction on the parameter vector β whose effect is controlled by a nonneg-\nativetuningparameterλ.Forinstance,ridgeregressionandthelassoboth\ntake this form with\n2\nn p\nL(X,y,β)= y β x β\ni 0 ij j\n − − \ni=1 j=1",
    "source": "book_islr",
    "section_title": "9.5 Relationship to Logistic Regression 385"
  },
  {
    "instruction": "How is another interpretation of principal components different from principal components analysis 509 in data science?",
    "input": "",
    "output": "another interpretation of principal components: The first two principal component loading vectors in a simulated three-\ndimensional data set are shown in the left-hand panel of Figure 12.2; these\ntwo loading vectors span a plane along which the observations have the\nhighest variance. Intheprevioussection,wedescribetheprincipalcomponentloadingvec-\ntors as the directions in feature space along which the data vary the most,\nand the principal component scores as projections along these directions. principal components analysis 509: First principal component\ntnenopmoc\nlapicnirp dnoceS\n−1.0 −0.5 0.0 0.5 1.0\n0.1\n5.0\n0.0\n5.0−\n0.1−\n• • •\n• • • • • • •• • • •• • •• • ••\n•\n• •\n•• •\n•\n• • •• • • • • •• • •• • •• ••• • • •• • • • • • • •• • • • • •• • • • • •\n•• • • •\n• • •\n••\n•\n•••\n• • • • • •\nFIGURE 12.2. Ninety observations simulated in three dimensions.",
    "source": "book_islr",
    "section_title": "12.2.2 Another Interpretation of Principal Components vs 12.2 Principal Components Analysis 509"
  },
  {
    "instruction": "Explain 1 2 t 0 1 2 t in simple terms for a beginner in data science.",
    "input": "",
    "output": "ending in state i. This is done by a dynamic programming algorithm starting with t = 0\nand increasing t. For t = 0 there have been no transitions. Thus, the probability of\nobserving O ending in state i is the initial probability of starting in state i times the\n0\nprobability of observing O in state i.",
    "source": "book_fods",
    "section_title": "0 1 2 t 0 1 2 t"
  },
  {
    "instruction": "Explain 13. multiple testing in simple terms for a beginner in data science.",
    "input": "",
    "output": "2. Suppose that we test m hypotheses, and control the Type I error for\neach hypothesis at level α. Assume that all m p-values are indepen-\ndent, and that all null hypotheses are true.",
    "source": "book_islr",
    "section_title": "594 13. Multiple Testing"
  },
  {
    "instruction": "How is r different from 6. linear model selection and regularization in data science?",
    "input": "",
    "output": "r: detsujdA\nFIGURE 6.2. C p, BIC, and adjusted R2 are shown for the best models of each\nsize for the Credit data set (the lower frontier in Figure 6.1). 6. linear model selection and regularization: the BIC statistic generally places a heavier penalty on models with many\nvariables, and hence results in the selection of smaller models than C . p\nIn Figure 6.2, we see that this is indeed the case for the Credit data set;\nBIC chooses a model that contains only the four predictors income, limit,\ncards, and student.",
    "source": "book_islr",
    "section_title": "2R vs 238 6. Linear Model Selection and Regularization"
  },
  {
    "instruction": "Describe the typical steps involved in lab: linear models and regularization methods 277 in a data science workflow.",
    "input": "",
    "output": "Alternatively, we can use 5-fold cross-validation. In[33]: grid = skm.GridSearchCV(pipe,\nparam_grid,\ncv=kfold,\nscoring='neg_mean_squared_error')\ngrid.fit(X, Y)\ngrid.best_params_['ridge__alpha']\ngrid.best_estimator_\nRecall we set up the kfold object for 5-fold cross-validation on page 271. We now plot the cross-validated MSE as a function of log(λ), which has\n−\nshrinkage decreasing from left to right. In[34]: ridge_fig, ax = subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n-grid.cv_results_['mean_test_score'],\nyerr=grid.cv_results_['std_test_score'] / np.sqrt(K))\nax.set_ylim([50000,250000])\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20);\nOne can cross-validate different metrics to choose a parameter. The de-\nfault metric for skl.ElasticNet() is test R2. Let’s compare R2 to MSE for\ncross-validation here.",
    "source": "book_islr",
    "section_title": "6.5 Lab: Linear Models and Regularization Methods 277"
  },
  {
    "instruction": "What is solving the dilation equation in data science?",
    "input": "",
    "output": "Consider solving a dilation equation\nd−1\n(cid:88)\nφ(x) = c φ(2x−k)\nk\nk=0\nto obtain the scale function for a wavelet system. Perhaps the easiest way is to assume\na solution and then calculate the scale function by successive approximation as in the\nfollowing program for the Daubechies scale function:\n√ √ √ √\nφ(x) = 1+ 3φ(2x)+ 3+ 3φ(2x−1)+ 3− 3φ(2x−2)+ 1− 3φ(2x−3),",
    "source": "book_fods",
    "section_title": "11.4 Solving the Dilation Equation"
  },
  {
    "instruction": "Describe the typical steps involved in o in a data science workflow.",
    "input": "",
    "output": "possible n p pairs. ×\nOnce we solve this problem:\n• wecanestimateamissingobservationx usingxˆ = M aˆ ˆb ,\nij ij m=1 im jm\nwhere aˆ and ˆb are the (i,m) and (j,m) elements, respectively,\nim jm )\nof the matrices Aˆ and Bˆ that solve (12.12); and\n• we can (approximately) recover the M principal component scores\nand loadings, as we did when the data were complete. It turns out that solving (12.12) exactly is difficult, unlike in the case of\ncomplete data: the eigen decomposition no longer applies. But the sim-\nple iterative approach in Algorithm 12.1, which is demonstrated in Sec-\ntion 12.5.2, typically provides a good solution.56\nWe illustrate Algorithm 12.1 on the USArrests data. There are p = 4\nvariables and n=50 observations (states). We first standardized the data\nso each variable has mean zero and standard deviation one.",
    "source": "book_islr",
    "section_title": "O"
  },
  {
    "instruction": "What is 12. unsupervised learning in data science?",
    "input": "",
    "output": "n\nJerr y M O a c g e u a ir n e s R o a d t o A P F e o r d rt i u t C i n o a a n t t c e h M M D a e n ri I v f i n Y g o T u M h C e is a s T w D T o a h i P s e y o L p a e u s C n o d d r e o m 8 T at he S o ci al · N ·· et w or k\nCustomer 1 4\n• • • • • • • • • ···\nCustomer 2 3 3 3\n• • • • • • • ···\nCustomer 3 2 4 2\n• • • • • • • ···\nCustomer 4 3\n• • • • • • • • • ···\nCustomer 5 5 1 4\n• • • • • • • ···\nCustomer 6 2 4\n• • • • • • • • ···\nCustomer 7 5 3\n• • • • • • • • ···\nCustomer 8\n• • • • • • • • • • ···\nCustomer 9 3 5 1\n• • • • • • • ···\n. . .",
    "source": "book_islr",
    "section_title": "520 12. Unsupervised Learning"
  },
  {
    "instruction": "Explain 2 n−1 i in simple terms for a beginner in data science.",
    "input": "",
    "output": "approximating n − 1 with n, the theorem follows from Theorem 12.6 in the appendix. Although the probability that the degree of a single vertex differs significantly from\nits expected value drops exponentially, the statement that the degree of every vertex is\nclose to its expected value requires that p is Ω(logn). That is, the expected degree grows\nn\nat least logarithmically with the number of vertices.",
    "source": "book_fods",
    "section_title": "1 2 n−1 i"
  },
  {
    "instruction": "Describe the typical steps involved in 1 1 i i 1 1 1 in a data science workflow.",
    "input": "",
    "output": "i=1\nNormalizing the resulting vector yields v , the first singular vector of A. The way Bkx\n1\nis computed is by a series of matrix vector products, instead of matrix products. Bkx =\nATA...ATAx, which can be computed right-to-left. This consists of 2k vector times\nsparse matrix multiplications. To compute k singular vectors, one selects a random vector r and finds an orthonormal\nbasis for the space spanned by r,Ar,...,Ak−1r. Then compute A times each of the basis\nvectors, and find an orthonormal basis for the space spanned by the resulting vectors.",
    "source": "book_fods",
    "section_title": "1 1 1 i i 1 1 1"
  },
  {
    "instruction": "What is 3 in data science?",
    "input": "",
    "output": "efficients, f (x) and its iterates are all monotonically increasing and convex on the unit\ninterval. Since the probabilities of the number of children of a node sum to one, if p < 1,\n0\nsome coefficient of x to a power other than zero in f (x) is nonzero and f (x) is strictly\nincreasing. Let q be the probability that the branching process dies out.",
    "source": "book_fods",
    "section_title": "2 3"
  },
  {
    "instruction": "Explain 0m in simple terms for a beginner in data science.",
    "input": "",
    "output": "therefore,thenumberofpermutedt-statisticsthatexceedthethresholdcin\nabsolutevalueprovidesanestimateforE(V).Thisestimatecanbefurther\nimproved by repeating the permutation process B times, for a large value\nof B, and averaging the results. Algorithm 13.4 details this procedure.18 It provides what is known as a\nplug-inestimateoftheFDR,becausetheapproximationin(13.13)allowsus\nto estimate the FDR by plugging R into the denominator and an estimate\nfor E(V) into the numerator. We apply the re-sampling approach to the FDR from Algorithm 13.4,\nas well as the Benjamini–Hochberg approach from Algorithm 13.2 using\ntheoreticalp-values,tothem=2,308genesintheKhandataset.Resultsare\nshowninFigure13.9.Weseethatforagivennumberofrejectedhypotheses,\nthe estimated FDRs are almost identical for the two methods.",
    "source": "book_islr",
    "section_title": "01 0m"
  },
  {
    "instruction": "Explain k-fold cross-validation in simple terms for a beginner in data science.",
    "input": "",
    "output": "An alternative to LOOCV is k-fold CV. This approach involves randomly\nk-foldCV\ndividing the set of observations into k groups, or folds, of approximately\nequal size. The first fold is treated as a validation set, and the method\nis fit on the remaining k 1 folds.",
    "source": "book_islr",
    "section_title": "5.1.3 k-Fold Cross-Validation"
  },
  {
    "instruction": "How is exercises 397 different from 2 in data science?",
    "input": "",
    "output": "exercises 397: (a) Generateadatasetwithn=500andp=2,suchthattheobser-\nvationsbelongtotwoclasseswithaquadraticdecisionboundary\nbetween them. For instance, you can do this as follows:\nrng = np.random.default_rng(5)\nx1 = rng.uniform(size=500) - 0.5\nx2 = rng.uniform(size=500) - 0.5\ny = x1**2 - x2**2 > 0\n(b) Plot the observations, colored according to their class labels. 2: predictors. (d) Apply this model to the training data in order to obtain a pre-\ndicted class label for each training observation.",
    "source": "book_islr",
    "section_title": "9.7 Exercises 397 vs 1 2"
  },
  {
    "instruction": "What is j in data science?",
    "input": "",
    "output": "(y yˆ )2, (8.1)\ni − Rj\n0\nj=1i\n0∈\nRj\nwhere yˆ is the mean response for the training observations within the\nRj\njth box. Unfortunately, it is computationally infeasible to consider every\npossiblepartitionofthefeaturespaceintoJ boxes.Forthisreason,wetake\natop-down,greedyapproachthatisknownasrecursivebinarysplitting.The\nrecursive\napproachistop-downbecauseitbeginsatthetopofthetree(atwhichpoint\nbinary\nall observations belong to a single region) and then successively splits the splitting\npredictor space; each split is indicated via two new branches further down\non the tree. It is greedy because at each step of the tree-building process,\nthe best split is made at that particular step, rather than looking ahead\nand picking a split that will lead to a better tree in some future step.",
    "source": "book_islr",
    "section_title": "J"
  },
  {
    "instruction": "When or why would a data scientist use t0?",
    "input": "",
    "output": "error zero. Proof: Suppose m is the number of examples the final classifier gets wrong. Each of\nthese m examples was misclassified at least t /2 times so each has weight at least αt0/2. 0\nThus the total weight is at least mαt0/2. On the other hand, at time t+1, only the weights\nof examples misclassified at time t were increased.",
    "source": "book_fods",
    "section_title": "1 t0"
  },
  {
    "instruction": "Explain 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Write a program to see what the solution looks like. 2. How does the solution change when the equation is changed to f(x) = 1f(2x) +\n3\n5f(2x−1)?",
    "source": "book_fods",
    "section_title": "2 2"
  },
  {
    "instruction": "When or why would a data scientist use 1?",
    "input": "",
    "output": "q x\nFigure 8.10: Illustration of the root of equation f(x) = x in the interval [0,1). The second observation is that the coefficient of xi in f (x) is the probability of\nj\nthere being i children in the jth generation. If there are i children in the jth generation,\nthe number of children in the j + 1st generation is the sum of i independent random\nvariables each with generating function f(x). Thus, the generating function for the j+1st\ngeneration, given i children in the jth generation, is fi(x). The generating function for\nthe j +1st generation is given by\n∞\n(cid:88)\nf (x) = Prob(z = i)fi(x).",
    "source": "book_fods",
    "section_title": "0 1"
  },
  {
    "instruction": "What is 0 in data science?",
    "input": "",
    "output": "t\nThe number of degree k vertices increases whenever a new edge is added to a degree k−1\nvertex and decreases when a new edge is added to a degree k vertex. Reasoning as above,\nd (t) d (t)\nk−1 k\nd (t+1) = d (t)+2δ −2δ . (8.4)\nk k\nt t\nNote that this formula, as others in this section, is not quite precise.",
    "source": "book_fods",
    "section_title": "0 0"
  },
  {
    "instruction": "What is classification trees in data science?",
    "input": "",
    "output": "A classification tree is very similar to a regression tree, except that it is\nclassification\nused to predict a qualitative response rather than a quantitative one. Re-\ntree\ncall that for a regression tree, the predicted response for an observation is\ngiven by the mean response of the training observations that belong to the\nsame terminal node. In contrast, for a classification tree, we predict that\neach observation belongs to the most commonly occurring class of training\nobservationsintheregiontowhichitbelongs.Ininterpretingtheresultsof\naclassificationtree,weareofteninterestednotonlyintheclassprediction\ncorresponding to a particular terminal node region, but also in the class\nproportions among the training observations that fall into that region.",
    "source": "book_islr",
    "section_title": "8.1.2 Classification Trees"
  },
  {
    "instruction": "What is assessing model accuracy 35 in data science?",
    "input": "",
    "output": "our estimate\nfˆ\nto the training observations:\nn\n1\nI(y =yˆ). (2.8)\ni i\nn %\ni=1\n0\nHere yˆ is the predicted class label for the ith observation using fˆ . And\ni\nI(y =yˆ)isanindicatorvariablethatequals1ify =yˆ andzeroify =yˆ.",
    "source": "book_islr",
    "section_title": "2.2 Assessing Model Accuracy 35"
  },
  {
    "instruction": "Explain 7. moving beyond linearity in simple terms for a beginner in data science.",
    "input": "",
    "output": "Out[17]: coef std err ...\nintercept 60.494 9.460 ...\nbs(age, internal_knots=[25, 40, 60])[0] 3.980 12.538 ...\nbs(age, internal_knots=[25, 40, 60])[1] 44.631 9.626 ...\nbs(age, internal_knots=[25, 40, 60])[2] 62.839 10.755 ...\nbs(age, internal_knots=[25, 40, 60])[3] 55.991 10.706 ...\nbs(age, internal_knots=[25, 40, 60])[4] 50.688 14.402 ...\nbs(age, internal_knots=[25, 40, 60])[5] 16.606 19.126 ... The column names are a little cumbersome, and have caused us to trun-\ncate the printed summary. They can be set on construction using the name\nargument as follows.",
    "source": "book_islr",
    "section_title": "316 7. Moving Beyond Linearity"
  },
  {
    "instruction": "What is local spectral clustering in data science?",
    "input": "",
    "output": "So far our emphasis has been on partitioning the data into disjoint clusters. However,\nthe structure of many data sets consists of over lapping communities. In this case using\nk-means with spectral clustering, the overlap of two communities shows up as a commu-\nnity.",
    "source": "book_fods",
    "section_title": "7.5.5 Local spectral clustering"
  },
  {
    "instruction": "When or why would a data scientist use p(n) 0?",
    "input": "",
    "output": "n→∞\nproperty Q. Give the symmetric argument that for any p (n) such that lim p(n) = 0,\n1\nn→∞\np1(n)\nG(n,p ) almost surely has property Q. 1\nExercise 8.35 Consider a model of a random subset N(n,p) of integers {1,2,...n} de-\nfined by independently at random including each of {1,2,...n} into the set with probability\np. Define what an “increasing property” of N(n,p) means. Prove that every increasing\nproperty of N(n,p) has a threshold. 305\nExercise 8.36 N(n,p) is a model of a random subset of integers {1,2,...n} defined by\nindependently at random including each of {1,2,...n} into the set with probability p.\nWhat is the threshold for N (n,p) to contain\n1. a perfect square,\n2. a perfect cube,\n3. an even number,\n4. three numbers such that x+y = z ?",
    "source": "book_fods",
    "section_title": "0 p(n) 0"
  },
  {
    "instruction": "How is spectral clustering different from why project? in data science?",
    "input": "",
    "output": "spectral clustering: Let A be a n×d data matrix with each row a data point and suppose we want to partition\nthedatapointsintok clusters. Spectral Clusteringreferstoaclassofclusteringalgorithms\nwhich share the following outline:\n• Find the space V spanned by the top k (right) singular vectors of A. why project?: The reader may want to read Section 3.9.3, which shows the efficacy of spectral clustering\nfor data stochastically generated from a mixture of spherical Gaussians. Here, we look at\ngeneral data which may not have a stochastic generation model.",
    "source": "book_fods",
    "section_title": "7.5 Spectral Clustering vs 7.5.1 Why Project?"
  },
  {
    "instruction": "Explain generative models for classification 147 in simple terms for a beginner in data science.",
    "input": "",
    "output": "π f (x)\nk k\nPr(Y =k X =x)= . (4.15)\n| K π f (x)\nl=1 l l\nInaccordancewithourearliernotation,we)willusetheabbreviationp\nk\n(x)=\nPr(Y = k X = x); this is the posterior probability that an observation\n| posterior\nX = x belongs to the kth class. That is, it is the probability that the\nobservation belongs to the kth class, given the predictor value for that\nobservation.",
    "source": "book_islr",
    "section_title": "4.4 Generative Models for Classification 147"
  },
  {
    "instruction": "Explain other measures of complexity in simple terms for a beginner in data science.",
    "input": "",
    "output": "VC-dimension and number of bits needed to describe a set are not the only measures\nof complexity one can use to derive generalization guarantees. There has been significant\n156\nwork on a variety of measures. One measure called Rademacher complexity measures\nthe extent to which a given concept class H can fit random noise.",
    "source": "book_fods",
    "section_title": "5.11.5 Other Measures of Complexity"
  },
  {
    "instruction": "Explain 30 40 50 60 70 80 in simple terms for a beginner in data science.",
    "input": "",
    "output": "02.0\n51.0\n01.0\n50.0\n00.0\n| ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| |\n||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||||\nAge\n)egA|052>egaW(rP\nFIGURE7.2.TheWagedata.Left:Thesolidcurvedisplaysthefittedvaluefrom\na least squares regression of wage (in thousands of dollars) using step functions\nof age. The dashed curves indicate an estimated 95% confidence interval. Right:\nWe model the binary event wage>250 using logistic regression, again using step\nfunctions of age.",
    "source": "book_islr",
    "section_title": "20 30 40 50 60 70 80"
  },
  {
    "instruction": "When or why would a data scientist use x[:,1],?",
    "input": "",
    "output": "c=y,\ncmap=cm.coolwarm);\nThey are not. We now fit the classifier. In[5]: svm_linear = SVC(C=10, kernel='linear')\nsvm_linear.fit(X, y)\nOut[5]:SVC(C=10, kernel='linear')\nThesupportvectorclassifierwithtwofeaturescanbevisualizedbyplot-\nting values of its decision function. We have included a function for this in\ndecision\nthe ISLP package (inspired by a similar example in the sklearn docs). function",
    "source": "book_islr",
    "section_title": "X[:,1],"
  },
  {
    "instruction": "How is 0 0 0 different from 0.2 0.4 0.6 0.8 1.0 in data science?",
    "input": "",
    "output": "0 0 0:  \nwhere λ 0 is a tuning parameter, to be determined separately. Equa-\n≥ tuning\ntion6.5tradesofftwodifferentcriteria.Aswithleastsquares,ridgeregres-\nparameter\nsion seeks coefficient estimates that fit the data well, by making the RSS\nsmall. 0.2 0.4 0.6 0.8 1.0: stneiciffeoC\ndezidradnatS\n004\n003\n002\n001\n0\n001−\n003−\nλ !β ˆ λ R!2/!β ˆ !2\nFIGURE 6.4. The standardized ridge regression coefficients are displayed for\nthe Credit data set, as a function of λ and $ βˆ λ R $ 2 / $ βˆ $ 2.\nthe relative impact of these two terms on the regression coefficient esti-\nmates.",
    "source": "book_islr",
    "section_title": "0 0 0 0 vs 0.0 0.2 0.4 0.6 0.8 1.0"
  },
  {
    "instruction": "Explain 1 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Maximum likelihood is a very general approach that is used to fit many\nof the non-linear models that we examine throughout this book. In the\nlinearregressionsetting,theleastsquaresapproachisinfactaspecialcase\nof maximum likelihood. The mathematical details of maximum likelihood\nare beyond the scope of this book.",
    "source": "book_islr",
    "section_title": "0 1"
  },
  {
    "instruction": "What is 100 200 300 400 500 in data science?",
    "input": "",
    "output": "5.0\n4.0\n3.0\n2.0\nNumber of Trees\nrorrE\nnoitacifissalC\ntseT\nm=p\nm=p/2\nm= p\nFIGURE 8.10. Results from random forests for the 15-class gene expression\ndata set with p = 500 predictors. The test error is displayed as a function of\nthe number of trees.",
    "source": "book_islr",
    "section_title": "0 100 200 300 400 500"
  },
  {
    "instruction": "When or why would a data scientist use single layer network on hitters data?",
    "input": "",
    "output": "We start by fitting the models in Section 10.6 on the Hitters data. In[10]: Hitters = load_data('Hitters').dropna()\nn = Hitters.shape[0]\nWe will fit two linear models (least squares and lasso) and compare their\nperformance to that of a neural network. For this comparison we will use\nmean absolute error on a validation dataset. n\n1\nMAE(y,yˆ)= y yˆ . i i\nn | − |\ni=1\n0\nWe set up the model matrix and the response.",
    "source": "book_islr",
    "section_title": "10.9.1 Single Layer Network on Hitters Data"
  },
  {
    "instruction": "How is shrinkage for the cox model different from p j=1 j in data science?",
    "input": "",
    "output": "shrinkage for the cox model: In this section, we illustrate that the shrinkage methods of Section 6.2\ncan be applied to the survival data setting. In particular, motivated by\nthe “loss+penalty” formulation of Section 6.2, we consider minimizing a\npenalized version of the negative log partial likelihood in (11.16),\nexp p x β\nj=1 ij j\nlog +λP(β), (11.17)\n−  1 e)xp p x 2 β \ni: E δi=1 i! p j=1 j: correspondstoaridgepenalty,orP(β)= p β ,whichcorrespondsto\nj=1| j | )\na lasso penalty.\n) In (11.17), λ is a non-negative tuning parameter; typically we will mini-\nmizeitoverarangeofvaluesofλ.Whenλ=0,thenminimizing(11.17)is\nequivalent to simply maximizing the usual Cox partial likelihood (11.16).",
    "source": "book_islr",
    "section_title": "11.6 Shrinkage for the Cox Model vs 1 p j=1 j"
  },
  {
    "instruction": "Describe the typical steps involved in random walks and markov chains in a data science workflow.",
    "input": "",
    "output": "A random walk on a directed graph consists of a sequence of vertices generated from\na start vertex by probabilistically selecting an incident edge, traversing the edge to a new\nvertex, and repeating the process. We generally assume the graph is strongly connected, meaning that for any pair of\nvertices x and y, the graph contains a path of directed edges starting at x and ending\nat y. If the graph is strongly connected, then, as we will see, no matter where the walk\nbegins the fraction of time the walk spends at the different vertices of the graph converges\nto a stationary probability distribution. Start a random walk at a vertex x and think of the starting probability distribution as\nputting a mass of one on x and zero on every other vertex. More generally, one could start\nwith any probability distribution p, where p is a row vector with nonnegative components\nsumming to one, with p being the probability of starting at vertex x. The probability\nx\nof being at vertex x at time t+1 is the sum over each adjacent vertex y of being at y at\ntime t and taking the transition from y to x.",
    "source": "book_fods",
    "section_title": "4 Random Walks and Markov Chains"
  },
  {
    "instruction": "When or why would a data scientist use implementing length squared sampling in two passes?",
    "input": "",
    "output": "Traditional matrix algorithms often assume that the input matrix is in random access\nmemory (RAM) and so any particular entry of the matrix can be accessed in unit time. For massive matrices, RAM may be too small to hold the entire matrix, but may be able\nto hold and compute with the sampled columns and rows. Consider a high-level model where the input matrix or matrices have to be read from\nexternal memory using one pass in which one can read sequentially all entries of the ma-\ntrix and sample. It is easy to see that two passes suffice to draw a sample of columns of A according\nto length squared probabilities, even if the matrix is not in row-order or column-order\nand entries are presented as a linked list. In the first pass, compute the length squared of\neach column and store this information in RAM.",
    "source": "book_fods",
    "section_title": "6.3.2 Implementing Length Squared Sampling in Two Passes"
  },
  {
    "instruction": "When or why would a data scientist use nonuniform models of random graphs?",
    "input": "",
    "output": "So far we have considered the G(n,p) random graph model in which all vertices have\nthe same expected degree, and moreover degrees are concentrated close to their expecta-\ntion. However, large graphs occurring in the real world tend to have power law degree\ndistributions. For a power law degree distribution, the number f(d) of vertices of degree\nd scales as 1/dα for some constant α > 0. One way to generate such graphs is to stipulate that there are f(d) vertices of degree\nd and choose uniformly at random from the set of graphs with this degree distribution. Clearly, in this model the graph edges are not independent and this makes these random\ngraphs harder to analyze.",
    "source": "book_fods",
    "section_title": "8.8 Nonuniform Models of Random Graphs"
  },
  {
    "instruction": "How is p j=1 j different from 20 40 60 80 100 120 in data science?",
    "input": "",
    "output": "p j=1 j: correspondstoaridgepenalty,orP(β)= p β ,whichcorrespondsto\nj=1| j | )\na lasso penalty.\n) In (11.17), λ is a non-negative tuning parameter; typically we will mini-\nmizeitoverarangeofvaluesofλ.Whenλ=0,thenminimizing(11.17)is\nequivalent to simply maximizing the usual Cox partial likelihood (11.16). 20 40 60 80 100 120: 0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nMonths\ndehsilbuP\ngnieB\ntoN\nfo\nytilibaborP\nNegative Result\nPositive Result\nFIGURE 11.6. For the Publication data, we display survival curves for time\nuntil publication, stratified by whether or not the study produced a positive result,\nafter adjusting for all other covariates.",
    "source": "book_islr",
    "section_title": "1 p j=1 j vs 0 20 40 60 80 100 120"
  },
  {
    "instruction": "When or why would a data scientist use 1,i+1 1 2 2,i+1 2?",
    "input": "",
    "output": "put in these two vectors for each vector in the previous list. Then, crucially, we prune -\ni.e., eliminate duplicates. Assume that k is constant. Now, we show that the error is at most √ n2 as claimed. √ k+1\n¯\nSince u and v are unit length vectors, |u (S)|,|v (S)| ≤ n. Also |u˜ (S) − u (S)| ≤\ni i i i i i\nn = 1 andsimilarlyforv .",
    "source": "book_fods",
    "section_title": "1 1,i+1 1 2 2,i+1 2"
  },
  {
    "instruction": "What is b=1000, in data science?",
    "input": "",
    "output": "seed=0):\nrng = np.random.default_rng(seed)\nfirst_, second_ = 0, 0\nn = n or D.shape[0]\nfor _ in range(B):\nidx = rng.choice(D.index,\nn,\nreplace=True)\nvalue = func(D, idx)\nfirst_ += value\nsecond_ += value**2\nreturn np.sqrt(second_ / B - (first_ / B)**2)\nNotice the use of _ as a loop variable in for _ in range(B). This is often\nused if the value of the counter is unimportant and simply makes sure the\nloop is executed B times. Let’suseourfunctiontoevaluatetheaccuracyofourestimateofαusing\nB =1,000 bootstrap replications.",
    "source": "book_islr",
    "section_title": "B=1000,"
  },
  {
    "instruction": "Describe the typical steps involved in 2 n in a data science workflow.",
    "input": "",
    "output": "that the y should generally be close to the corresponding x and the second, a term\ni i\ncorrecting possible observation errors, stipulating that y should generally be close to the\ni\nvalues of y for j ∼ i. This can be formulated as\nj\n(cid:32) (cid:33)\n(cid:88) (cid:88)\nmin |x −y |+ |y −y | ,\ni i i j\ny\ni i∼j\nwhere the values of x are constrained to be the observed values. The objective function\ni\nis convex and polynomial time minimization algorithms exist. Other objective functions\n339\nx +x +x x +x x +x x +x",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "When or why would a data scientist use interpreting results in high dimensions?",
    "input": "",
    "output": "When we perform the lasso, ridge regression, or other regression proce-\nduresinthehigh-dimensionalsetting,wemustbequitecautiousintheway\nthat we report the results obtained. In Chapter 3, we learned about multi-\ncollinearity, the concept that the variables in a regression might be corre-\nlatedwitheachother.Inthehigh-dimensionalsetting,themulticollinearity\nproblem is extreme: any variable in the model can be written as a linear\ncombination of all of the other variables in the model. Essentially, this\nmeans that we can never know exactly which variables (if any) truly are\npredictive of the outcome, and we can never identify the best coefficients\nfor use in the regression. At most, we can hope to assign large regression\ncoefficientstovariablesthatarecorrelatedwiththevariablesthattrulyare\npredictive of the outcome.",
    "source": "book_islr",
    "section_title": "6.4.4 Interpreting Results in High Dimensions"
  },
  {
    "instruction": "Explain 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "These are the hypotheses that we don’t want to output. Consider drawing the sample S\nof size n and let A be the event that h is consistent with S. Since every h has true error\ni i i\ngreater than or equal to (cid:15)\nProb(A ) ≤ (1−(cid:15))n.\ni\nIn other words, if we fix h and draw a sample S of size n, the chance that h makes no\ni i\nmistakes on S is at most the probability that a coin of bias (cid:15) comes up tails n times in a\nrow, which is (1−(cid:15))n. By the union bound over all i we have\nProb(∪ A ) ≤ |H|(1−(cid:15))n.\ni i\nUsing the fact that (1−(cid:15)) ≤ e−(cid:15), the probability that any hypothesis in H with true error\ngreater than or equal to (cid:15) has training error zero is at most |H|e−(cid:15)n. Replacing n by the\nsample size bound from the theorem statement, this is at most |H|e−ln|H|−ln(1/δ) = δ as\ndesired. 136\nNotspam Spam\n(cid:122) (cid:125)(cid:124) (cid:123) (cid:122) (cid:125)(cid:124) (cid:123)\nx x x x x x x x x x x x x x x x emails\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n↓ ↓ ↓\n0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 target concept\n(cid:108) (cid:108) (cid:108)\n0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 hypothesis h\ni\n↑ ↑ ↑\nFigure 5.3: Thehypothesish disagreeswiththetruthinonequarteroftheemails.",
    "source": "book_fods",
    "section_title": "1 2"
  },
  {
    "instruction": "Explain why estimate f? in simple terms for a beginner in data science.",
    "input": "",
    "output": "There are two main reasons that we may wish to estimate f: prediction\nand inference. We discuss each in turn. Prediction\nIn many situations, a set of inputs X are readily available, but the output\nY cannot be easily obtained.",
    "source": "book_islr",
    "section_title": "2.1.1 Why Estimate f?"
  },
  {
    "instruction": "What is 0 0 0 in data science?",
    "input": "",
    "output": " \nComparing (6.7) to (6.5), we see that the lasso and ridge regression have\nsimilar formulations. The only difference is that the β2 term in the ridge\nj\nregressionpenalty(6.5)hasbeenreplacedby β inthelassopenalty(6.7). j\n| |\nIn statistical parlance, the lasso uses an % (pronounced “ell 1”) penalty\n1\ninstead of an % penalty.",
    "source": "book_islr",
    "section_title": "0 0 0 0"
  },
  {
    "instruction": "What is exercises 193 in data science?",
    "input": "",
    "output": "ax.set_ylabel('Poisson Regression Fit', fontsize=20)\nax.axline([0,0], c='black', linewidth=3,\nlinestyle='--', slope=1);\nThe predictions from the Poisson regression model are correlated with\nthose from the linear model; however, the former are non-negative. As a\nresult the Poisson regression predictions tend to be larger than those from\nthe linear model for either very low or very high levels of ridership. Inthissection,wefitPoissonregressionmodelsusingthesm.GLM()func-\ntion with the argument family=sm.families.Poisson().",
    "source": "book_islr",
    "section_title": "4.8 Exercises 193"
  },
  {
    "instruction": "What is lab: linear models and regularization methods 277 in data science?",
    "input": "",
    "output": "Alternatively, we can use 5-fold cross-validation. In[33]: grid = skm.GridSearchCV(pipe,\nparam_grid,\ncv=kfold,\nscoring='neg_mean_squared_error')\ngrid.fit(X, Y)\ngrid.best_params_['ridge__alpha']\ngrid.best_estimator_\nRecall we set up the kfold object for 5-fold cross-validation on page 271. We now plot the cross-validated MSE as a function of log(λ), which has\n−\nshrinkage decreasing from left to right.",
    "source": "book_islr",
    "section_title": "6.5 Lab: Linear Models and Regularization Methods 277"
  },
  {
    "instruction": "How is 2 d different from 2 d j in data science?",
    "input": "",
    "output": "2 d: it for prediction. We will maintain the invariant that every variable in the target disjunc-\ntion is also in our hypothesis, which is clearly true at the start. 2 d j: except for a 1 in the jth position. Imagine running A on sequence σ and telling A it made\na mistake on every example; that is, if A predicts positive on e we set c∗(e ) = −1 and if\nj j\nA predicts negative on e we set c∗(e ) = +1.",
    "source": "book_fods",
    "section_title": "1 2 d vs 1 2 d j"
  },
  {
    "instruction": "What is k in data science?",
    "input": "",
    "output": "Proof: ConsiderrunningthealgorithmuntilallpairsofclustersC andC(cid:48) haved (C,C(cid:48)) ≥ σ.\nmin\nAt that point, by (2), each target cluster C∗ will be fully contained within some cluster\ni\nof the single-linkage algorithm. On the other hand, by (1) and by induction, each cluster\nC of the single-linkage algorithm will be fully contained within some C∗ of the target\ni\nclustering, since any merger of subsets of distinct target clusters would require d ≥ σ.\nmin\nTherefore, the single-linkage clusters are indeed the target clusters. 36Other agglomerative algorithms include complete linkage which merges the two clusters whose max-\nimum distance between points is smallest, and Ward’s algorithm described earlier that merges the two\nclusters that cause the k-means cost to increase by the least.",
    "source": "book_fods",
    "section_title": "1 k"
  },
  {
    "instruction": "Explain cross-validation (statistics) part 9 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Cross-validation only yields meaningful results if the validation set and training set are drawn from the same population and only if human biases are controlled. In many applications of predictive modeling, the structure of the system being studied evolves over time (i.e. it is \"non-stationary\").",
    "source": "web_wikipedia",
    "section_title": "Cross-validation (statistics) part 9"
  },
  {
    "instruction": "When or why would a data scientist use the validation set approach?",
    "input": "",
    "output": "We explore the use of the validation set approach in order to estimate the\ntest error rates that result from fitting various linear models on the Auto\ndata set. We use the function train_test_split() to split the data into training\ntrain_test_\nand validation sets. As there are 392 observations, we split into two equal\nsplit()\nsets of size 196 using the argument test_size=196. It is generally a good\nideatosetarandomseedwhenperformingoperationslikethisthatcontain\nan element of randomness, so that the results obtained can be reproduced\nprecisely at a later time. We set the random seed of the splitter with the\nargument random_state=0.",
    "source": "book_islr",
    "section_title": "5.3.1 The Validation Set Approach"
  },
  {
    "instruction": "When or why would a data scientist use |a∪b|?",
    "input": "",
    "output": "Suppose that instead of using the sets A and B, one sampled the sets and compared\nrandom subsets of size ten. How accurate would the estimate be? One way to sample\nwould be to select ten elements uniformly at random from A and B. Suppose A and B\nwere each of size 1000, over lapped by 500, and both were represented by six samples. Even though half of the six samples of A were in B they would not likely be among the\nsamples representing B.",
    "source": "book_fods",
    "section_title": "|A∪B|"
  },
  {
    "instruction": "Explain a b c in simple terms for a beginner in data science.",
    "input": "",
    "output": "(b)\nFigure 4.1: (a) A directed graph with vertices having no out out edges and a strongly\nconnected component A with no in edges. (b) A directed graph with three strongly connected components. probability distribution of the random walk converges to a unique stationary distribution.",
    "source": "book_fods",
    "section_title": "A B C"
  },
  {
    "instruction": "Explain generalized linear models 167 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Coefficient Std. error t-statistic p-value\nIntercept 73.60 5.13 14.34 0.00\nworkingday 1.27 1.78 0.71 0.48\ntemp 157.21 10.26 15.32 0.00\nweathersit[cloudy/misty] -12.89 1.96 -6.56 0.00\nweathersit[light rain/snow] -66.49 2.97 -22.43 0.00\nweathersit[heavy rain/snow] -109.75 76.67 -1.43 0.15\nTABLE 4.10. Results for a least squares linear model fit to predict bikers in\nthe Bikeshare data.",
    "source": "book_islr",
    "section_title": "4.6 Generalized Linear Models 167"
  },
  {
    "instruction": "How is lab: logistic regression, lda, qda, and knn 187 different from k, in data science?",
    "input": "",
    "output": "lab: logistic regression, lda, qda, and knn 187: Out[58]: Truth No Yes\nPredicted\nNo 880 58\nYes 53 9\nIt turns out that KNN with K = 1 does far better than random guessing\namong the customers that are predicted to buy insurance. Among 62 such\ncustomers, 9, or 14.5%, actually do purchase insurance. k,: pred,\ndid_rent,\ndid_rent / pred))\nK=1: # predicted to rent: 62,# who did rent 9, accuracy 14.5%\nK=2: # predicted to rent: 6,# who did rent 1, accuracy 16.7%\nK=3: # predicted to rent: 20,# who did rent 3, accuracy 15.0%\nK=4: # predicted to rent: 3,# who did rent 0, accuracy 0.0%\nK=5: # predicted to rent: 7,# who did rent 1, accuracy 14.3%\nWe see some variability — the numbers for K=4 are very different from the\nrest. Comparison to Logistic Regression\nAs a comparison, we can also fit a logistic regression model to the data.",
    "source": "book_islr",
    "section_title": "4.7 Lab: Logistic Regression, LDA, QDA, and KNN 187 vs K,"
  },
  {
    "instruction": "How is proportional hazards different from 1.0 1.5 2.0 in data science?",
    "input": "",
    "output": "proportional hazards: The Proportional Hazards Assumption\nThe proportional hazards assumption states that\nproportional\nhazards\np\nassumption\nh(tx )=h (t)exp x β , (11.14)\ni 0 ij j\n|  \nj=1\n0\n \nwhere h (t) 0 is an unspecified function, known as the baseline hazard. 0\n≥ baseline\nIt is the hazard function for an individual with features x = =x =\ni1 ··· ip hazard\n0. 1.0 1.5 2.0: 0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nTime\nytilibaborP\nlavivruS\nFIGURE 11.4. Top: In a simple example with p = 1 and a binary covariate\nx i 0,1 , the log hazard and the survival function under the model (11.14)\n∈ { }\nare shown (green for x i = 0 and black for x i = 1).",
    "source": "book_islr",
    "section_title": "11.5.2 Proportional Hazards vs 0.5 1.0 1.5 2.0"
  },
  {
    "instruction": "Explain 1 1 ij 2 kl in simple terms for a beginner in data science.",
    "input": "",
    "output": "It remains to solve the nonlinear problem in 2r2 variables. There is a classical algo-\nrithm which solves such problems in time exponential only in r2 (polynomial in the other\nparameters). In fact, there is a logical theory, called the Theory of Reals, of which this\nis a special case and any problem in this theory can be solved in time exponential in the\nnumber of variables.",
    "source": "book_fods",
    "section_title": "2 1 1 ij 2 kl"
  },
  {
    "instruction": "What is left singular vectors in data science?",
    "input": "",
    "output": "The left singular vectors are also pairwise orthogonal. Intuitively if u and u , i < j, were\ni j\nnot orthogonal, one would suspect that the right singular vector v had a component of v\nj i\n48\nwhich would contradict that v and v were orthogonal. Let i be the smallest integer such\ni j\nthat u is not orthogonal to all other u .",
    "source": "book_fods",
    "section_title": "3.6 Left Singular Vectors"
  },
  {
    "instruction": "What is 9. support vector machines in data science?",
    "input": "",
    "output": "−6 −4 −2 0 2\nssoL\n8\n6\n4\n2\n0\nSVM Loss\nLogistic Regression Loss\ny i(β0+β1x i1+...+β p x ip)\nFIGURE 9.12. The SVM and logistic regression loss functions are compared,\nasafunctionofy\ni\n(β\n0\n+β\n1\nx\ni1\n+ +β\np\nx\nip\n).Wheny\ni\n(β\n0\n+β\n1\nx\ni1\n+ +β\np\nx\nip\n)is\n··· ···\ngreaterthan1,thentheSVMlossiszero,sincethiscorrespondstoanobservation\nthatisonthecorrectsideofthemargin.Overall,thetwolossfunctionshavequite\nsimilar behavior. We have established that the support vector classifier is closely related\ntologisticregressionandotherpreexistingstatisticalmethods.IstheSVM\nunique in its use of kernels to enlarge the feature space to accommodate\nnon-linear class boundaries?",
    "source": "book_islr",
    "section_title": "386 9. Support Vector Machines"
  },
  {
    "instruction": "When or why would a data scientist use 12 20?",
    "input": "",
    "output": "Number of resistors\nin parallel\n(a) (b)\nFigure 4.10: 2-dimensional lattice along with the linear network resulting from shorting\nresistors on the concentric squares about the origin. dimensional lattice will return to the origin with probability one. Note, however, that\nthe expected time to return to the origin having taken one step away, which is equal to\ncommute(1,0), is infinite (Theorem 4.9. Two dimensions\nFor the 2-dimensional lattice, consider a larger and larger square about the origin for\nthe boundary as shown in Figure 4.10a and consider the limit of r as the squares get\neff\nlarger. Shorting the resistors on each square can only reduce r .",
    "source": "book_fods",
    "section_title": "4 12 20"
  },
  {
    "instruction": "How is evp different from nwonknu nairavo in data science?",
    "input": "",
    "output": "evp: evitalumuC\nFIGURE12.18. ThePVEoftheprincipalcomponentsoftheNCI60cancercell\nline microarray data set. nwonknu nairavo: CLCSN CLCSN ETATSORP AMONALEM NOLOC NAIRAVO CLCSN LANER NOLOC ETATSORP NOLOC NAIRAVO NOLOC NOLOC CLCSN CLCSN LANER CLCSN LANER LANER LANER LANER LANER SNC SNC SNC\nSingle Linkage\nFIGURE 12.19. The NCI60 cancer cell line microarray data, clustered with\naverage,complete,andsinglelinkage,andusingEuclideandistanceasthedissim-\nilarity measure.",
    "source": "book_islr",
    "section_title": "EVP vs NWONKNU NAIRAVO"
  },
  {
    "instruction": "Explain 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "(d) Show that f (ξ)=f (ξ). That is, f (x) is continuous at ξ. 1$ 2$ $\n(e) Show that f (ξ)=f (ξ).",
    "source": "book_islr",
    "section_title": "1 2"
  },
  {
    "instruction": "Explain 4. classification in simple terms for a beginner in data science.",
    "input": "",
    "output": "pd.Series([-coef_hr.sum()], index=['hr[23]'])\n])\nWe now make the hour plot. In[75]: fig_hr, ax_hr = subplots(figsize=(8,8))\nx_hr = np.arange(coef_hr.shape[0])\nax_hr.plot(x_hr, coef_hr, marker='o', ms=10)\nax_hr.set_xticks(x_hr[::2])\nax_hr.set_xticklabels(range(24)[::2], fontsize=20)\nax_hr.set_xlabel('Hour', fontsize=20)\nax_hr.set_ylabel('Coefficient', fontsize=20);\nPoisson Regression\nNow we fit instead a Poisson regression model to the Bikeshare data. Very\nlittlechanges,exceptthatwenowusethefunctionsm.GLM()withthePois-\nson family specified:\nIn[76]: M_pois = sm.GLM(Y, X2, family=sm.families.Poisson()).fit()\nWe can plot the coefficients associated with mnth and hr, in order to\nreproduce Figure 4.15.",
    "source": "book_islr",
    "section_title": "192 4. Classification"
  },
  {
    "instruction": "When or why would a data scientist use simple linear regression 73?",
    "input": "",
    "output": "β0\nFIGURE 3.2. Contour and three-dimensional plots of the RSS on the\nAdvertising data, using sales as the response and TV as the predictor. The\nred dots correspond to the least squares estimates βˆ 0 and βˆ 1, given by (3.4). The model given by (3.5) defines the population regression line, which\npopulation\nis the best linear approximation to the true relationship between X and\nregression\nY.1 Theleastsquaresregressioncoefficientestimates(3.4)characterizethe line\nleast squares line (3.2). The left-hand panel of Figure 3.3 displays these\nleastsquares\ntwo lines in a simple simulated example.",
    "source": "book_islr",
    "section_title": "3.1 Simple Linear Regression 73"
  },
  {
    "instruction": "What is k in data science?",
    "input": "",
    "output": "dimensional case, given in (4.20). To assign a new observation X = x,\nLDA plugs these estimates into (4.24) to obtain quantities δˆ (x), and clas-\nk\nsifies to the class for which δˆ (x) is largest. Note that in (4.24) δ (x) is\nk k\na linear function of x; that is, the LDA decision rule depends on x only",
    "source": "book_islr",
    "section_title": "1 K"
  },
  {
    "instruction": "What is 1 1 in data science?",
    "input": "",
    "output": "44\nNote that the n-dimensional vector Av is a list of lengths (with signs) of the projec-\ni\ntions of the rows of A onto v . Think of |Av | = σ (A) as the component of the matrix\ni i i\nA along v . For this interpretation to make sense, it should be true that adding up the\ni\nsquares of the components of A along each of the v gives the square of the “whole content\ni\nof A”.",
    "source": "book_fods",
    "section_title": "2 1 1"
  },
  {
    "instruction": "When or why would a data scientist use 2?",
    "input": "",
    "output": "- - #2 norm\n“ell 2”) of a vector, and is defined as β = p β 2. It measures the\n- - 2 j=1 j\ndistance of β from zero. As λ increases, theG% 2)norm of βˆ\nλ\nR will always\ndecrease,andsowill βˆR / βˆ .Thelatterquantityrangesfrom1(when\n- λ- 2 - - 2\nλ = 0, in which case the ridge regression coefficient estimate is the same\nas the least squares estimate, and so their % norms are the same) to 0\n2\n(when λ = , in which case the ridge regression coefficient estimate is a\n∞\nvectorofzeros,with% normequaltozero).Therefore,wecanthinkofthe\n2\nx-axis in the right-hand panel of Figure 6.4 as the amount that the ridge\nregression coefficient estimates have been shrunken towards zero; a small\nvalue indicates that they have been shrunken very close to zero. The standard least squares coefficient estimates discussed in Chapter 3\nare scale equivariant: multiplying X by a constant c simply leads to a\nj\nscale\nscalingoftheleastsquarescoefficientestimatesbyafactorof1/c.Inother\nequivariant\nwords, regardless of how the jth predictor is scaled, X βˆ will remain the\nj j\nsame.Incontrast,theridgeregressioncoefficientestimatescanchangesub-\nstantially when multiplying a given predictor by a constant. For instance,\nconsider the income variable, which is measured in dollars.",
    "source": "book_islr",
    "section_title": "2 2"
  },
  {
    "instruction": "How is 10. deep learning different from pooling layers in data science?",
    "input": "",
    "output": "10. deep learning: • If we use K different convolution filters at this first hidden layer,\nwe get K two-dimensional output feature maps, which together are\ntreated as a single three-dimensional feature map. We view each of\nthe K output feature maps as a separate channel of information, so\nnow we have K channels in contrast to the three color channels of\ntheoriginalinputfeaturemap.Thethree-dimensionalfeaturemapis\njust like the activations in a hidden layer of a simple neural network,\nexcept organized and produced in a spatially structured way. pooling layers: A pooling layer provides a way to condense a large image into a smaller\npooling\nsummary image. While there are a number of possible ways to perform\npooling, the max pooling operation summarizes each non-overlapping 2 2\n×\nblock of pixels in an image using the maximum value in the block.",
    "source": "book_islr",
    "section_title": "410 10. Deep Learning vs 10.3.2 Pooling Layers"
  },
  {
    "instruction": "How is lab: support vector machines 389 different from 8 10 in data science?",
    "input": "",
    "output": "lab: support vector machines 389: In[11]: X_test = rng.standard_normal((20, 2))\ny_test = np.array([-1]*10+[1]*10)\nX_test[y_test==1] += 1\nNowwepredicttheclasslabelsofthesetestobservations.Hereweusethe\nbest model selected by cross-validation in order to make the predictions. In[12]: best_ = grid.best_estimator_\ny_test_hat = best_.predict(X_test)\nconfusion_table(y_test_hat, y_test)\nOut[12]: Truth -1 1\nPredicted\n-1 8 4 8 10: In this case 60% of test observations are correctly classified. We now consider a situation in which the two classes are linearly sepa-\nrable.",
    "source": "book_islr",
    "section_title": "9.6 Lab: Support Vector Machines 389 vs 1 8 10"
  },
  {
    "instruction": "When or why would a data scientist use c=1)?",
    "input": "",
    "output": "svm_flex.fit(X_train, y_train)\nfig, ax = subplots(figsize=(8,8))\nroc_curve(svm_flex,\nX_train,\ny_train,\nname='Training $\\gamma=50$',\ncolor='r',\nax=ax);\nHowever, these ROC curves are all on the training data. We are really\nmore interested in the level of prediction accuracy on the test data. When\nwe compute the ROC curves on the test data, the model with γ = 0.5\nappears to provide the most accurate results. In[28]: roc_curve(svm_flex,\nX_test,\ny_test,\nname='Test $\\gamma=50$',\ncolor='b',\nax=ax)\nfig;\nLet’s look at our tuned SVM. In[29]: fig, ax = subplots(figsize=(8,8))\nfor (X_, y_, c, name) in zip(\n(X_train, X_test),\n(y_train, y_test),\n('r', 'b'),\n('CV tuned on training',\n'CV tuned on test')):\nroc_curve(best_svm,",
    "source": "book_islr",
    "section_title": "C=1)"
  },
  {
    "instruction": "When or why would a data scientist use y,?",
    "input": "",
    "output": "scoring='neg_mean_squared_error',\ncv=validation)\n-results['test_score']\nOut[31]:array([231788.32])\nObviously choosing λ=0.01 is arbitrary, so we will use cross-validation or\nthe validation-set approach to choose the tuning parameter λ. The object\nGridSearchCV() allows exhaustive grid search to choose such a parameter. Grid\nWe first use the validation set method to choose λ.\nSearchCV()\nIn[32]: param_grid = {'ridge__alpha': lambdas}\ngrid = skm.GridSearchCV(pipe,\nparam_grid,\ncv=validation,\nscoring='neg_mean_squared_error')\ngrid.fit(X, Y)\ngrid.best_params_['ridge__alpha']\ngrid.best_estimator_\nOut[32]:Pipeline(steps=[('scaler', StandardScaler()),\n('ridge', ElasticNet(alpha=0.005899, l1_ratio=0))])",
    "source": "book_islr",
    "section_title": "Y,"
  },
  {
    "instruction": "When or why would a data scientist use 2. statistical learning?",
    "input": "",
    "output": "In[57]: A[1,2]\nOut[57]:6\nThe first number after the open-bracket symbol [ refers to the row, and\nthe second number refers to the column. Indexing Rows, Columns, and Submatrices\nTo select multiple rows at a time, we can pass in a list specifying our\nselection. For instance, [1,3] will retrieve the second and fourth rows:\nIn[58]: A[[1,3]]\nOut[58]:array([[ 4, 5, 6, 7],\n[12, 13, 14, 15]])\nTo select the first and third columns, we pass in [0,2] as the second ar-\ngument in the square brackets. In this case we need to supply the first\nargument : which selects all rows. In[59]: A[:,[0,2]]\nOut[59]:array([[ 0, 2],\n[ 4, 6],\n[ 8, 10],\n[12, 14]])\nNow, suppose that we want to select the submatrix made up of the second\nandfourthrowsaswellasthefirstandthirdcolumns.Thisiswhereindex-\ning gets slightly tricky.",
    "source": "book_islr",
    "section_title": "52 2. Statistical Learning"
  },
  {
    "instruction": "When or why would a data scientist use 5. resampling methods?",
    "input": "",
    "output": "Out[25]:intercept 2.067840\npoly(horsepower, 2, raw=True)[0] 0.033019\npoly(horsepower, 2, raw=True)[1] 0.000120\ndtype: float64\nWecomparetheresultstothestandarderrorscomputedusingsm.OLS(). In[26]: M = sm.OLS(Auto['mpg'],\nquad_model.fit_transform(Auto))\nsummarize(M.fit())['std err']\nOut[26]:intercept 1.800\npoly(horsepower, 2, raw=True)[0] 0.031\npoly(horsepower, 2, raw=True)[1] 0.000\nName: std err, dtype: float64",
    "source": "book_islr",
    "section_title": "224 5. Resampling Methods"
  },
  {
    "instruction": "What is assessing model accuracy 27 in data science?",
    "input": "",
    "output": "measurement. For the remaining n m observations, we have predictor\n−\nmeasurements but no response measurement. Such a scenario can arise if\nthe predictors can be measured relatively cheaply but the corresponding\nresponses are much more expensive to collect.",
    "source": "book_islr",
    "section_title": "2.2 Assessing Model Accuracy 27"
  },
  {
    "instruction": "Explain convolutional neural networks in simple terms for a beginner in data science.",
    "input": "",
    "output": "In this section we fit a CNN to the CIFAR100 data, which is available in the\ntorchvision package. It is arranged in a similar fashion as the MNIST data. In[48]: (cifar_train,\ncifar_test) = [CIFAR100(root=\"data\",\ntrain=train,\ndownload=True)\nfor train in [True, False]]\nIn[49]: transform = ToTensor()\ncifar_train_X = torch.stack([transform(x) for x in\ncifar_train.data])\ncifar_test_X = torch.stack([transform(x) for x in\ncifar_test.data])\ncifar_train = TensorDataset(cifar_train_X,\ntorch.tensor(cifar_train.targets))\ncifar_test = TensorDataset(cifar_test_X,\ntorch.tensor(cifar_test.targets))\nTheCIFAR100datasetconsistsof50,000trainingimages,eachrepresented\nby a three-dimensional tensor: each three-color image is represented as a\nset of three channels, each of which consists of 32 32 eight-bit pixels.",
    "source": "book_islr",
    "section_title": "10.9.3 Convolutional Neural Networks"
  },
  {
    "instruction": "What is multiple linear regression in data science?",
    "input": "",
    "output": "In order to fit a multiple linear regression model using least squares, we\nagainusetheModelSpec()transformtoconstructtherequiredmodelmatrix\nand response. The arguments to ModelSpec() can be quite general, but in\nthiscasealistofcolumnnamessuffice.Weconsiderafitherewiththetwo\nvariables lstat and age. In[25]: X = MS(['lstat', 'age']).fit_transform(Boston)\nmodel1 = sm.OLS(y, X)\nresults1 = model1.fit()\nsummarize(results1)\nOut[25]: coef std err t P>|t|\nintercept 33.2228 0.731 45.458 0.000\nlstat -1.0321 0.048 -21.416 0.000\nage 0.0345 0.012 2.826 0.005\nNotice how we have compacted the first line into a succinct expression\ndescribing the construction of X.\nTheBostondatasetcontains12variables,andsoitwouldbecumbersome\nto have to type all of these in order to perform a regression using all of the\npredictors.",
    "source": "book_islr",
    "section_title": "3.6.3 Multiple Linear Regression"
  },
  {
    "instruction": "What is logistic regression part 9 in data science?",
    "input": "",
    "output": "Like other forms of regression analysis, logistic regression makes use of one or more predictor variables that may be either continuous or categorical. Unlike ordinary linear regression, however, logistic regression is used for predicting dependent variables that take membership in one of a limited number of categories (treating the dependent variable in the binomial case as the outcome of a Bernoulli trial) rather than a continuous outcome. Given this difference, the assumptions of linear regression are violated.",
    "source": "web_wikipedia",
    "section_title": "Logistic regression part 9"
  },
  {
    "instruction": "When or why would a data scientist use v?",
    "input": "",
    "output": "dim(V)=k\nwhere proj(x,V) is the orthogonal projection of x onto V.\nLemma 3.17 For a spherical Gaussian with center µ, a k-dimensional subspace is a best\nfit subspace if and only if it contains µ. Proof: If µ = 0, then by symmetry any k-dimensional subspace is a best-fit subspace. If\nµ (cid:54)= 0, then, the best-fit line must pass through µ by Lemma 3.16. Now, as in the greedy\nalgorithm for finding subsequent singular vectors, we would project perpendicular to the\nfirst singular vector. But after the projection, the mean of the Gaussian becomes 0 and\nany vectors will do as subsequent best-fit directions.",
    "source": "book_fods",
    "section_title": "V"
  },
  {
    "instruction": "When or why would a data scientist use 4000 6000 8000 12000?",
    "input": "",
    "output": "008\n006\n004\n002\nLimit\ngnitaR\nFIGURE 3.14.Scatterplots of the observations from the Credit data set. Left:\nA plot of age versus limit. These two variables are not collinear. Right: A plot\nof rating versus limit. There is high collinearity.",
    "source": "book_islr",
    "section_title": "2000 4000 6000 8000 12000"
  },
  {
    "instruction": "When or why would a data scientist use survival and censoring times?",
    "input": "",
    "output": "Foreachindividual,wesupposethatthereisatruesurvivaltime,T,aswell\nsurvivaltime\nasatruecensoring time,C. (Thesurvivaltimeisalsoknownasthefailure\ncensoring\ntimeortheevent time. )Thesurvivaltimerepresentsthetimeatwhichthe\ntime\nevent of interest occurs: for instance, the time at which the patient dies,\nfailuretime\nor the customer cancels his or her subscription. By contrast, the censoring eventtime\ntime is the time at which censoring occurs: for example, the time at which\nthe patient drops out of the study or the study ends. We observe either the survival time T or else the censoring time C.\nSpecifically, we observe the random variable\nY =min(T,C).",
    "source": "book_islr",
    "section_title": "11.1 Survival and Censoring Times"
  },
  {
    "instruction": "What is comparison of linear regression in data science?",
    "input": "",
    "output": "with K-Nearest Neighbors\nAs discussed in Chapter 2, linear regression is an example of a parametric\napproachbecauseitassumesalinearfunctionalformforf(X).Parametric\nmethods have several advantages. They are often easy to fit, because one\nneed estimate only a small number of coefficients. In the case of linear re-\ngression,thecoefficientshavesimpleinterpretations,andtestsofstatistical\nsignificance can be easily performed.",
    "source": "book_islr",
    "section_title": "3.5 Comparison of Linear Regression"
  },
  {
    "instruction": "When or why would a data scientist use m m?",
    "input": "",
    "output": "prediction error on a validation set, C (AIC), BIC, or adjusted R2. p\nOr use the cross-validation method. In Algorithm 6.1, Step 2 identifies the best model (on the training data)\nforeachsubsetsize,inordertoreducetheproblemfromoneof2p possible\nmodels to one of p+1 possible models. In Figure 6.1, these models form\nthe lower frontier depicted in red. Nowinordertoselectasinglebestmodel,wemustsimplychooseamong\nthese p+1 options.",
    "source": "book_islr",
    "section_title": "M M"
  },
  {
    "instruction": "How is 2 n i j different from bibliographic notes in data science?",
    "input": "",
    "output": "2 n i j: definite. Specifically, A = BBT where the ith row of B equals φ(a ). bibliographic notes: Arrow’s impossibility theorem, stating that any ranking of three or more items satisfying\nunanimity and independence of irrelevant alternatives must be a dictatorship, is from\n[Arr50]. For extensions to Arrow’s theorem on the manipulability of voting rules, see\nGibbard [Gib73] and Satterthwaite [Sat75].",
    "source": "book_fods",
    "section_title": "1 2 n i j vs 10.9 Bibliographic Notes"
  },
  {
    "instruction": "What is m in data science?",
    "input": "",
    "output": "As with PCR, the number M of partial least squares directions used in\nPLS is a tuning parameter that is typically chosen by cross-validation. We\ngenerally standardize the predictors and response before performing PLS. PLS is popular in the field of chemometrics, where many variables arise\nfromdigitizedspectrometrysignals.Inpracticeitoftenperformsnobetter\nthan ridge regression or PCR.",
    "source": "book_islr",
    "section_title": "1 M"
  },
  {
    "instruction": "What is spectral clustering in data science?",
    "input": "",
    "output": "An important part of a clustering toolkit when data lies in Rd is Singular Value De-\ncomposition. Spectral Clustering refers to the following algorithm: Find the space V\nspanned by the top k right singular vectors of the matrix A whose rows are the data\npoints. Project data points to V and cluster in the projection.",
    "source": "book_fods",
    "section_title": "7.1.3 Spectral Clustering"
  },
  {
    "instruction": "Explain 13. multiple testing in simple terms for a beginner in data science.",
    "input": "",
    "output": "challenges are presented in Section 13.3, and more contemporary solutions\nin Sections 13.4 and 13.5. In particular, Section 13.4 focuses on the false discovery rate. The no-\ntion of the false discovery rate dates back to the 1990s.",
    "source": "book_islr",
    "section_title": "558 13. Multiple Testing"
  },
  {
    "instruction": "When or why would a data scientist use j?",
    "input": "",
    "output": "(e) On the basis of your response to the previous question, fit a\nsmaller model that only uses the predictors for which there is\nevidence of association with the outcome. (f) How well do the models in (a) and (e) fit the data? (g) Using the model from (e), obtain 95% confidence intervals for\nthe coefficient(s). (h) Is there evidence of outliers or high leverage observations in the\nmodel from (e)? 11.",
    "source": "book_islr",
    "section_title": "0 j"
  },
  {
    "instruction": "When or why would a data scientist use lab: linear models and regularization methods 267?",
    "input": "",
    "output": "Forinstance,supposethatwearetryingtopredictbloodpressureonthe\nbasis of half a million SNPs, and that forward stepwise selection indicates\nthat17ofthoseSNPsleadtoagoodpredictivemodelonthetrainingdata. Itwouldbeincorrecttoconcludethatthese17SNPspredictbloodpressure\nmore effectively than the other SNPs not included in the model. There are\nlikely to be many sets of 17 SNPs that would predict blood pressure just\naswellastheselectedmodel.Ifweweretoobtainanindependentdataset\nand perform forward stepwise selection on that data set, we would likely\nobtain a model containing a different, and perhaps even non-overlapping,\nset of SNPs. This does not detract from the value of the model obtained—\nfor instance, the model might turn out to be very effective in predicting\nblood pressure on an independent set of patients, and might be clinically\nuseful for physicians. But we must be careful not to overstate the results\nobtained, and to make it clear that what we have identified is simply one\nof many possible models for predicting blood pressure, and that it must be\nfurther validated on independent data sets.",
    "source": "book_islr",
    "section_title": "6.5 Lab: Linear Models and Regularization Methods 267"
  },
  {
    "instruction": "What is 0 in data science?",
    "input": "",
    "output": "2WeexcludeC0(X)asapredictorin(7.5)becauseitisredundantwiththeintercept. Thisissimilartothefactthatweneedonlytwodummyvariablestocodeaqualitative\nvariablewiththreelevels,providedthatthemodelwillcontainanintercept.Thedecision\nto exclude C0(X) instead of some other Ck(X) in (7.5) is arbitrary. Alternatively, we\ncouldincludeC0(X),C1(X),...,CK(X),andexcludetheintercept.",
    "source": "book_islr",
    "section_title": "1 0"
  },
  {
    "instruction": "What is overfitting and uniform convergence in data science?",
    "input": "",
    "output": "We now present two generalization guarantees that explain how one can guard against\noverfitting. To keep things simple, we assume our hypothesis class H is finite. Later, we\n17The only two subsets that are not in the class are the sets {(−1,−1),(1,1)} and {(−1,1),(1,−1)}.",
    "source": "book_fods",
    "section_title": "5.5 Overfitting and Uniform Convergence"
  },
  {
    "instruction": "What is 2 i in data science?",
    "input": "",
    "output": "(f(cid:48)(q))i. The expected size of the tree is the sum of the expected sizes of each generation. That is,\n∞ ∞\nExpected size of (cid:88) (cid:88) 1\n= E(z |extinction) = (f(cid:48)(q))i = .",
    "source": "book_fods",
    "section_title": "1 2 i"
  },
  {
    "instruction": "What is example: brain cancer data in data science?",
    "input": "",
    "output": "Table 11.2 shows the result of fitting the proportional hazards model to\nthe BrainCancer data, which was originally described in Section 11.3. The\ncoefficient column displays\nβˆ\n. The results indicate, for instance, that the\nj\nestimated hazard for a male patient is e0.18 = 1.2 times greater than for\na female patient: in other words, with all other features held fixed, males\nhavea1.2timesgreaterchanceofdyingthanfemales,atanypointintime.",
    "source": "book_islr",
    "section_title": "11.5.3 Example: Brain Cancer Data"
  },
  {
    "instruction": "What is 1 p in data science?",
    "input": "",
    "output": "Table 4.3 shows the coefficient estimates for a logistic regression model\nthat uses balance, income (in thousands of dollars), and student status to\npredict probability of default. There is a surprising result here. The p-\nvalues associated with balance and the dummy variable for student status\nare very small, indicating that each of these variables is associated with\ntheprobabilityofdefault.However,thecoefficientforthedummyvariable\nis negative, indicating that students are less likely to default than non-\nstudents.",
    "source": "book_islr",
    "section_title": "0 1 p"
  },
  {
    "instruction": "Explain 3 dcba in simple terms for a beginner in data science.",
    "input": "",
    "output": "The Hare system would first eliminate d since d gets only three rank one votes. Then\nit would eliminate b since b gets only six rank one votes whereas a gets seven and c gets\neight. At this point a is declared the winner since a has thirteen votes to c’s eight votes.",
    "source": "book_fods",
    "section_title": "4 3 dcba"
  },
  {
    "instruction": "Explain lab: deep learning 435 in simple terms for a beginner in data science.",
    "input": "",
    "output": "parametrized neural network (many layers, and many hidden units),\nand then fitting all the way to zero training error. However, fitting\nto zero error is not always optimal, and whether it is advisable de-\npends on the signal-to-noise ratio. For instance, we may use ridge\nregularization to avoid overfitting a neural network, as in (10.31).",
    "source": "book_islr",
    "section_title": "10.9 Lab: Deep Learning 435"
  },
  {
    "instruction": "What is comparison of linear regression with k-nearest neighbors 113 in data science?",
    "input": "",
    "output": "−1.0 −0.5 0.0 0.5 1.0\n4\n3\n2\n1\n−1.0 −0.5 0.0 0.5 1.0\ny y\n4\n3\n2\n1\nx x\nFIGURE3.17. Plotsoffˆ(X)usingKNNregressiononaone-dimensionaldata\nset with 50 observations. The true relationship is given by the black solid line.",
    "source": "book_islr",
    "section_title": "3.5 Comparison of Linear Regression with K-Nearest Neighbors 113"
  },
  {
    "instruction": "Explain pooling layers in simple terms for a beginner in data science.",
    "input": "",
    "output": "A pooling layer provides a way to condense a large image into a smaller\npooling\nsummary image. While there are a number of possible ways to perform\npooling, the max pooling operation summarizes each non-overlapping 2 2\n×\nblock of pixels in an image using the maximum value in the block. This\nreduces the size of the image by a factor of two in each direction, and it\nalso provides some location invariance: i.e.",
    "source": "book_islr",
    "section_title": "10.3.2 Pooling Layers"
  },
  {
    "instruction": "What is x 2 in data science?",
    "input": "",
    "output": "FIGURE 2.13. A simulated data set consisting of 100 observations in each of\ntwo groups, indicated in blue and in orange. The purple dashed line represents\nthe Bayes decision boundary.",
    "source": "book_islr",
    "section_title": "X 2"
  },
  {
    "instruction": "Explain 0 1 0 in simple terms for a beginner in data science.",
    "input": "",
    "output": "column of A is unit length, the matrix ATA has ones on its diagonal. Since every pair of\ndistinct columns of A has dot-product in the range (− 1 , 1 ), each off-diagonal entry in\n2s 2s\nATA is in the range (− 1 , 1 ). These two facts imply that unless z = 0, every entry in z\n2s 2s\nmust have absolute value less than 1 ||z|| .",
    "source": "book_fods",
    "section_title": "1 0 1 0"
  },
  {
    "instruction": "When or why would a data scientist use φ(s,t) = .?",
    "input": "",
    "output": "Total number of edges incident to S in G\nΦ(S,T) measures the relative strength of similarities between S and T. Let d(i) be the\n(cid:80)\ndegree of vertex i and for a subset S of vertices, let d(S) = d(i). Let m be the total\ni∈S\nnumber of edges in the graph. The following algorithm aims to cut only a small fraction\nof the edges and to produce clusters that are internally consistent in that no subset of the\ncluster has low similarity to the rest of the cluster. Recursive Clustering: Select an appropriate value for (cid:15). If a current cluster\nW has a subset S with d(S) ≤ 1d(W) and Φ(S,S ⊆ W) ≤ ε, then split W\n2\ninto two clusters S and W \\S.",
    "source": "book_fods",
    "section_title": "Φ(S,T) = ."
  },
  {
    "instruction": "What is p(x) × in data science?",
    "input": "",
    "output": "* − +\nwhere\np(X)=Pr(wage>250year,age,education). |\nOnce again f is fit using a smoothing spline with five degrees of freedom,\n2\nandf isfitasastepfunction,bycreatingdummyvariablesforeachofthe\n3\nlevelsofeducation.TheresultingfitisshowninFigure7.13.Thelastpanel\nlooks suspicious, with very wide confidence intervals for level <HS. In fact,\nnoresponsevaluesequaloneforthatcategory:noindividualswithlessthan\na high school education make more than $250,000 per year.",
    "source": "book_islr",
    "section_title": "1 p(X) ×"
  },
  {
    "instruction": "How is that achieve greater than 30 miles per gallon: different from for loops in data science?",
    "input": "",
    "output": "that achieve greater than 30 miles per gallon:: In[94]: Auto_re.loc[lambda df: (df['year'] > 80) & (df['mpg'] > 30),\n['weight', 'origin']\n]\nThe symbol & computes an element-wise and operation. As another ex-\nample, suppose that we want to retrieve all Ford and Datsun cars with\ndisplacement less than 300. for loops: A for loop is a standard tool in many languages that repeatedly evaluates\nfor\nsome chunk of code while varying different values inside the code. For\nexample, suppose we loop over elements of a list and compute their sum.",
    "source": "book_islr",
    "section_title": "1980 that achieve greater than 30 miles per gallon: vs 2.3.8 For Loops"
  },
  {
    "instruction": "Explain 2 n in simple terms for a beginner in data science.",
    "input": "",
    "output": "n\nunique point x, namely their centroid, which minimizes (cid:80) |a − x|2. Show examples\ni\ni=1\nn\n(cid:80)\nwhere the x minimizing |a −x| is not unique. (Consider just points on the real line.)",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "When or why would a data scientist use logistic regression part 1?",
    "input": "",
    "output": "In statistics, a logistic model (or logit model) is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) estimates the parameters of a logistic model (the coefficients in the linear or non linear combinations). In binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \"0\" and \"1\", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names.",
    "source": "web_wikipedia",
    "section_title": "Logistic regression part 1"
  },
  {
    "instruction": "Explain lab: deep learning 445 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Let’s take a look at the data that will get fed into our network. We loop\nthrough the first few chunks of the test dataset, breaking after 2 batches:\nIn[35]: for idx, (X_ ,Y_) in enumerate(mnist_dm.train_dataloader()):\nprint('X: ', X_.shape)\nprint('Y: ', Y_.shape)\nif idx >= 1:\nbreak\nX: torch.Size([256, 1, 28, 28])\nY: torch.Size([256])\nX: torch.Size([256, 1, 28, 28])\nY: torch.Size([256])\nWe see that the X for each batch consists of 256 images of size 1x28x28. Here the 1 indicates a single channel (greyscale).",
    "source": "book_islr",
    "section_title": "10.9 Lab: Deep Learning 445"
  },
  {
    "instruction": "Explain data science part 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business.",
    "source": "web_wikipedia",
    "section_title": "Data science part 2"
  },
  {
    "instruction": "When or why would a data scientist use what goes wrong in high dimensions??",
    "input": "",
    "output": "In order to illustrate the need for extra care and specialized techniques\nfor regression and classification when p > n, we begin by examining what\ncan go wrong if we apply a statistical technique not intended for the high-\ndimensionalsetting.Forthispurpose,weexamineleastsquares regression. Butthesameconceptsapplytologisticregression,lineardiscriminantanal-\nysis, and other classical statistical approaches. Whenthenumberoffeaturespisaslargeas,orlargerthan,thenumber\nofobservationsn,leastsquaresasdescribedinChapter3cannot(orrather,\nshould not) be performed. The reason is simple: regardless of whether or\nnot there truly is a relationship between the features and the response,\nleast squares will yield a set of coefficient estimates that result in a perfect\nfit to the data, such that the residuals are zero. AnexampleisshowninFigure6.22withp=1feature(plusanintercept)\nin two cases: when there are 20 observations, and when there are only\ntwo observations.",
    "source": "book_islr",
    "section_title": "6.4.2 What Goes Wrong in High Dimensions?"
  },
  {
    "instruction": "What is lab: linear models and regularization methods 267 in data science?",
    "input": "",
    "output": "Forinstance,supposethatwearetryingtopredictbloodpressureonthe\nbasis of half a million SNPs, and that forward stepwise selection indicates\nthat17ofthoseSNPsleadtoagoodpredictivemodelonthetrainingdata. Itwouldbeincorrecttoconcludethatthese17SNPspredictbloodpressure\nmore effectively than the other SNPs not included in the model. There are\nlikely to be many sets of 17 SNPs that would predict blood pressure just\naswellastheselectedmodel.Ifweweretoobtainanindependentdataset\nand perform forward stepwise selection on that data set, we would likely\nobtain a model containing a different, and perhaps even non-overlapping,\nset of SNPs.",
    "source": "book_islr",
    "section_title": "6.5 Lab: Linear Models and Regularization Methods 267"
  },
  {
    "instruction": "What is 2 n n i in data science?",
    "input": "",
    "output": "i=1\nbe the cluster centroid. The centroid c is not in general a unit vector. Define the similarity\nbetween two points a and a as their dot product.",
    "source": "book_fods",
    "section_title": "1 2 n n i"
  },
  {
    "instruction": "What is |s| in data science?",
    "input": "",
    "output": "To discuss this, we need to define what we mean by a “pruning” of T and what we mean\nby the “size” of h. A pruning h of T is a tree in which some internal nodes of T have been\nturned into leaves, labeled “+” or “−” depending on whether the majority of examples in\nS that reach that node are positive or negative. Let size(h) = L(h)log(n) where L(h) is\nthe number of leaves in h.\nHint #1: it is sufficient, for each integer L = 1,2,...,L(T), to find the pruning of T\nwith L leaves of lowest empirical error on S, that is, h = argmin err (h). Then\nL h:L(h)=L S\nyou can just plug them all into the displayed formula above and pick the best one.",
    "source": "book_fods",
    "section_title": "2|S|"
  },
  {
    "instruction": "Explain 2 r in simple terms for a beginner in data science.",
    "input": "",
    "output": "max |Av| = 0.\nv⊥v1,v2,...,vr\n|v|=1\nThe greedy algorithm found the v that maximized |Av| and then the best fit 2-\n1\ndimensional subspace containing v . Is this necessarily the best-fit 2-dimensional sub-\n1\nspace overall? The following theorem establishes that the greedy algorithm finds the best\nsubspaces of every dimension.",
    "source": "book_fods",
    "section_title": "1 2 r"
  },
  {
    "instruction": "How is i=1 different from 10. deep learning in data science?",
    "input": "",
    "output": "i=1: −\nwhere\nK p\nf(x )=β + β g w + w x . (10.24)\ni 0 k k0 kj ij\nk 0 =1 1 0 j=1 2\nThe objective in (10.23) looks simple enough, but because of the nested\narrangementoftheparametersandthesymmetryofthehiddenunits,itis\nnot straightforward to minimize. 10. deep learning: −1.0 −0.5 0.0 0.5 1.0\nFIGURE 10.17. Illustration of gradient descent for one-dimensional θ.",
    "source": "book_islr",
    "section_title": "0 i=1 vs 428 10. Deep Learning"
  },
  {
    "instruction": "What is 1 1 i k k i in data science?",
    "input": "",
    "output": "···\ninordertopredicttheprobabilitythatanindividualisahighearneronthe\nbasisofage.Theright-handpanelofFigure7.2displaysthefittedposterior\nprobabilities obtained using this approach. Unfortunately, unless there are natural breakpoints in the predictors,\npiecewise-constant functions can miss the action. For example, in the left-\nhand panel of Figure 7.2, the first bin clearly misses the increasing trend\nof wage with age.",
    "source": "book_islr",
    "section_title": "0 1 1 i K K i"
  },
  {
    "instruction": "When or why would a data scientist use y =2+3x +\", (3.6)?",
    "input": "",
    "output": "where \" was generated from a normal distribution with mean zero. The\nred line in the left-hand panel of Figure 3.3 displays the true relationship,\nf(X) = 2+3X, while the blue line is the least squares estimate based\non the observed data. The true relationship is generally not known for\nreal data, but the least squares line can always be computed using the\ncoefficient estimates given in (3.4). In other words, in real applications,\nwe have access to a set of observations from which we can compute the\nleast squares line; however, the population regression line is unobserved. In the right-hand panel of Figure 3.3 we have generated ten different data\nsets from the model given by (3.6) and plotted the corresponding ten least\nsquares lines.",
    "source": "book_islr",
    "section_title": "Y =2+3X +\", (3.6)"
  },
  {
    "instruction": "Explain definitions and key theorems in simple terms for a beginner in data science.",
    "input": "",
    "output": "Definition 5.1 Given a set S of examples and a concept class H, we say that S is\nshattered by H if for every A ⊆ S there exists some h ∈ H that labels all examples in A\nas positive and all examples in S \\A as negative. Definition 5.2 The VC-dimension of H is the size of the largest set shattered by H.\nFor example, there exist sets of four points in the plane that can be shattered by rect-\nangles with axis-parallel edges, e.g., four points at the vertices of a diamond (see Figure\n5.5). Given such a set S, for any A ⊆ S, there exists a rectangle with the points in A\ninside the rectangle and the points in S \\ A outside the rectangle.",
    "source": "book_fods",
    "section_title": "5.11.1 Definitions and Key Theorems"
  },
  {
    "instruction": "Explain simple linear regression in simple terms for a beginner in data science.",
    "input": "",
    "output": "In this section we will construct model matrices (also called design matri-\nces) using the ModelSpec() transform from ISLP.models. We will use the Boston housing data set, which is contained in the ISLP\npackage. The Boston dataset records medv (median house value) for 506\nneighborhoods around Boston.",
    "source": "book_islr",
    "section_title": "3.6.2 Simple Linear Regression"
  },
  {
    "instruction": "Explain tv in simple terms for a beginner in data science.",
    "input": "",
    "output": "Radio\nFIGURE 3.5. For the Advertising data, a linear regression fit to sales using\nTV and radio as predictors. From the pattern of the residuals, we can see that\nthere is a pronounced non-linear relationship in the data.",
    "source": "book_islr",
    "section_title": "TV"
  },
  {
    "instruction": "How is time series forecasting different from 1970 1975 1980 1985 in data science?",
    "input": "",
    "output": "time series forecasting: Figure 10.14 shows historical trading statistics from the New York Stock\nExchange.ShownarethreedailytimeseriescoveringtheperiodDecember\n3, 1962 to December 31, 1986:18\n17An IMDb leaderboard can be found at https://paperswithcode.com/sota/\nsentiment-analysis-on-imdb. 18ThesedatawereassembledbyLeBaronandWeigend(1998)IEEETransactionson\nNeural Networks,9(1):213–220. 1970 1975 1980 1985: )ytilitaloV(goL\n8−\n9−\n11−\n31−\nFIGURE10.14.HistoricaltradingstatisticsfromtheNewYorkStockExchange. Dailyvaluesofthenormalizedlogtradingvolume,DJIAreturn,andlogvolatility\nareshownfora24-yearperiodfrom1962–1986.Wewishtopredicttradingvolume\nonanyday,giventhehistoryonallearlierdays.Totheleftoftheredbar(January\n2, 1980) is training data, and to the right test data.",
    "source": "book_islr",
    "section_title": "10.5.2 Time Series Forecasting vs 1965 1970 1975 1980 1985"
  },
  {
    "instruction": "When or why would a data scientist use 2. statistical learning?",
    "input": "",
    "output": "thetruef givenbytheblackcurve.Theorange,blueandgreencurvesillus-\ntratethreepossibleestimatesforf obtainedusingmethodswithincreasing\nlevelsofflexibility.Theorangelineisthelinearregressionfit,whichisrela-\ntivelyinflexible.Theblueandgreencurveswereproducedusingsmoothing\nsplines, discussed in Chapter 7, with different levels of smoothness. It is\nsmoothing\nclear that as the level of flexibility increases, the curves fit the observed\nspline\ndata more closely. The green curve is the most flexible and matches the\ndata very well; however, we observe that it fits the true f (shown in black)\npoorly because it is too wiggly. By adjusting the level of flexibility of the\nsmoothing spline fit, we can produce many different fits to this data. We now move on to the right-hand panel of Figure 2.9.",
    "source": "book_islr",
    "section_title": "30 2. Statistical Learning"
  },
  {
    "instruction": "What is data science part 3 in data science?",
    "input": "",
    "output": "=== Early usage ===\nIn 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.",
    "source": "web_wikipedia",
    "section_title": "Data science part 3"
  },
  {
    "instruction": "Explain 12. unsupervised learning in simple terms for a beginner in data science.",
    "input": "",
    "output": "Uniqueness of the Principal Components\nWhileintheorytheprincipalcomponentsneednotbeunique,inalmostall\npractical settings they are (up to sign flips). This means that two different\nsoftwarepackageswillyieldthesameprincipalcomponentloadingvectors,\nalthoughthesignsofthoseloadingvectorsmaydiffer.Thesignsmaydiffer\nbecause each principal component loading vector specifies a direction in p-\ndimensional space: flipping the sign has no effect as the direction does not\nchange. (Consider Figure 6.14—the principal component loading vector is\na line that extends in either direction, and flipping its sign would have no\neffect.)",
    "source": "book_islr",
    "section_title": "514 12. Unsupervised Learning"
  },
  {
    "instruction": "What is 1 p in data science?",
    "input": "",
    "output": "regression coefficient estimates. Unlike the simple linear regression esti-\nmates given in (3.4), the multiple regression coefficient estimates have\nsomewhat complicated forms that are most easily represented using ma-\ntrix algebra. For this reason, we do not provide them here.",
    "source": "book_islr",
    "section_title": "0 1 p"
  },
  {
    "instruction": "Explain exercises 397 in simple terms for a beginner in data science.",
    "input": "",
    "output": "(a) Generateadatasetwithn=500andp=2,suchthattheobser-\nvationsbelongtotwoclasseswithaquadraticdecisionboundary\nbetween them. For instance, you can do this as follows:\nrng = np.random.default_rng(5)\nx1 = rng.uniform(size=500) - 0.5\nx2 = rng.uniform(size=500) - 0.5\ny = x1**2 - x2**2 > 0\n(b) Plot the observations, colored according to their class labels. Your plot should display X on the x-axis, and X on the y-",
    "source": "book_islr",
    "section_title": "9.7 Exercises 397"
  },
  {
    "instruction": "What is = e in data science?",
    "input": "",
    "output": "(cid:0)\nvT (x−µ)\n(cid:1)2\n+\n(cid:0)\nvTµ\n(cid:1)2\nx∼p\n= σ2 +\n(cid:0)\nvTµ\n(cid:1)2\n60\nwhere the fourth line follows from the fact that E[vT(x−µ)] = 0, and the fifth line\nfollows from the fact that E[(vT(x−µ))2] is the variance in the direction v. The best fit\nline v maximizes E [(vTx)2] and therefore maximizes\n(cid:0)\nvTµ\n(cid:1)2\n. This is maximized when\nx∼p\nv is aligned with the center µ. To see uniqueness, just note that if µ (cid:54)= 0, then vTµ is\nstrictly less when v is not aligned with the center.",
    "source": "book_fods",
    "section_title": "= E"
  },
  {
    "instruction": "When or why would a data scientist use electrical networks and random walks?",
    "input": "",
    "output": "In the next few sections, we study the relationship between electrical networks and\nrandom walks on undirected graphs. The graphs have nonnegative weights on each edge. A step is executed by picking a random edge from the current vertex with probability\nproportional to the edge’s weight and traversing the edge. An electrical network is a connected, undirected graph in which each edge (x,y) has\na resistance r > 0. In what follows, it is easier to deal with conductance defined as the\nxy\nreciprocal of resistance, c = 1 , rather than resistance.",
    "source": "book_fods",
    "section_title": "4.5 Electrical Networks and Random Walks"
  },
  {
    "instruction": "When or why would a data scientist use 4?",
    "input": "",
    "output": "When we convolve the image with the filter, we get the result7\naα+bβ+dγ+eδ bα+cβ+eγ+fδ\nConvolvedImage= dα+eβ+gγ+hδ eα+fβ+hγ+iδ .  \ngα+hβ+jγ+kδ hα+iβ+kγ+lδ\n \nFor instance, the top-left element comes from multiplying each element in\nthe 2 2 filter by the corresponding element in the top left 2 2 portion\n× ×\nof the image, and adding the results. The other elements are obtained in a\nsimilarway:theconvolutionfilterisappliedtoevery2 2submatrixofthe\n×\noriginalimagein ordertoobtain theconvolvedimage.Ifa 2 2 submatrix\n×\nof the original image resembles the convolution filter, then it will have a\nlarge value in the convolved image; otherwise, it will have a small value. Thus, the convolved image highlights regions of the original image that\nresemble the convolution filter. We have used 2 2 as an example; in\n×\ngeneral convolution filters are small % % arrays, with % and % small",
    "source": "book_islr",
    "section_title": "3 4"
  },
  {
    "instruction": "How is regression models with a survival response 477 different from l = in data science?",
    "input": "",
    "output": "regression models with a survival response 477: whereT isthe(unobserved)survivaltime.Itisthedeathrateintheinstant\nafter time t, given survival past that time.7 In (11.9), we take the limit as\n∆t approaches zero, so we can think of ∆t as being an extremely tiny\nnumber. Thus, more informally, (11.9) implies that\nPr(t<T t+∆tT >t)\nh(t) ≤ |\n≈ ∆t\nfor some arbitrarily small ∆t. l =: i\n=S(y\ni\n) if the ith observation is censored\n=f(y i )δiS(y i )1 − δi. (11.12)\nTheintuitionbehind(11.12)isasfollows:ifY =y andtheithobservation\ni\nis not censored, then the likelihood is the probability of dying in a tiny in-\ntervalaroundtimey .Iftheithobservationiscensored,thenthelikelihood\ni\n7Duetothe∆tinthedenominatorof (11.9),thehazardfunctionisarateofdeath,\nrather than a probability of death.",
    "source": "book_islr",
    "section_title": "11.5 Regression Models With a Survival Response 477 vs L ="
  },
  {
    "instruction": "What is 2 n in data science?",
    "input": "",
    "output": "1. Give an algorithm that will select a symbol uniformly at random from the stream. How much memory does your algorithm require?",
    "source": "book_fods",
    "section_title": "1 2 n"
  },
  {
    "instruction": "Explain 4 in simple terms for a beginner in data science.",
    "input": "",
    "output": "When we convolve the image with the filter, we get the result7\naα+bβ+dγ+eδ bα+cβ+eγ+fδ\nConvolvedImage= dα+eβ+gγ+hδ eα+fβ+hγ+iδ .  \ngα+hβ+jγ+kδ hα+iβ+kγ+lδ\n \nFor instance, the top-left element comes from multiplying each element in\nthe 2 2 filter by the corresponding element in the top left 2 2 portion\n× ×\nof the image, and adding the results. The other elements are obtained in a\nsimilarway:theconvolutionfilterisappliedtoevery2 2submatrixofthe\n×\noriginalimagein ordertoobtain theconvolvedimage.Ifa 2 2 submatrix\n×\nof the original image resembles the convolution filter, then it will have a\nlarge value in the convolved image; otherwise, it will have a small value.",
    "source": "book_islr",
    "section_title": "3 4"
  },
  {
    "instruction": "When or why would a data scientist use f = − . (3.24)?",
    "input": "",
    "output": "RSS/(n p 1)\n− −\nNotice that in Table 3.4, for each individual predictor a t-statistic and\na p-value were reported. These provide information about whether each\nindividualpredictorisrelatedtotheresponse,afteradjustingfortheother\npredictors. It turns out that each of these is exactly equivalent7 to the F-\ntest that omits that single variable from the model, leaving all the others\nin—i.e.q=1in(3.24).Soitreportsthepartialeffectofaddingthatvariable\nto the model. For instance, as we discussed earlier, these p-values indicate\nthat TV and radio are related to sales, but that there is no evidence that\nnewspaper is associated with sales, when TV and radio are held fixed. Giventheseindividualp-valuesforeachvariable,whydoweneedtolook\nat the overall F-statistic?",
    "source": "book_islr",
    "section_title": "F = − . (3.24)"
  },
  {
    "instruction": "How is k different from 10 15 in data science?",
    "input": "",
    "output": "k: D = pˆ logpˆ . (8.7)\nmk mk\n−\nk=1\n0\nSince 0 pˆ 1, it follows that 0 pˆ logpˆ . 10 15: 6.0\n5.0\n4.0\n3.0\n2.0\n1.0\n0.0\nTree Size\nrorrE\nThal:a\n| Training\nCross−Validation\nTest\nCa < 0.5 Ca < 0.5\nMaxHR < 161.5 ChestPain:bc Yes Yes\nNo No\nNo Yes\nFIGURE 8.6.Heart data. Top: The unpruned tree.",
    "source": "book_islr",
    "section_title": "K vs 5 10 15"
  },
  {
    "instruction": "What is 4 in data science?",
    "input": "",
    "output": "no polylog length path. An algorithm for the r = 2 case\nFor r = 2, the local algorithm that selects the edge that ends closest to the destination\nt finds a path of expected length O(lnn)3. Suppose the algorithm is at a vertex u which\nis at distance k from t. Then within an expected O(lnn)2 steps, the algorithm reaches a\npoint at distance at most k/2.",
    "source": "book_fods",
    "section_title": "4 4"
  },
  {
    "instruction": "How is an uncertainty principle different from 1 n−1 0 1 n−1 in data science?",
    "input": "",
    "output": "an uncertainty principle: Given a function x(t), one can represent the function by the composition of sinusoidal\nfunctions. Basically one is representing the time function by its frequency components. 1 n−1 0 1 n−1: form is f = Ax with a = √ 1 ωij where ω is the principal nth root of unity. The inverse\nij n\ntransform is x = Bf where B = A−1 has the simple form b = √ 1 ω−ij.",
    "source": "book_fods",
    "section_title": "10.4 An Uncertainty Principle vs 0 1 n−1 0 1 n−1"
  },
  {
    "instruction": "Explain lab: deep learning in simple terms for a beginner in data science.",
    "input": "",
    "output": "In this section we demonstrate how to fit the examples discussed in the\ntext. We use the Python torch package, along with the pytorch_lightning\ntorch\npackage which provides utilities to simplify fitting and evaluating mod-\npytorch_\nels. This code can be impressively fast with certain special processors, lightning\nsuch as Apple’s new M1 chip.",
    "source": "book_islr",
    "section_title": "10.9 Lab: Deep Learning"
  },
  {
    "instruction": "When or why would a data scientist use the support vector machine?",
    "input": "",
    "output": "The support vector machine (SVM) is an extension of the support vector\nsupport\nclassifier that results from enlarging the feature space in a specific way,\nvector\nusing kernels. We will now discuss this extension, the details of which are machine\nsomewhat complex and beyond the scope of this book. However, the main kernel\nideaisdescribedinSection9.3.1:wemaywanttoenlargeourfeaturespace\nin order to accommodate a non-linear boundary between the classes. The\nkernel approach that we describe here is simply an efficient computational\napproach for enacting this idea. We have not discussed exactly how the support vector classifier is com-\nputed because the details become somewhat technical.",
    "source": "book_islr",
    "section_title": "9.3.2 The Support Vector Machine"
  },
  {
    "instruction": "Explain local spectral clustering in simple terms for a beginner in data science.",
    "input": "",
    "output": "So far our emphasis has been on partitioning the data into disjoint clusters. However,\nthe structure of many data sets consists of over lapping communities. In this case using\nk-means with spectral clustering, the overlap of two communities shows up as a commu-\nnity.",
    "source": "book_fods",
    "section_title": "7.5.5 Local spectral clustering"
  },
  {
    "instruction": "What is x 2 in data science?",
    "input": "",
    "output": "FIGURE 12.12. An illustration of how to properly interpret a dendrogram\nwith nine observations in two-dimensional space. Left: a dendrogram generated\nusing Euclidean distance and complete linkage.",
    "source": "book_islr",
    "section_title": "X 2"
  },
  {
    "instruction": "When or why would a data scientist use exercises?",
    "input": "",
    "output": "Exercise 12.1 What is the difference between saying f(n) is O(n3) and f(n) is o(n3)? Exercise 12.2 If f (n) ∼ g(n) what can we say about f(n)+g(n) and f(n)−g(n)? Exercise 12.3 What is the difference between ∼ and Θ? Exercise 12.4 If f (n) is O(g(n)) does this imply that g(n) is Ω(f (n))? Exercise 12.5 What is lim\n(cid:0)\nk−1\n(cid:1)k−2\n.",
    "source": "book_fods",
    "section_title": "12.11 Exercises"
  },
  {
    "instruction": "Explain mse in simple terms for a beginner in data science.",
    "input": "",
    "output": "Bias\nVar\nFlexibility\nFIGURE 2.12. Squared bias (blue curve), variance (orange curve), Var(!) (dashedline),andtestMSE(redcurve)forthethreedatasetsinFigures2.9–2.11.",
    "source": "book_islr",
    "section_title": "MSE"
  },
  {
    "instruction": "Explain assessing model accuracy in simple terms for a beginner in data science.",
    "input": "",
    "output": "Oneofthekeyaimsofthisbookistointroducethereadertoawiderange\nof statistical learning methods that extend far beyond the standard linear\nregression approach. Why is it necessary to introduce so many different\nstatisticallearningapproaches,ratherthanjustasinglebestmethod?There\nis no free lunch in statistics: no one method dominates all others over all\npossible data sets. On a particular data set, one specific method may work",
    "source": "book_islr",
    "section_title": "2.2 Assessing Model Accuracy"
  },
  {
    "instruction": "What is e maxpr(y =j x) , (2.11) in data science?",
    "input": "",
    "output": "− j |\n* +\nwhere the expectation averages the probability over all possible values of\nX. For our simulated data, the Bayes error rate is 0.133. It is greater than\nzero,becausetheclassesoverlapinthetruepopulation,whichimpliesthat\nmax Pr(Y =j X =x )<1 for some values of x .",
    "source": "book_islr",
    "section_title": "1 E maxPr(Y =j X) , (2.11)"
  },
  {
    "instruction": "Explain f 2 in simple terms for a beginner in data science.",
    "input": "",
    "output": "squared distances to cluster centers. We will see later that in many clustering problems, including models like mixtures of\nGaussians and Stochastic Block Models of communities, there is a desired clustering C\nwhere the regions overlap in the whole space, but are separated in the projection. Figure\n7.3 is a schematic illustration.",
    "source": "book_fods",
    "section_title": "F 2"
  },
  {
    "instruction": "When or why would a data scientist use 1?",
    "input": "",
    "output": "one neighbor one neighbor\nFigure 9.8: Portion of factor graph for the maximum weight matching problem. of times which depends on how deep a tree is built. Each occurrence of a variable such\nas c is deemed to be a distinct variable. 12\nLemma 9.14 If the tree obtained by unrolling the graph is of depth k, then the messages\nto the root are the same as the messages in the constraint graph after k-iterations. Proof: Straightforward.",
    "source": "book_fods",
    "section_title": "2 1"
  },
  {
    "instruction": "How is 2. statistical learning different from exercises 65 in data science?",
    "input": "",
    "output": "2. statistical learning: 4. You will now think of some real-life applications for statistical learn-\ning. exercises 65: Applied\n8. This exercise relates to the College data set, which can be found in\nthe file College.csv on the book website.",
    "source": "book_islr",
    "section_title": "64 2. Statistical Learning vs 2.4 Exercises 65"
  }
]