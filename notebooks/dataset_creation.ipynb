{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqqFgNeJ4RMp",
        "outputId": "f776afc3-9e3f-40bb-b95f-1bdfc7aa9249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting pdfminer.six==20251107 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20251107 pdfplumber-0.11.8 pypdfium2-5.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cid8UMPP4ior",
        "outputId": "0b141584-920b-46cf-9fe4-9ff462441bb7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    pages_text = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                pages_text.append(text)\n",
        "    return \"\\n\".join(pages_text)\n",
        "\n",
        "def basic_clean(text: str) -> str:\n",
        "    text = text.replace('\\r', '\\n')\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # collapse 3+ blank lines\n",
        "    return text\n",
        "\n",
        "def split_pdf_sections(clean_text, min_body_len=300):\n",
        "    \"\"\"\n",
        "    Split PDF text into (title, body) sections using heuristics for headings.\n",
        "    \"\"\"\n",
        "    lines = clean_text.split(\"\\n\")\n",
        "    sections = []\n",
        "    current_title = None\n",
        "    current_lines = []\n",
        "\n",
        "    def flush_section(title, buf):\n",
        "        text = \"\\n\".join(buf).strip()\n",
        "        if title and text and len(text) >= min_body_len:\n",
        "            sections.append((title.strip(), text))\n",
        "\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        # Heading heuristic: short-ish, all caps or numbered like \"2.3 Something\"\n",
        "        if (len(stripped) > 0\n",
        "            and len(stripped.split()) < 12\n",
        "            and (stripped.isupper() or re.match(r'^\\d+(\\.\\d+)*\\s+', stripped))):\n",
        "            flush_section(current_title, current_lines)\n",
        "            current_title = stripped\n",
        "            current_lines = []\n",
        "        else:\n",
        "            if stripped != \"\":\n",
        "                current_lines.append(stripped)\n",
        "\n",
        "    flush_section(current_title, current_lines)\n",
        "    return sections\n",
        "\n",
        "def split_markdown_sections(markdown_text, min_body_len=300):\n",
        "    \"\"\"\n",
        "    Split markdown into (title, body) using headings starting with '#', '##', etc.\n",
        "    \"\"\"\n",
        "    lines = markdown_text.split(\"\\n\")\n",
        "    sections = []\n",
        "    current_title = None\n",
        "    current_lines = []\n",
        "\n",
        "    def flush_section(title, buf):\n",
        "        text = \"\\n\".join(buf).strip()\n",
        "        if title and text and len(text) >= min_body_len:\n",
        "            sections.append((title.strip(), text))\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip().startswith(\"#\"):\n",
        "            # new heading\n",
        "            flush_section(current_title, current_lines)\n",
        "            current_title = re.sub(r'^#+', '', line).strip()\n",
        "            current_lines = []\n",
        "        else:\n",
        "            if line.strip() != \"\":\n",
        "                current_lines.append(line.strip())\n",
        "\n",
        "    flush_section(current_title, current_lines)\n",
        "    return sections\n",
        "\n",
        "def first_n_sentences(text, n=3):\n",
        "    sents = sent_tokenize(text)\n",
        "    return \" \".join(sents[:n])\n",
        "\n",
        "def normalize_title_for_question(title: str) -> str:\n",
        "    t = re.sub(r'^\\d+(\\.\\d+)*\\s*', '', title).strip()\n",
        "    return t.lower()\n",
        "\n",
        "def qa_from_sections(sections, source_tag):\n",
        "    qa_pairs = []\n",
        "    for title, body in sections:\n",
        "        if len(body) < 300:\n",
        "            continue\n",
        "        clean_title = normalize_title_for_question(title)\n",
        "        intro = first_n_sentences(body, n=3)\n",
        "        if len(intro) < 50:\n",
        "            continue\n",
        "\n",
        "        # Q1: What is X?\n",
        "        qa_pairs.append({\n",
        "            \"instruction\": f\"What is {clean_title} in data science?\",\n",
        "            \"input\": \"\",\n",
        "            \"output\": intro,\n",
        "            \"source\": source_tag,\n",
        "            \"section_title\": title\n",
        "        })\n",
        "\n",
        "        # Q2: Explain X simply\n",
        "        qa_pairs.append({\n",
        "            \"instruction\": f\"Explain {clean_title} in simple terms for a beginner in data science.\",\n",
        "            \"input\": \"\",\n",
        "            \"output\": intro,\n",
        "            \"source\": source_tag,\n",
        "            \"section_title\": title\n",
        "        })\n",
        "\n",
        "        # Q3: When / why is X used?\n",
        "        more = first_n_sentences(body, n=5)\n",
        "        qa_pairs.append({\n",
        "            \"instruction\": f\"When or why would a data scientist use {clean_title}?\",\n",
        "            \"input\": \"\",\n",
        "            \"output\": more,\n",
        "            \"source\": source_tag,\n",
        "            \"section_title\": title\n",
        "        })\n",
        "\n",
        "        # Q4: steps / workflow if algorithmic\n",
        "        if any(k in body.lower() for k in [\"algorithm\", \"procedure\", \"steps\", \"workflow\", \"process\"]):\n",
        "            algo_ans = first_n_sentences(body, n=6)\n",
        "            qa_pairs.append({\n",
        "                \"instruction\": f\"Describe the typical steps involved in {clean_title} in a data science workflow.\",\n",
        "                \"input\": \"\",\n",
        "                \"output\": algo_ans,\n",
        "                \"source\": source_tag,\n",
        "                \"section_title\": title\n",
        "            })\n",
        "\n",
        "    # Comparison questions between neighboring sections\n",
        "    for i in range(1, len(sections)):\n",
        "        t1, b1 = sections[i-1]\n",
        "        t2, b2 = sections[i]\n",
        "        c1 = normalize_title_for_question(t1)\n",
        "        c2 = normalize_title_for_question(t2)\n",
        "        ans1 = first_n_sentences(b1, 2)\n",
        "        ans2 = first_n_sentences(b2, 2)\n",
        "        if len(ans1) < 40 or len(ans2) < 40:\n",
        "            continue\n",
        "        qa_pairs.append({\n",
        "            \"instruction\": f\"How is {c1} different from {c2} in data science?\",\n",
        "            \"input\": \"\",\n",
        "            \"output\": f\"{c1}: {ans1} {c2}: {ans2}\",\n",
        "            \"source\": source_tag,\n",
        "            \"section_title\": f\"{t1} vs {t2}\"\n",
        "        })\n",
        "\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "Lkx0gOvN4kny"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_qa = []\n",
        "\n",
        "# ISLR\n",
        "print(\"Processing ISLR (islr.pdf)...\")\n",
        "islr_raw = extract_text_from_pdf(\"ISLP_website.pdf\")\n",
        "islr_clean = basic_clean(islr_raw)\n",
        "islr_sections = split_pdf_sections(islr_clean, min_body_len=300)\n",
        "print(f\"  -> {len(islr_sections)} sections\")\n",
        "islr_qa = qa_from_sections(islr_sections, source_tag=\"book_islr\")\n",
        "print(f\"  -> {len(islr_qa)} QA pairs\")\n",
        "all_qa.extend(islr_qa)\n",
        "\n",
        "# FODS\n",
        "print(\"\\nProcessing Foundations of Data Science (fods.pdf)...\")\n",
        "fods_raw = extract_text_from_pdf(\"Foundations of Data Science.pdf\")\n",
        "fods_clean = basic_clean(fods_raw)\n",
        "fods_sections = split_pdf_sections(fods_clean, min_body_len=300)\n",
        "print(f\"  -> {len(fods_sections)} sections\")\n",
        "fods_qa = qa_from_sections(fods_sections, source_tag=\"book_fods\")\n",
        "print(f\"  -> {len(fods_qa)} QA pairs\")\n",
        "all_qa.extend(fods_qa)\n",
        "\n",
        "print(\"\\nQA count so far:\", len(all_qa))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82eKVcdz5oox",
        "outputId": "b94b1cd3-f236-4787-996b-ef31a685f5fd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ISLR (islr.pdf)...\n",
            "  -> 1092 sections\n",
            "  -> 4447 QA pairs\n",
            "\n",
            "Processing Foundations of Data Science (fods.pdf)...\n",
            "  -> 721 sections\n",
            "  -> 3044 QA pairs\n",
            "\n",
            "QA count so far: 7491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia\n",
        "import wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JiOpI1n5yRK",
        "outputId": "b8d7ab6a-bd46-4d34-dbf6-f1af6b98ffa6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (4.15.0)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=baadb640deab3bf4f5365208b55d84e03c20a165cae104e32c6ee9a737f78532\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_titles = [\n",
        "    \"Data science\",\n",
        "    \"Machine learning\",\n",
        "    \"Overfitting\",\n",
        "    \"Cross-validation (statistics)\",\n",
        "    \"Logistic regression\",\n",
        "    \"Bias–variance tradeoff\",\n",
        "    \"Random forest\"\n",
        "]"
      ],
      "metadata": {
        "id": "chUdc_pe9bDh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_sections = []\n",
        "\n",
        "for title in wiki_titles:\n",
        "    try:\n",
        "        page = wikipedia.page(title, auto_suggest=False)\n",
        "        text = basic_clean(page.content)\n",
        "        # split on headings like \"== Something ==\"\n",
        "        chunks = [c.strip() for c in re.split(r'\\n==[^=]+==\\n', text) if len(c.strip()) > 300]\n",
        "        # first chunk is usually intro/definition\n",
        "        for i, ch in enumerate(chunks):\n",
        "            sec_title = f\"{title} part {i+1}\"\n",
        "            wiki_sections.append((sec_title, ch))\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {title}: {e}\")\n",
        "\n",
        "print(\"Wikipedia sections:\", len(wiki_sections))\n",
        "wiki_qa = qa_from_sections(wiki_sections, source_tag=\"web_wikipedia\")\n",
        "print(\"Wikipedia QA pairs:\", len(wiki_qa))\n",
        "\n",
        "all_qa.extend(wiki_qa)\n",
        "print(\"Total QA pairs after adding Wikipedia:\", len(all_qa))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N-vcqA19h_P",
        "outputId": "56c5909c-c676-4809-9555-a07387bdc839"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wikipedia sections: 67\n",
            "Wikipedia QA pairs: 310\n",
            "Total QA pairs after adding Wikipedia: 7801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv\n",
        "import arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXa4qube9j3-",
        "outputId": "dcca9e71-c754-46c2-e38a-179738a89519"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.12/dist-packages (from arxiv) (2.32.4)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2025.11.12)\n",
            "Downloading arxiv-2.3.1-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=947b23d92095becc2943415ae62f132881f04bf79faa1524a513f35a6a059087\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.3.1 feedparser-6.0.12 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search = arxiv.Search(\n",
        "    query=\"machine learning survey OR data science introduction\",\n",
        "    max_results=20,\n",
        "    sort_by=arxiv.SortCriterion.Relevance\n",
        ")\n",
        "\n",
        "arxiv_sections = []\n",
        "\n",
        "for result in search.results():\n",
        "    title = result.title.strip()\n",
        "    abstract = result.summary.strip()\n",
        "    if len(abstract) < 300:\n",
        "        continue\n",
        "    text = basic_clean(abstract)\n",
        "    arxiv_sections.append((title, text))\n",
        "\n",
        "print(\"arXiv sections:\", len(arxiv_sections))\n",
        "arxiv_qa = qa_from_sections(arxiv_sections, source_tag=\"web_arxiv\")\n",
        "print(\"arXiv QA pairs:\", len(arxiv_qa))\n",
        "\n",
        "all_qa.extend(arxiv_qa)\n",
        "print(\"Total QA pairs after adding arXiv:\", len(all_qa))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu8evBhK9mbO",
        "outputId": "f500bf2e-da3f-4919-be5f-e28b9a99af4d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4278955223.py:9: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arXiv sections: 20\n",
            "arXiv QA pairs: 89\n",
            "Total QA pairs after adding arXiv: 7890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "print(\"Sample QA pairs:\\n\")\n",
        "for ex in random.sample(all_qa, min(5, len(all_qa))):\n",
        "    print(\"Source:\", ex[\"source\"])\n",
        "    print(\"Section:\", ex[\"section_title\"])\n",
        "    print(\"Q:\", ex[\"instruction\"])\n",
        "    print(\"A:\", ex[\"output\"][:300], \"...\\n---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlRwNqih9qPv",
        "outputId": "165a48e5-4937-40c1-db4e-e3179bd751c1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample QA pairs:\n",
            "\n",
            "Source: book_islr\n",
            "Section: 524 12. Unsupervised Learning\n",
            "Q: When or why would a data scientist use 12. unsupervised learning?\n",
            "A: Data Step 1 Iteration 1, Step 2a\n",
            "Iteration 1, Step 2b Iteration 2, Step 2a Final Results\n",
            "FIGURE 12.8. The progress of the K-means algorithm on the example of\n",
            "Figure 12.7 with K=3. Top left: the observations are shown. Top center: in\n",
            "Step 1 of the algorithm, each observation is randomly assigned to a ...\n",
            "---\n",
            "\n",
            "Source: book_fods\n",
            "Section: 1 1\n",
            "Q: Explain 1 in simple terms for a beginner in data science.\n",
            "A: A = L + R and UΣVT being the singular value decomposition of A. This can be done\n",
            "using Lagrange multipliers (??). Write R = R+ +R− where R+ ≥ 0 and R− ≥ 0. ...\n",
            "---\n",
            "\n",
            "Source: book_fods\n",
            "Section: 2 i\n",
            "Q: When or why would a data scientist use i?\n",
            "A: p ji 1 2 0.85π i π i = 0.85π j p ji + 0. 2 85π i\n",
            "j i\n",
            "π = 1.48π p\n",
            "i j ji\n",
            "0.15π 0.15π\n",
            "j i\n",
            "Figure 4.13: Impact on pagerank of adding a self loop\n",
            "list of webpages in response to each search query. To do this, they have to solve two\n",
            "problems at query time: (i) find the set of all webpages containing the  ...\n",
            "---\n",
            "\n",
            "Source: web_wikipedia\n",
            "Section: Machine learning part 13 vs Machine learning part 14\n",
            "Q: How is machine learning part 13 different from machine learning part 14 in data science?\n",
            "A: machine learning part 13: Automated machine learning – Process of automating the application of machine learning\n",
            "Big data – Extremely large or complex datasets\n",
            "Deep learning — branch of ML concerned with artificial neural networks\n",
            "Differentiable programming – Programming paradigm\n",
            "List of datasets fo ...\n",
            "---\n",
            "\n",
            "Source: book_islr\n",
            "Section: 2 5 10 20\n",
            "Q: When or why would a data scientist use 5 10 20?\n",
            "A: 5.2\n",
            "0.2\n",
            "5.1\n",
            "0.1\n",
            "5.0\n",
            "0.0\n",
            "Flexibility\n",
            "rorrE\n",
            "derauqS\n",
            "naeM\n",
            "FIGURE 2.9. Left: Data simulated from f, shown in black. Three estimates of\n",
            "f are shown: the linear regression line (orange curve), and two smoothing spline\n",
            "fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red\n",
            "curve), an ...\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total QA pairs:\", len(all_qa))\n",
        "\n",
        "train, test = train_test_split(all_qa, test_size=0.1, random_state=42)\n",
        "train, val  = train_test_split(train,   test_size=0.1, random_state=42)\n",
        "\n",
        "print(\"Train:\", len(train), \"Val:\", len(val), \"Test:\", len(test))\n",
        "\n",
        "with open(\"ds_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(train, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "with open(\"ds_val.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "with open(\"ds_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(test, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Saved ds_train.json, ds_val.json, ds_test.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IElfLoFk91CW",
        "outputId": "02389901-e216-4d95-c264-683d35b51574"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total QA pairs: 7890\n",
            "Train: 6390 Val: 711 Test: 789\n",
            "Saved ds_train.json, ds_val.json, ds_test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7f1GaRXp-Zs-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}