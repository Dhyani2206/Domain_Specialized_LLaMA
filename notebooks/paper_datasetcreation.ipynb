{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "014406cf78574ef2bc80972540968c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_219b48bd08b747c0ab32e5450b066d53",
              "IPY_MODEL_e6c313878eb746d7bf56b22de26c2290",
              "IPY_MODEL_530a53ac36ee4f59b28bb3b697cd7f46"
            ],
            "layout": "IPY_MODEL_0d146003a2894c00b1a160ad015b49fb"
          }
        },
        "219b48bd08b747c0ab32e5450b066d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44f7e3a837fc4e08a230181d96674107",
            "placeholder": "​",
            "style": "IPY_MODEL_d8b21b0f26fb44f1a28d45fe9388f844",
            "value": "README.md: "
          }
        },
        "e6c313878eb746d7bf56b22de26c2290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dfcb71b959e4944b92185e46d3cd47f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e92b6c929f9496d9dfd638459c422a3",
            "value": 1
          }
        },
        "530a53ac36ee4f59b28bb3b697cd7f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a411b7ad5cb34dda92484c820dbf7aef",
            "placeholder": "​",
            "style": "IPY_MODEL_86708720e11944b09a487ca874d4c61a",
            "value": " 1.33k/? [00:00&lt;00:00, 131kB/s]"
          }
        },
        "0d146003a2894c00b1a160ad015b49fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44f7e3a837fc4e08a230181d96674107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8b21b0f26fb44f1a28d45fe9388f844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dfcb71b959e4944b92185e46d3cd47f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7e92b6c929f9496d9dfd638459c422a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a411b7ad5cb34dda92484c820dbf7aef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86708720e11944b09a487ca874d4c61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI2BllFmeLnv",
        "outputId": "904b2765-a5d4-45ee-afc7-e150ead1f67b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.12/dist-packages (2.3.1)\n",
            "Requirement already satisfied: stackapi in /usr/local/lib/python3.12/dist-packages (0.3.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.12/dist-packages (0.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.12/dist-packages (from arxiv) (6.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from stackapi) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=ebb8ea12136b7a490c82885e504d32b153cbe6132ccf32054d6be2b24e2693c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets arxiv stackapi bs4 nltk transformers wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "import arxiv\n",
        "from stackapi import StackAPI\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import wikipedia"
      ],
      "metadata": {
        "id": "rPREtX9peZ-R"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSUtb6VhfTuL",
        "outputId": "e443bb60-d14d-4231-8c63-87a1b7571cb6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "def chunk_text(text: str, max_length: int = 1000) -> List[str]:\n",
        "    \"\"\"Split text into chunks of approximately max_length characters\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent_len = len(sent)\n",
        "        if current_length + sent_len > max_length and current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sent]\n",
        "            current_length = sent_len\n",
        "        else:\n",
        "            current_chunk.append(sent)\n",
        "            current_length += sent_len\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_qa_pairs(text: str, context: str = \"\", source: str = \"\") -> List[Dict]:\n",
        "    \"\"\"Generate QA pairs from text using various templates\"\"\"\n",
        "    chunks = chunk_text(text, max_length=800)\n",
        "    qa_pairs = []\n",
        "\n",
        "    templates = [\n",
        "        \"Explain the following concept: \",\n",
        "        \"What does this mean: \",\n",
        "        \"Can you summarize this: \",\n",
        "        \"Provide more details about: \",\n",
        "        \"What is the main idea of: \"\n",
        "    ]\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if len(chunk) < 100:  # Skip very short chunks\n",
        "            continue\n",
        "\n",
        "        # Extract first sentence as potential topic\n",
        "        first_sent = sent_tokenize(chunk)[0] if sent_tokenize(chunk) else chunk[:100]\n",
        "\n",
        "        qa_pairs.append({\n",
        "            \"instruction\": random.choice(templates) + first_sent[:100],\n",
        "            \"input\": \"\",\n",
        "            \"output\": chunk,\n",
        "            \"source\": source,\n",
        "            \"context\": context\n",
        "        })\n",
        "\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "0eWRaND3fVXF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_common_crawl(num_samples: int = 1000) -> List[Dict]:\n",
        "    \"\"\"Fetch data from Common Crawl via HuggingFace datasets\"\"\"\n",
        "    print(f\"Fetching {num_samples} samples from Common Crawl...\")\n",
        "    qa_pairs = []\n",
        "\n",
        "    try:\n",
        "        # Using alternative cleaned web datasets\n",
        "        # Option 1: Try RedPajama\n",
        "        try:\n",
        "            dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", split=\"train\", streaming=True)\n",
        "            count = 0\n",
        "            for item in dataset:\n",
        "                if count >= num_samples:\n",
        "                    break\n",
        "\n",
        "                text = clean_text(item['text'])\n",
        "                if len(text) > 200:\n",
        "                    pairs = create_qa_pairs(\n",
        "                        text[:2000],\n",
        "                        context=\"web_content\",\n",
        "                        source=\"common_crawl\"\n",
        "                    )\n",
        "                    qa_pairs.extend(pairs)\n",
        "                    count += 1\n",
        "\n",
        "                if count % 100 == 0:\n",
        "                    print(f\"  Processed {count}/{num_samples} Common Crawl samples\")\n",
        "\n",
        "            print(f\"  Generated {len(qa_pairs)} QA pairs from Common Crawl (RedPajama)\")\n",
        "            return qa_pairs\n",
        "\n",
        "        except:\n",
        "            # Option 2: Try Wikipedia dataset as fallback\n",
        "            print(\"  RedPajama unavailable, using Wikipedia dataset as web content alternative...\")\n",
        "            dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True)\n",
        "            count = 0\n",
        "            for item in dataset:\n",
        "                if count >= num_samples:\n",
        "                    break\n",
        "\n",
        "                text = clean_text(item['text'])\n",
        "                if len(text) > 200:\n",
        "                    pairs = create_qa_pairs(\n",
        "                        text[:2000],\n",
        "                        context=\"encyclopedic_content\",\n",
        "                        source=\"common_crawl_alt\"\n",
        "                    )\n",
        "                    qa_pairs.extend(pairs)\n",
        "                    count += 1\n",
        "\n",
        "                if count % 100 == 0:\n",
        "                    print(f\"  Processed {count}/{num_samples} samples\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error fetching Common Crawl alternatives: {e}\")\n",
        "        print(\"  Skipping Common Crawl - other sources will compensate\")\n",
        "\n",
        "    print(f\"  Generated {len(qa_pairs)} QA pairs from web sources\")\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "ZWdqjEXlfw6u"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_stack_exchange(num_questions: int = 500) -> List[Dict]:\n",
        "    \"\"\"Fetch Q&A from Stack Exchange using HuggingFace dataset (no rate limits)\"\"\"\n",
        "    print(f\"Fetching {num_questions} questions from Stack Exchange...\")\n",
        "    qa_pairs = []\n",
        "\n",
        "    try:\n",
        "        # Use a working Stack Overflow dataset\n",
        "        dataset = load_dataset(\"koutch/stackoverflow_python\", split=\"train\", streaming=True)\n",
        "\n",
        "        count = 0\n",
        "        items_checked = 0\n",
        "        max_items_to_check = num_questions * 5  # Check at most 5x the target to avoid infinite loops\n",
        "\n",
        "        for item in dataset:\n",
        "            items_checked += 1\n",
        "\n",
        "            if count >= num_questions or items_checked >= max_items_to_check:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                question = item.get('question', '')\n",
        "                answer = item.get('answer', '')\n",
        "\n",
        "                if len(question) > 50 and len(answer) > 100:\n",
        "                    qa_pairs.append({\n",
        "                        \"instruction\": clean_text(question)[:500],\n",
        "                        \"input\": \"\",\n",
        "                        \"output\": clean_text(answer)[:1000],\n",
        "                        \"source\": \"stack_overflow\",\n",
        "                        \"context\": \"programming_qa\"\n",
        "                    })\n",
        "                    count += 1\n",
        "\n",
        "                if count % 100 == 0 and count > 0:\n",
        "                    print(f\"  Processed {count}/{num_questions} Stack Overflow QA pairs\")\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(f\"  Checked {items_checked} items, collected {count} valid QA pairs\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error with Stack Overflow dataset: {e}\")\n",
        "        print(f\"  Skipping Stack Exchange - will rely on other sources\")\n",
        "\n",
        "    print(f\"  Total Stack Exchange QA pairs: {len(qa_pairs)}\")\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "E1Vve2Ecf6v8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_arxiv(num_papers: int = 200) -> List[Dict]:\n",
        "    \"\"\"Fetch papers from arXiv using the new Client API\"\"\"\n",
        "    print(f\"Fetching {num_papers} papers from arXiv...\")\n",
        "    qa_pairs = []\n",
        "\n",
        "    # Diverse search queries for general knowledge\n",
        "    queries = [\n",
        "        \"machine learning\",\n",
        "        \"computer science theory\",\n",
        "        \"mathematics\",\n",
        "        \"physics\",\n",
        "        \"artificial intelligence\",\n",
        "        \"machinen learnn=ing algorithms\",\n",
        "        \"genAI\"\n",
        "    ]\n",
        "\n",
        "    papers_per_query = num_papers // len(queries)\n",
        "\n",
        "    # Use the new Client API\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    for query in queries:\n",
        "        try:\n",
        "            search = arxiv.Search(\n",
        "                query=query,\n",
        "                max_results=papers_per_query,\n",
        "                sort_by=arxiv.SortCriterion.Relevance\n",
        "            )\n",
        "\n",
        "            for result in client.results(search):\n",
        "                title = clean_text(result.title)\n",
        "                abstract = clean_text(result.summary)\n",
        "\n",
        "                if len(abstract) > 200:\n",
        "                    # Create QA pairs from abstract\n",
        "                    qa_pairs.append({\n",
        "                        \"instruction\": f\"Explain the research paper: {title}\",\n",
        "                        \"input\": \"\",\n",
        "                        \"output\": abstract,\n",
        "                        \"source\": \"arxiv\",\n",
        "                        \"context\": f\"category_{result.primary_category}\"\n",
        "                    })\n",
        "\n",
        "                    # Create summary QA\n",
        "                    qa_pairs.append({\n",
        "                        \"instruction\": f\"What is the main contribution of this paper: {title}\",\n",
        "                        \"input\": abstract[:300],\n",
        "                        \"output\": abstract[300:600] if len(abstract) > 300 else abstract,\n",
        "                        \"source\": \"arxiv\",\n",
        "                        \"context\": f\"category_{result.primary_category}\"\n",
        "                    })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error with query '{query}': {e}\")\n",
        "\n",
        "    print(f\"  Generated {len(qa_pairs)} QA pairs from arXiv\")\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "GMeqVtCDf92_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wikipedia(num_articles: int = 300) -> List[Dict]:\n",
        "    \"\"\"Fetch articles from Wikipedia\"\"\"\n",
        "    print(f\"Fetching {num_articles} articles from Wikipedia...\")\n",
        "    qa_pairs = []\n",
        "\n",
        "    # Diverse categories for general knowledge\n",
        "    categories = [\n",
        "        # Science & Technology\n",
        "        \"Artificial intelligence\", \"Computer science\", \"Physics\", \"Biology\",\n",
        "        \"Chemistry\", \"Mathematics\", \"Engineering\", \"Medicine\",\n",
        "        # History & Geography\n",
        "        \"World history\", \"Ancient history\", \"Geography\", \"Countries\",\n",
        "        # Arts & Culture\n",
        "        \"Literature\", \"Music\", \"Art\", \"Philosophy\", \"Religion\",\n",
        "        # Social Sciences\n",
        "        \"Psychology\", \"Economics\", \"Sociology\", \"Political science\",\n",
        "        # General\n",
        "        \"Science\", \"Technology\", \"History\", \"Culture\"\n",
        "    ]\n",
        "\n",
        "    articles_per_category = num_articles // len(categories)\n",
        "    processed_titles = set()  # Avoid duplicates\n",
        "\n",
        "    for category in categories:\n",
        "        try:\n",
        "            # Search for articles related to the category\n",
        "            search_results = wikipedia.search(category, results=articles_per_category + 10)\n",
        "\n",
        "            count = 0\n",
        "            for title in search_results:\n",
        "                if count >= articles_per_category:\n",
        "                    break\n",
        "\n",
        "                if title in processed_titles:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    page = wikipedia.page(title, auto_suggest=False)\n",
        "                    content = clean_text(page.content)\n",
        "                    summary = clean_text(page.summary)\n",
        "\n",
        "                    processed_titles.add(title)\n",
        "\n",
        "                    # Create QA from summary\n",
        "                    if len(summary) > 200:\n",
        "                        qa_pairs.append({\n",
        "                            \"instruction\": f\"What is {title}?\",\n",
        "                            \"input\": \"\",\n",
        "                            \"output\": summary,\n",
        "                            \"source\": \"wikipedia\",\n",
        "                            \"context\": f\"category_{category.replace(' ', '_')}\"\n",
        "                        })\n",
        "\n",
        "                        qa_pairs.append({\n",
        "                            \"instruction\": f\"Explain {title} in detail.\",\n",
        "                            \"input\": \"\",\n",
        "                            \"output\": summary,\n",
        "                            \"source\": \"wikipedia\",\n",
        "                            \"context\": f\"category_{category.replace(' ', '_')}\"\n",
        "                        })\n",
        "\n",
        "                    # Create QA from content sections\n",
        "                    # Split content into paragraphs\n",
        "                    paragraphs = [p.strip() for p in content.split('\\n\\n') if len(p.strip()) > 300]\n",
        "\n",
        "                    # Take a few paragraphs from the article\n",
        "                    for i, para in enumerate(paragraphs[:3]):\n",
        "                        if len(para) > 200:\n",
        "                            # Extract first sentence as potential question topic\n",
        "                            first_sent = sent_tokenize(para)[0] if sent_tokenize(para) else para[:100]\n",
        "\n",
        "                            qa_pairs.append({\n",
        "                                \"instruction\": f\"Tell me about {first_sent[:80]}\",\n",
        "                                \"input\": \"\",\n",
        "                                \"output\": para[:1000],  # Limit length\n",
        "                                \"source\": \"wikipedia\",\n",
        "                                \"context\": f\"{title}_section_{i}\"\n",
        "                            })\n",
        "\n",
        "                    count += 1\n",
        "\n",
        "                except wikipedia.exceptions.DisambiguationError as e:\n",
        "                    # If disambiguation page, try first option\n",
        "                    try:\n",
        "                        if e.options:\n",
        "                            page = wikipedia.page(e.options[0], auto_suggest=False)\n",
        "                            summary = clean_text(page.summary)\n",
        "                            if len(summary) > 200:\n",
        "                                qa_pairs.append({\n",
        "                                    \"instruction\": f\"What is {e.options[0]}?\",\n",
        "                                    \"input\": \"\",\n",
        "                                    \"output\": summary,\n",
        "                                    \"source\": \"wikipedia\",\n",
        "                                    \"context\": f\"category_{category.replace(' ', '_')}\"\n",
        "                                })\n",
        "                                count += 1\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                except wikipedia.exceptions.PageError:\n",
        "                    continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            print(f\"  Processed {count} articles from category '{category}'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error with category '{category}': {e}\")\n",
        "\n",
        "    print(f\"  Generated {len(qa_pairs)} QA pairs from Wikipedia\")\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "2-blzk2SgRS8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GENERALIZED DATASET CREATION FOR LLM TRAINING\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    all_qa = []\n",
        "\n",
        "\n",
        "    # 1. Common Crawl / Web Content (using alternative datasets)\n",
        "    cc_data = fetch_common_crawl(num_samples=300)\n",
        "    all_qa.extend(cc_data)\n",
        "\n",
        "    # 2. Stack Exchange (using HuggingFace datasets to avoid rate limits)\n",
        "    se_data = fetch_stack_exchange(num_questions=800)\n",
        "    all_qa.extend(se_data)\n",
        "\n",
        "    # 3. arXiv (academic papers)\n",
        "    arxiv_data = fetch_arxiv(num_papers=900)\n",
        "    all_qa.extend(arxiv_data)\n",
        "\n",
        "    # 4. Wikipedia (encyclopedic knowledge)\n",
        "    wiki_data = fetch_wikipedia(num_articles=300)\n",
        "    all_qa.extend(wiki_data)\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"TOTAL QA PAIRS COLLECTED: {len(all_qa)}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Distribution stats\n",
        "    sources = {}\n",
        "    for qa in all_qa:\n",
        "        src = qa['source']\n",
        "        sources[src] = sources.get(src, 0) + 1\n",
        "\n",
        "    print(\"\\nData distribution by source:\")\n",
        "    for src, count in sorted(sources.items()):\n",
        "        print(f\"  {src}: {count} pairs ({count/len(all_qa)*100:.1f}%)\")\n",
        "\n",
        "    # Split into train/val/test\n",
        "    train, test = train_test_split(all_qa, test_size=0.1, random_state=42)\n",
        "    train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
        "\n",
        "    print(f\"\\nSplit sizes:\")\n",
        "    print(f\"  Train: {len(train)} ({len(train)/len(all_qa)*100:.1f}%)\")\n",
        "    print(f\"  Validation: {len(val)} ({len(val)/len(all_qa)*100:.1f}%)\")\n",
        "    print(f\"  Test: {len(test)} ({len(test)/len(all_qa)*100:.1f}%)\")\n",
        "\n",
        "    # Save datasets\n",
        "    with open(\"general_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(train, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    with open(\"general_val.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(val, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    with open(\"general_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(test, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\n✓ Saved: general_train.json, general_val.json, general_test.json\")\n",
        "\n",
        "    # Show sample\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SAMPLE QA PAIRS:\")\n",
        "    print(\"=\" * 60)\n",
        "    for qa in random.sample(all_qa, min(5, len(all_qa))):\n",
        "        print(f\"\\nSource: {qa['source']}\")\n",
        "        print(f\"Context: {qa['context']}\")\n",
        "        print(f\"Q: {qa['instruction'][:100]}...\")\n",
        "        print(f\"A: {qa['output'][:200]}...\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "014406cf78574ef2bc80972540968c62",
            "219b48bd08b747c0ab32e5450b066d53",
            "e6c313878eb746d7bf56b22de26c2290",
            "530a53ac36ee4f59b28bb3b697cd7f46",
            "0d146003a2894c00b1a160ad015b49fb",
            "44f7e3a837fc4e08a230181d96674107",
            "d8b21b0f26fb44f1a28d45fe9388f844",
            "9dfcb71b959e4944b92185e46d3cd47f",
            "7e92b6c929f9496d9dfd638459c422a3",
            "a411b7ad5cb34dda92484c820dbf7aef",
            "86708720e11944b09a487ca874d4c61a"
          ]
        },
        "id": "vm4nB6gZgevc",
        "outputId": "52e1d15a-00d9-4e21-d2af-e8569dd7502f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GENERALIZED DATASET CREATION FOR LLM TRAINING\n",
            "============================================================\n",
            "Fetching 300 samples from Common Crawl...\n",
            "  RedPajama unavailable, using Wikipedia dataset as web content alternative...\n",
            "  Error fetching Common Crawl alternatives: Dataset scripts are no longer supported, but found wikipedia.py\n",
            "  Skipping Common Crawl - other sources will compensate\n",
            "  Generated 0 QA pairs from web sources\n",
            "Fetching 800 questions from Stack Exchange...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "014406cf78574ef2bc80972540968c62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Checked 4000 items, collected 0 valid QA pairs\n",
            "  Total Stack Exchange QA pairs: 0\n",
            "Fetching 900 papers from arXiv...\n",
            "  Generated 1748 QA pairs from arXiv\n",
            "Fetching 300 articles from Wikipedia...\n",
            "  Processed 12 articles from category 'Artificial intelligence'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processed 12 articles from category 'Computer science'\n",
            "  Processed 12 articles from category 'Physics'\n",
            "  Processed 12 articles from category 'Biology'\n",
            "  Processed 12 articles from category 'Chemistry'\n",
            "  Processed 12 articles from category 'Mathematics'\n",
            "  Processed 12 articles from category 'Engineering'\n",
            "  Processed 12 articles from category 'Medicine'\n",
            "  Processed 12 articles from category 'World history'\n",
            "  Processed 12 articles from category 'Ancient history'\n",
            "  Processed 12 articles from category 'Geography'\n",
            "  Processed 12 articles from category 'Countries'\n",
            "  Processed 12 articles from category 'Literature'\n",
            "  Processed 12 articles from category 'Music'\n",
            "  Processed 12 articles from category 'Art'\n",
            "  Processed 12 articles from category 'Philosophy'\n",
            "  Processed 12 articles from category 'Religion'\n",
            "  Processed 12 articles from category 'Psychology'\n",
            "  Processed 12 articles from category 'Economics'\n",
            "  Processed 12 articles from category 'Sociology'\n",
            "  Processed 12 articles from category 'Political science'\n",
            "  Processed 12 articles from category 'Science'\n",
            "  Processed 12 articles from category 'Technology'\n",
            "  Processed 12 articles from category 'History'\n",
            "  Processed 12 articles from category 'Culture'\n",
            "  Generated 839 QA pairs from Wikipedia\n",
            "\n",
            "============================================================\n",
            "TOTAL QA PAIRS COLLECTED: 2587\n",
            "============================================================\n",
            "\n",
            "Data distribution by source:\n",
            "  arxiv: 1748 pairs (67.6%)\n",
            "  wikipedia: 839 pairs (32.4%)\n",
            "\n",
            "Split sizes:\n",
            "  Train: 2095 (81.0%)\n",
            "  Validation: 233 (9.0%)\n",
            "  Test: 259 (10.0%)\n",
            "\n",
            "✓ Saved: general_train.json, general_val.json, general_test.json\n",
            "\n",
            "============================================================\n",
            "SAMPLE QA PAIRS:\n",
            "============================================================\n",
            "\n",
            "Source: arxiv\n",
            "Context: category_cs.CY\n",
            "Q: What is the main contribution of this paper: Benefits and Limitations of Using GenAI for Political E...\n",
            "A: o facilitate political education? To explore respective potentials and limitations, we therefore performed an experimental study that combines different GenAI approaches. Language models were used to ...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: wikipedia\n",
            "Context: Album_section_0\n",
            "Q: Tell me about An album is a collection of audio recordings (e.g., music) issued on a medium su...\n",
            "A: An album is a collection of audio recordings (e.g., music) issued on a medium such as compact disc (CD), vinyl (record), audio tape (like 8-track or cassette), or digital. Albums of recorded sound wer...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: arxiv\n",
            "Context: category_cs.LG\n",
            "Q: Explain the research paper: A Review on Explainable Artificial Intelligence for Healthcare: Why, How...\n",
            "A: Artificial intelligence (AI) models are increasingly finding applications in the field of medicine. Concerns have been raised about the explainability of the decisions that are made by these AI models...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: arxiv\n",
            "Context: category_physics.comp-ph\n",
            "Q: Explain the research paper: Unveiling the optimization process of Physics Informed Neural Networks: ...\n",
            "A: This study investigates the potential accuracy boundaries of physics-informed neural networks, contrasting their approach with previous similar works and traditional numerical methods. We find that se...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: arxiv\n",
            "Context: category_cs.LG\n",
            "Q: Explain the research paper: XtarNet: Learning to Extract Task-Adaptive Representation for Incrementa...\n",
            "A: Learning novel concepts while preserving prior knowledge is a long-standing challenge in machine learning. The challenge gets greater when a novel task is given with only a few labeled examples, a pro...\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-B0fFvghjY82"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}